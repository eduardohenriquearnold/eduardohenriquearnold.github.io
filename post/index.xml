<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts | Eduardo Arnold</title><link>https://earnold.me/post/</link><atom:link href="https://earnold.me/post/index.xml" rel="self" type="application/rss+xml"/><description>Posts</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 17 Nov 2020 15:54:00 -0300</lastBuildDate><image><url>https://earnold.me/images/icon_hu86725dfa3481f0beabea4a23db3db448_10764_512x512_fill_lanczos_center_3.png</url><title>Posts</title><link>https://earnold.me/post/</link></image><item><title>Real-time visualisation of simulated lidar point-clouds with Mayavi and CARLA</title><link>https://earnold.me/post/mayavilidar/</link><pubDate>Tue, 17 Nov 2020 15:54:00 -0300</pubDate><guid>https://earnold.me/post/mayavilidar/</guid><description>&lt;p>This post shows how to visualise lidar point-clouds obtained from
&lt;a href="https://carla.org/" target="_blank" rel="noopener">CARLA&lt;/a> in real-time using the animation functionality from
&lt;a href="https://docs.enthought.com/mayavi/mayavi/index.html" target="_blank" rel="noopener">Mayavi&lt;/a>.
Although this post uses real-time data from CARLA, one can easily change the source of information to real sensors or simply replay recorded sensor data.&lt;/p>
&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>CARLA provides a lidar visualisation script using
&lt;a href="http://www.open3d.org/" target="_blank" rel="noopener">Open3D&lt;/a> available
&lt;a href="https://github.com/carla-simulator/carla/blob/dev/PythonAPI/examples/open3d_lidar.py" target="_blank" rel="noopener">here&lt;/a>.
However, I personally found Open3D to have quite a long dependency list since it is a library for manipulating 3D data including an extensive list of algorithms.
So I would rather use a tool created specifically for 3D data visualisation - enters Mayavi.
Mayavi has a much smaller dependency list and is widely used to plot point clouds in the autonomous driving domain - usually adding bounding boxes to represent objects.
However it comes with some perks, namely when creating visualisations for a continuous and asynchronous data stream.
Since I could not find a better reference for this problem I decided to share my solution in this post.&lt;/p>
&lt;h2 id="carla-setup">CARLA Setup&lt;/h2>
&lt;p>Firstly, let&amp;rsquo;s set up the CARLA end to receive the data. This section assumes prior knowledge of the CARLA simulation environment and is partly adapted from &lt;code>PythonAPI/open3d_lidar.py&lt;/code>&lt;/p>
&lt;p>The first step is connecting with the CARLA server and setting up the synchronous mode, which will ensure that we get consistent point clouds.&lt;/p>
&lt;pre>&lt;code class="language-python">import carla
client = carla.Client('localhost', 2000)
client.set_timeout(2.0)
world = client.get_world()
try:
settings = world.get_settings()
traffic_manager = client.get_trafficmanager(8000)
traffic_manager.set_synchronous_mode(True)
delta = 0.05
settings.fixed_delta_seconds = delta
settings.synchronous_mode = True
settings.no_rendering_mode = arg.no_rendering
world.apply_settings(settings)
&lt;/code>&lt;/pre>
&lt;p>Next we spawn an ego-vehicle into our simulated world using a random starting position and a random blueprint (vehicle model):&lt;/p>
&lt;pre>&lt;code class="language-python">blueprint_library = world.get_blueprint_library()
vehicle_bp = blueprint_library.filter(arg.filter)[0]
vehicle_transform = random.choice(world.get_map().get_spawn_points())
vehicle = world.spawn_actor(vehicle_bp, vehicle_transform)
vehicle.set_autopilot(arg.no_autopilot)
&lt;/code>&lt;/pre>
&lt;p>We must now create our ray-cast lidar sensor and set-up some parameters (a complete list of sensor parameters is available
&lt;a href="https://carla.readthedocs.io/en/latest/ref_sensors/#lidar-sensor" target="_blank" rel="noopener">here&lt;/a>):&lt;/p>
&lt;pre>&lt;code class="language-python">lidar_bp = blueprint_library.find('sensor.lidar.ray_cast')
lidar_bp.set_attribute('noise_stddev', '0.2')
lidar_bp.set_attribute('channels', str(64)) #number of lasers, normally 64 or 128 (on newer lidar models).
lidar_bp.set_attribute('range', str(100)) #range in meters
lidar_bp.set_attribute('rotation_frequency', str(1.0 / delta)) #ensures we will get a full sweep within a simulation frame
lidar_transform = carla.Transform(carla.Location(x=-0.5, z=1.8))
lidar = world.spawn_actor(lidar_bp, lidar_transform, attach_to=vehicle)
lidar.listen(lidar_callback)
&lt;/code>&lt;/pre>
&lt;p>The last line sets up the callback function that gets called everytime new data (point clouds) arrives, but we still do not know what this function should look like.&lt;/p>
&lt;p>The simulation runs in synchronous mode in such a way that we must send a &lt;code>world.tick()&lt;/code> event at every iteration step so that the simulation can run another loop iteration, updating the physical models and generating new sensor data. This prevents us getting flooded with data if our processing pipeline runs much slower than the simulation itself.
Although the simulation runs in synchronous mode, the data is still transfered in an ansynchronous manner, which we receive through the callback function &lt;code>lidar_callback(data)&lt;/code>.
To make sure the simulation runs continuously we create an infinite loop as&lt;/p>
&lt;pre>&lt;code class="language-python">while True:
time.sleep(0.005)
world.tick()
&lt;/code>&lt;/pre>
&lt;p>We still need to figure out what the &lt;code>lidar_callback&lt;/code> function looks like. This will depend on how we visualise our data, so now we dive into the Mayavi part!&lt;/p>
&lt;h2 id="mayavi">Mayavi&lt;/h2>
&lt;p>Given a set of points &lt;code>pts&lt;/code> with shape &lt;code>[N,3]&lt;/code> where $N$ is the number of points and an optional set of intensities for each given point, one can visualise the point cloud using&lt;/p>
&lt;pre>&lt;code class="language-python">from mayavi import mlab
#given a set of points pts [N,3] and a set of intensities [N,]
mlab.points3d(pts[:,0], pts[:,1], pts[:,2], intensity, mode='point')
mlab.show()
&lt;/code>&lt;/pre>
&lt;p>Given this, one could create the lidar callback function as&lt;/p>
&lt;pre>&lt;code class="language-python">def lidar_callback(data):
data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4')))
data = np.reshape(data, (int(data.shape[0] / 4), 4))
#Isolate the intensity
intensity = data[:, -1]
#Isolate the 3D data
points = data[:, :-1]
#We're negating the y to correclty visualize a world that matches
#what we see in Unreal since Mayavi uses a right-handed coordinate system
points[:, :1] = -points[:, :1]
mlab.points3d(points[:,0], points[:,1], points[:,2], intensity, mode='point')
mlab.show()
&lt;/code>&lt;/pre>
&lt;p>This creates a static visualisation each time a new packet of data arrives, which is quite inefficient and does not allow the user to interect with the data (i.e. change viewing angles).
Mayavi provides a
&lt;a href="https://docs.enthought.com/mayavi/mayavi/mlab_animating.html" target="_blank" rel="noopener">animation guide&lt;/a> that shows how to create visualisations that change with time.
However it assumes that the data is updated in synchronous intervals which is not the case when we obtain the data with a asynchronous callback function from an external source such as a simulation tool or a real sensor.&lt;/p>
&lt;p>One way to solve this is to create a visualisation within the main thread scope (required by Mayavi) and update this visualisation once we get any callback with new data.
This solution however requires calling &lt;code>mlab.show()&lt;/code> on the main thread, which blocks the execution of code until the Mayavi visualisation screen is closed and means that we can no longer keep sending the &lt;code>world.tick()&lt;/code> signals back to the simulator.
To overcome this limitation we create a secondary thread that is responsible for sending the &lt;code>world.tick()&lt;/code> updates, while at the same time creating an empty Mayavi visualisation window:&lt;/p>
&lt;pre>&lt;code class="language-python">import threading
def carlaEventLoop(world):
while True:
time.sleep(0.005)
world.tick()
loopThread = threading.Thread(target=carlaEventLoop, args=[world], daemon=True)
loopThread.start()
vis = mlab.points3d(0, 0, 0, 0, mode='point', figure=fig)
mlab.show()
&lt;/code>&lt;/pre>
&lt;p>Now one could implement the lidar callback function as&lt;/p>
&lt;pre>&lt;code class="language-python">def lidar_callback(data, vis):
data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4')))
data = np.reshape(data, (int(data.shape[0] / 4), 4))
#Isolate the intensity
intensity = data[:, -1]
#Isolate the 3D data
points = data[:, :-1]
#We're negating the y to correclty visualize a world that matches
#what we see in Unreal since Mayavi uses a right-handed coordinate system
points[:, :1] = -points[:, :1]
#Update visualisation using Mayavi animation guide
vis.mlab_source.reset(x=points[:,0], y=points[:,1], z=points[:,2], scalars=intensity)
#To register this callback we use a lambda function to mask the vis variable with the empty visualisation created previously
lidar.listen(lambda data: lidar_callback(data, vis))
&lt;/code>&lt;/pre>
&lt;p>This formulation tends to work most of the time, but occasionally fails with VTK errors such as &lt;code>Source array too small, requested tuple at index 11719, but there are only 11625 tuples in the array.&lt;/code>.
The error seems related to the frequency of updates created by the callback function. Although I did not have time to investigate why exactly this issue arises, I was able to come up with an alternative error-free solution.&lt;/p>
&lt;p>The alternative solution consists of creating a buffer that stores the most recent point cloud received through the callback function, but only updating the Mayavi visualisation in synchronous intervals.
The main part of the code (after creating the sensors and vehicle) looks like:&lt;/p>
&lt;pre>&lt;code class="language-python">def lidar_callback(data, buf):
data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4')))
data = np.reshape(data, (int(data.shape[0] / 4), 4))
#Isolate the intensity
intensity = data[:, -1]
#Isolate the 3D data
points = data[:, :-1]
#We're negating the y to correclty visualize a world that matches
#what we see in Unreal since Mayavi uses a right-handed coordinate system
points[:, :1] = -points[:, :1]
#copy points/intensities into buffer
buf['pts'] = points
buf['intensity'] = intensity
def carlaEventLoop(world):
while True:
time.sleep(0.005)
world.tick()
def main():
#creates client,world ...
#spawns vehicle ...
#creates sensor ...
#creates empty visualisation
vis = mlab.points3d(0, 0, 0, 0, mode='point', figure=fig)
#defines empty buffer
buf = {'pts': np.zeros((1,3)), 'intensity':np.zeros(1)}
#set callback
lidar.listen(lambda data: lidar_callback(data, buf))
#creates thread for event loop
loopThread = threading.Thread(target=carlaEventLoop, args=[world], daemon=True)
loopThread.start()
#define mayavi animation loop
@mlab.animate(delay=100)
def anim():
while True:
vis.mlab_source.reset(x=buf['pts'][:,0], y=buf['pts'][:,1], z=buf['pts'][:,2], scalars=buf['intensity'])
yield
#start visualisation loop in the main-thread, blocking other executions
anim()
mlab.show()
&lt;/code>&lt;/pre>
&lt;h2 id="visualisation-results">Visualisation results&lt;/h2>
&lt;p>Using the default lidar noise parameters for point dropout and Gaussian noise parameters we can now visualise the point clouds coming from the CARLA simulator in real time directly in the Mayavi interface:&lt;/p>
&lt;p>&lt;img src="lidar.gif" alt="lidar point cloud result">&lt;/p>
&lt;h2 id="source-code">Source code&lt;/h2>
&lt;p>You may find the complete code for this post in
&lt;a href="https://earnold.me/notebooks/mayavi_lidar.py">mayavi_lidar.py&lt;/a>.&lt;/p></description></item><item><title>Forecasting sales with Gaussian Processes and Autoregressive models</title><link>https://earnold.me/post/timeseriesforecasting/</link><pubDate>Thu, 23 Jul 2020 15:57:57 +0100</pubDate><guid>https://earnold.me/post/timeseriesforecasting/</guid><description>&lt;h1 id="problem-definition">Problem definition&lt;/h1>
&lt;p>Sales forecasting is a very common problem faced by many companies. Given a history of sales of a certain product we would like to predict the demand of that product for a time window in the future. This is useful as it allows companies and industries to plan their workload and reduce waste of resources. This problem is a common example of time series forecasting and there are many approaches to tackle it.&lt;/p>
&lt;p>Today we will learn a bit more about this with a practical example: the
&lt;a href="https://www.kaggle.com/c/competitive-data-science-predict-future-sales/" target="_blank" rel="noopener">Kaggle Predict Future Sales dataset&lt;/a>. This challenge aims to predict future sales of different products for the next month in different shops of a retail chain. In this notebook we do not aim at solving this challenge, rather, we will explore some concepts of time series forecasting that could be used to solve such problem.&lt;/p>
&lt;p>You can download the Jupyter notebook version of this tutorial
&lt;a href="https://github.com/eduardohenriquearnold/eduardohenriquearnold.github.io/blob/master/notebooks/timeseriesforecast.ipynb" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;h1 id="exploring-the-dataset">Exploring the dataset&lt;/h1>
&lt;p>First we need to download the whole dataset and extract it to a &lt;code>data&lt;/code> folder in the root of this notebook path.&lt;/p>
&lt;pre>&lt;code class="language-python">import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from ipywidgets import interact_manual
plt.style.use('ggplot')
%matplotlib inline
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">#load sales and rename columns to ease understanding
dtype = {'date_block_number':np.uint16, 'shop_id':np.uint32, 'item_id':np.uint32, 'item_cnt_day':np.float32}
sales = pd.read_csv('data/sales_train.csv', dtype=dtype) \
.rename(columns={'date_block_num': 'month', 'item_cnt_day':'sold'})
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">sales.head()
&lt;/code>&lt;/pre>
&lt;div>
&lt;style scoped>
.dataframe tbody tr th:only-of-type {
vertical-align: middle;
}
&lt;pre>&lt;code>.dataframe tbody tr th {
vertical-align: top;
}
.dataframe thead th {
text-align: right;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;/style>&lt;/p>
&lt;table border="1" class="dataframe">
&lt;thead>
&lt;tr style="text-align: right;">
&lt;th>&lt;/th>
&lt;th>date&lt;/th>
&lt;th>month&lt;/th>
&lt;th>shop_id&lt;/th>
&lt;th>item_id&lt;/th>
&lt;th>item_price&lt;/th>
&lt;th>sold&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;th>0&lt;/th>
&lt;td>02.01.2013&lt;/td>
&lt;td>0&lt;/td>
&lt;td>59&lt;/td>
&lt;td>22154&lt;/td>
&lt;td>999.00&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>1&lt;/th>
&lt;td>03.01.2013&lt;/td>
&lt;td>0&lt;/td>
&lt;td>25&lt;/td>
&lt;td>2552&lt;/td>
&lt;td>899.00&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>2&lt;/th>
&lt;td>05.01.2013&lt;/td>
&lt;td>0&lt;/td>
&lt;td>25&lt;/td>
&lt;td>2552&lt;/td>
&lt;td>899.00&lt;/td>
&lt;td>-1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>3&lt;/th>
&lt;td>06.01.2013&lt;/td>
&lt;td>0&lt;/td>
&lt;td>25&lt;/td>
&lt;td>2554&lt;/td>
&lt;td>1709.05&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>4&lt;/th>
&lt;td>15.01.2013&lt;/td>
&lt;td>0&lt;/td>
&lt;td>25&lt;/td>
&lt;td>2555&lt;/td>
&lt;td>1099.00&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;p>As we can see, we have daily records for each shop and item. Note that some of the &lt;code>sold&lt;/code> values are negative because they include returns.&lt;/p>
&lt;p>Firstly, since we are interested in estimating the monthly sales, we will aggregate the dataset by month using the handy &lt;code>month&lt;/code> feature, which ranges from 0 (representing January 2013) to 33 (October 2015). We will group the sales by the shop and item ids. The aggregation will sum all &lt;code>sold&lt;/code> fields during the month for each product and shop, and average the item_price (it is likely to change from month to month).&lt;/p>
&lt;pre>&lt;code class="language-python">gsales = sales.groupby(['shop_id','item_id','month']).agg({'item_price':np.mean, 'sold':np.sum})
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">gsales[:20]
&lt;/code>&lt;/pre>
&lt;div>
&lt;style scoped>
.dataframe tbody tr th:only-of-type {
vertical-align: middle;
}
&lt;pre>&lt;code>.dataframe tbody tr th {
vertical-align: top;
}
.dataframe thead th {
text-align: right;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;/style>&lt;/p>
&lt;table border="1" class="dataframe">
&lt;thead>
&lt;tr style="text-align: right;">
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>item_price&lt;/th>
&lt;th>sold&lt;/th>
&lt;/tr>
&lt;tr>
&lt;th>shop_id&lt;/th>
&lt;th>item_id&lt;/th>
&lt;th>month&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;th rowspan="20" valign="top">0&lt;/th>
&lt;th>30&lt;/th>
&lt;th>1&lt;/th>
&lt;td>265.0&lt;/td>
&lt;td>31.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>31&lt;/th>
&lt;th>1&lt;/th>
&lt;td>434.0&lt;/td>
&lt;td>11.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th rowspan="2" valign="top">32&lt;/th>
&lt;th>0&lt;/th>
&lt;td>221.0&lt;/td>
&lt;td>6.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>1&lt;/th>
&lt;td>221.0&lt;/td>
&lt;td>10.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th rowspan="2" valign="top">33&lt;/th>
&lt;th>0&lt;/th>
&lt;td>347.0&lt;/td>
&lt;td>3.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>1&lt;/th>
&lt;td>347.0&lt;/td>
&lt;td>3.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th rowspan="2" valign="top">35&lt;/th>
&lt;th>0&lt;/th>
&lt;td>247.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>1&lt;/th>
&lt;td>247.0&lt;/td>
&lt;td>14.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>36&lt;/th>
&lt;th>1&lt;/th>
&lt;td>357.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>40&lt;/th>
&lt;th>1&lt;/th>
&lt;td>127.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>42&lt;/th>
&lt;th>1&lt;/th>
&lt;td>127.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>43&lt;/th>
&lt;th>0&lt;/th>
&lt;td>221.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>49&lt;/th>
&lt;th>1&lt;/th>
&lt;td>127.0&lt;/td>
&lt;td>2.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th rowspan="2" valign="top">51&lt;/th>
&lt;th>0&lt;/th>
&lt;td>128.5&lt;/td>
&lt;td>2.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>1&lt;/th>
&lt;td>127.0&lt;/td>
&lt;td>3.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>57&lt;/th>
&lt;th>1&lt;/th>
&lt;td>167.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>59&lt;/th>
&lt;th>1&lt;/th>
&lt;td>110.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>61&lt;/th>
&lt;th>0&lt;/th>
&lt;td>195.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>75&lt;/th>
&lt;th>0&lt;/th>
&lt;td>76.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>85&lt;/th>
&lt;th>1&lt;/th>
&lt;td>190.0&lt;/td>
&lt;td>1.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;p>We can now observe that for the first shop many items only have selling records for the first two months. We would like to investigate how this varies for different products across all 60 shops.&lt;/p>
&lt;pre>&lt;code class="language-python">@interact_manual(shop_id = (0,59,1))
def plot_product_record_frequency(shop_id):
count_months = gsales.reset_index().groupby(['shop_id','item_id']).size()[shop_id]
plt.bar(count_months.keys(), count_months)
plt.xlabel('product_id')
plt.ylabel('Num months available')
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>interactive(children=(IntSlider(value=29, description='shop_id', max=59), Button(description='Run Interact', s…
&lt;/code>&lt;/pre>
&lt;p>However, this interactive visualisation will not work unless you have a notebook running, so we will plot the sales frequency for some of the stores below:&lt;/p>
&lt;pre>&lt;code class="language-python">def plot_product_record_frequency(shop_id):
count_months = gsales.reset_index().groupby(['shop_id','item_id']).size()[shop_id]
plt.bar(count_months.keys(), count_months)
plt.xlabel('product_id')
plt.ylabel('Num months available')
plt.title(f'shop_id {shop_id}')
fig=plt.figure(figsize=(12, 8), dpi= 80, facecolor='w', edgecolor='k')
for i, shop_id in enumerate([0,1,29,31]):
plt.subplot(2,2,i+1)
plot_product_record_frequency(shop_id)
plt.tight_layout()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_15_0.png" alt="png">&lt;/p>
&lt;p>We can observe that some shops have a very limited record of sales, e.g. shop 0 and 1. On the other hand, shop 29 and 31 have a considerable number of items with a record above 10 weeks.&lt;/p>
&lt;p>Perhaps we should look into the shops with the larger number of sales, which we can inspect by observing the distribution of &lt;code>shop_id&lt;/code>:&lt;/p>
&lt;pre>&lt;code class="language-python">sales['shop_id'].plot.hist(bins=60)
plt.xlabel('product_id');
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_18_0.png" alt="png">&lt;/p>
&lt;p>If we observe the previous histogram for the number of weeks of records for each product of a given shop, we will observe indeed that the shop 31 has a comprehensive number of products with a significant history of sales.&lt;/p>
&lt;p>Considering the behaviour of multiple shops introduce complexity steaming from this biased distribution of sales. Some shops will have very little preior information about their sales, so it would be difficult to make a good prediction of sales in these shops. For this reason we will neglect different shops and instead try to estimate a new volume of sales for all products in the whole supermarket chain. So, we modify our aggregated sales DataFrame &lt;code>gsales&lt;/code> accordingly:&lt;/p>
&lt;pre>&lt;code class="language-python">gsales = sales.groupby(['item_id','month']).agg({'item_price':np.mean, 'sold':np.sum})
&lt;/code>&lt;/pre>
&lt;p>We can also observe what is the number of months available for each product sale history in this chain-wide collection:&lt;/p>
&lt;pre>&lt;code class="language-python">count_months = gsales.reset_index().groupby(['item_id']).size()
plt.bar(count_months.keys(),count_months);
plt.xlabel('product_id')
plt.ylabel('Num months available');
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_23_0.png" alt="png">&lt;/p>
&lt;p>Although in a real forecast scenario we would like to have a good prediction of sales even when the history for a given product is minimal, in this example we will focus on products with full history for all 34 months.&lt;/p>
&lt;pre>&lt;code class="language-python">productsIdx = count_months[count_months == 34].index.values
selected_sales = gsales.loc[productsIdx]
productsIdx.shape
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>(523,)
&lt;/code>&lt;/pre>
&lt;p>We obtained 523 unique products that have a selling history for all of the 33 months. An example of the first 60 items are below:&lt;/p>
&lt;pre>&lt;code class="language-python">selected_sales[:60]
&lt;/code>&lt;/pre>
&lt;div>
&lt;style scoped>
.dataframe tbody tr th:only-of-type {
vertical-align: middle;
}
&lt;pre>&lt;code>.dataframe tbody tr th {
vertical-align: top;
}
.dataframe thead th {
text-align: right;
}
&lt;/code>&lt;/pre>
&lt;p>&lt;/style>&lt;/p>
&lt;table border="1" class="dataframe">
&lt;thead>
&lt;tr style="text-align: right;">
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;th>item_price&lt;/th>
&lt;th>sold&lt;/th>
&lt;/tr>
&lt;tr>
&lt;th>item_id&lt;/th>
&lt;th>month&lt;/th>
&lt;th>&lt;/th>
&lt;th>&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;th rowspan="34" valign="top">32&lt;/th>
&lt;th>0&lt;/th>
&lt;td>338.110349&lt;/td>
&lt;td>299.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>1&lt;/th>
&lt;td>337.771930&lt;/td>
&lt;td>208.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>2&lt;/th>
&lt;td>343.794702&lt;/td>
&lt;td>178.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>3&lt;/th>
&lt;td>341.888889&lt;/td>
&lt;td>97.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>4&lt;/th>
&lt;td>347.000000&lt;/td>
&lt;td>66.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>5&lt;/th>
&lt;td>342.070886&lt;/td>
&lt;td>79.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>6&lt;/th>
&lt;td>345.951190&lt;/td>
&lt;td>87.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>7&lt;/th>
&lt;td>340.172727&lt;/td>
&lt;td>72.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>8&lt;/th>
&lt;td>340.017544&lt;/td>
&lt;td>59.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>9&lt;/th>
&lt;td>184.592593&lt;/td>
&lt;td>58.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>10&lt;/th>
&lt;td>144.316456&lt;/td>
&lt;td>81.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>11&lt;/th>
&lt;td>147.994444&lt;/td>
&lt;td>89.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>12&lt;/th>
&lt;td>144.710526&lt;/td>
&lt;td>84.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>13&lt;/th>
&lt;td>144.066667&lt;/td>
&lt;td>48.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>14&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>44.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>15&lt;/th>
&lt;td>143.714286&lt;/td>
&lt;td>30.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>16&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>26.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>17&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>26.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>18&lt;/th>
&lt;td>130.500000&lt;/td>
&lt;td>12.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>19&lt;/th>
&lt;td>148.991176&lt;/td>
&lt;td>34.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>20&lt;/th>
&lt;td>144.771429&lt;/td>
&lt;td>37.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>21&lt;/th>
&lt;td>148.991667&lt;/td>
&lt;td>37.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>22&lt;/th>
&lt;td>146.060714&lt;/td>
&lt;td>29.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>23&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>40.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>24&lt;/th>
&lt;td>145.572727&lt;/td>
&lt;td>42.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>25&lt;/th>
&lt;td>146.387333&lt;/td>
&lt;td>32.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>26&lt;/th>
&lt;td>146.869167&lt;/td>
&lt;td>40.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>27&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>20.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>28&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>20.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>29&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>26.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>30&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>21.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>31&lt;/th>
&lt;td>148.714286&lt;/td>
&lt;td>30.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>32&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>19.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>33&lt;/th>
&lt;td>149.000000&lt;/td>
&lt;td>22.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th rowspan="26" valign="top">33&lt;/th>
&lt;th>0&lt;/th>
&lt;td>488.517241&lt;/td>
&lt;td>61.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>1&lt;/th>
&lt;td>484.170732&lt;/td>
&lt;td>39.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>2&lt;/th>
&lt;td>490.870968&lt;/td>
&lt;td>32.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>3&lt;/th>
&lt;td>489.500000&lt;/td>
&lt;td>16.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>4&lt;/th>
&lt;td>499.000000&lt;/td>
&lt;td>12.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>5&lt;/th>
&lt;td>205.046512&lt;/td>
&lt;td>44.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>6&lt;/th>
&lt;td>195.439130&lt;/td>
&lt;td>46.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>7&lt;/th>
&lt;td>197.277778&lt;/td>
&lt;td>35.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>8&lt;/th>
&lt;td>198.052381&lt;/td>
&lt;td>43.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>9&lt;/th>
&lt;td>195.915152&lt;/td>
&lt;td>33.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>10&lt;/th>
&lt;td>194.866667&lt;/td>
&lt;td>15.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>11&lt;/th>
&lt;td>195.900000&lt;/td>
&lt;td>42.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>12&lt;/th>
&lt;td>197.487805&lt;/td>
&lt;td>42.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>13&lt;/th>
&lt;td>196.862069&lt;/td>
&lt;td>29.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>14&lt;/th>
&lt;td>197.673333&lt;/td>
&lt;td>30.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>15&lt;/th>
&lt;td>199.000000&lt;/td>
&lt;td>18.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>16&lt;/th>
&lt;td>196.658824&lt;/td>
&lt;td>17.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>17&lt;/th>
&lt;td>199.000000&lt;/td>
&lt;td>21.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>18&lt;/th>
&lt;td>199.000000&lt;/td>
&lt;td>14.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>19&lt;/th>
&lt;td>197.756250&lt;/td>
&lt;td>17.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>20&lt;/th>
&lt;td>199.000000&lt;/td>
&lt;td>13.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>21&lt;/th>
&lt;td>199.000000&lt;/td>
&lt;td>14.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>22&lt;/th>
&lt;td>195.460000&lt;/td>
&lt;td>20.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>23&lt;/th>
&lt;td>199.000000&lt;/td>
&lt;td>21.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>24&lt;/th>
&lt;td>199.000000&lt;/td>
&lt;td>19.0&lt;/td>
&lt;/tr>
&lt;tr>
&lt;th>25&lt;/th>
&lt;td>199.000000&lt;/td>
&lt;td>26.0&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;/div>
&lt;p>We have explored the dataset and observed some items of interest. The next step is using this information to predict the number of sales over another month&lt;/p>
&lt;h1 id="time-series-analysis">Time Series Analysis&lt;/h1>
&lt;p>Firstly, let&amp;rsquo;s observe what the sales time series look like for some of the products:&lt;/p>
&lt;pre>&lt;code class="language-python">fig=plt.figure(figsize=(12, 8), dpi= 80, facecolor='w', edgecolor='k')
for i, idx in enumerate(selected_sales.index.get_level_values(0).unique()[10:20]):
plt.subplot(5,2,i+1)
selected_sales.loc[idx]['sold'].plot(style='r.')
plt.title(f'Sales for item_id {idx}')
plt.tight_layout()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_31_0.png" alt="png">&lt;/p>
&lt;p>We can see some seasonality patterns in the plotted data, so we will decompose one of the observed time series into a trend and seasonal effects with a multiplicative model $Y(t) = T(t) S(t) r(t)$ where $T(t)$ is the trend, $S(t)$ the the seasonal component and $r(t)$ is the residual.&lt;/p>
&lt;pre>&lt;code class="language-python">from statsmodels.tsa.seasonal import seasonal_decompose
item_id = 491
sales_product = selected_sales.loc[item_id]['sold'].values
dec = seasonal_decompose(sales_product, model='multiplicative', period=12)
dec.plot();
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_33_0.png" alt="png">&lt;/p>
&lt;p>We observed that the residual values tend to be close to 1, which means the observed series has a good fit to the seasonal and trend decomposition.&lt;/p>
&lt;p>This analysis shows that the observed time series have a trend that decreases throughout the months, and a seasonal component, which seems to peak around August and December. This decomposition could be exploited to improve prediction results, however we would require a model that incorporate this seasonality pattern.&lt;/p>
&lt;h1 id="time-series-forecasting">Time series forecasting&lt;/h1>
&lt;p>In this section we will evaluate two different models for time series forecasting: Auto Regressive Integrated Moving Average (ARIMA) and Gaussian Process (GP).&lt;/p>
&lt;p>Firstly, we train the models on each product time series up to month 30. Then, we evaluate for the remaining of months available across all selected products. Our metric will be the Root Mean Squared Error (RMSE) computed with the predicted and ground-truth time series. Please note that this metric is the same as used in the original challenge.&lt;/p>
&lt;h3 id="gaussian-processes">Gaussian Processes&lt;/h3>
&lt;p>GPs are non-parametric models that can represent a posterior over functions. They work well when we do not wish to impose strong assumptions on the generative data process. For a more detailed explanation of GPs, please visit this
&lt;a href="https://distill.pub/2019/visual-exploration-gaussian-processes/" target="_blank" rel="noopener">distill post&lt;/a>.&lt;/p>
&lt;p>We chose experimented with a multitude of kernels, the best performing were the simpler ones: RBF and Matern. Note that the parameters of these kernels are optimised (within the given bounds) during the &lt;code>fit&lt;/code> process. Please note that we normalise the target sales by the maximum value such that the target number of sales ranges from [0,1].&lt;/p>
&lt;p>First, let&amp;rsquo;s observe what happens using a single item_id and extrapolating between the months:&lt;/p>
&lt;pre>&lt;code class="language-python">from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.gaussian_process.kernels import WhiteKernel, RBF, Matern, ExpSineSquared, ConstantKernel
kernel = RBF()
gp = GaussianProcessRegressor(kernel = kernel, n_restarts_optimizer=2, alpha = 1e-5, normalize_y=True)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">item_id = 53
X = np.arange(0,34,0.05).reshape(-1,1)
Y = selected_sales['sold'][item_id].values.reshape(-1,1)
ymax = Y.max()
gp.fit(np.arange(30).reshape(-1,1), Y[:30]/ymax)
Y_pred, Y_pred_std = gp.predict(X, return_std=True)
Y_pred = Y_pred.reshape(-1)*ymax
fig=plt.figure(figsize=(6, 4), dpi= 80, facecolor='w', edgecolor='k')
plt.plot(X, Y_pred, 'r.', label='Pred')
plt.fill_between(X.reshape(-1), Y_pred - Y_pred_std, Y_pred + Y_pred_std, alpha=0.2, color='k');
plt.plot(np.arange(34), Y, 'bx', label='GT')
plt.axvline(29.5, 0, 1, color='black', ls='--', label='Train/Test split')
plt.legend();
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_44_0.png" alt="png">&lt;/p>
&lt;p>Now we will create a GP for each item_id time series within our selected sales:&lt;/p>
&lt;pre>&lt;code class="language-python">Y_preds_gp = []
Y_gts = []
for item_id in selected_sales.index.get_level_values(0).unique():
X = np.arange(34).reshape(-1,1)
X_train, X_test = X[:30], X[30:]
Y = selected_sales['sold'][item_id].values.reshape(-1,1)
Y_train, Y_test = Y[:30], Y[30:]
ymax = Y_train.max()
kernel = RBF()
gp = GaussianProcessRegressor(kernel = kernel, n_restarts_optimizer=0, alpha = 1e-2, normalize_y=True)
gp.fit(X_train, Y_train/ymax)
ypred = gp.predict(X_test)*ymax
Y_preds_gp.append(ypred)
Y_gts.append(Y_test)
Y_preds_gp = np.concatenate(Y_preds_gp, axis=0)
Y_gts = np.concatenate(Y_gts, axis=0)
&lt;/code>&lt;/pre>
&lt;h3 id="arima">ARIMA&lt;/h3>
&lt;p>Auto Regressive Integrated Moving Average models the time series using
$$Y(t) = \alpha + \beta_1 Y(t-1) + \beta_2 Y(t-2) + \dots + \beta_p Y(t-p) + \gamma_1 \epsilon(t-1) + \gamma_2 \epsilon(t-2) + \dots + \gamma_q \epsilon(t-q) + \epsilon(t)$$
where $\epsilon(t)$ is the residual from the ground-truth and estimated value. The parameters $\alpha, \beta, \gamma$ are optimised during fitting. The hyper-parameters $p,d,q$ correspond to the order of the process, &lt;strong>i.e.&lt;/strong> how many terms of previous timestamps, how many previous error terms and how many times to differentiate the time series until it becomes stationary.&lt;/p>
&lt;p>For simplicity, we assume our time-series are stationary and use $p=1$, $q=0$, $d=0$&lt;/p>
&lt;p>Again, let&amp;rsquo;s start by visualising a single time-series and the resulting ARIMA prediction.&lt;/p>
&lt;pre>&lt;code class="language-python">from statsmodels.tsa.arima_model import ARIMA
item_id = 53
Y = selected_sales['sold'][item_id].values.reshape(-1,1)
model = ARIMA(Y[:30], order=(1,0,0)).fit(trend='nc')
Y_pred = model.predict(0,33)
fig=plt.figure(figsize=(6, 4), dpi= 80, facecolor='w', edgecolor='k')
plt.plot(X, Y_pred, 'r.', label='Pred')
plt.plot(np.arange(34), Y, 'bx', label='GT')
plt.axvline(29.5, 0, 1, color='black', ls='--', label='Train/Test split')
plt.legend();
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_51_0.png" alt="png">&lt;/p>
&lt;p>We may also observe the approximate distribution of the residuals:&lt;/p>
&lt;pre>&lt;code class="language-python">residuals = pd.DataFrame(model.resid)
residuals.plot.kde();
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_53_0.png" alt="png">&lt;/p>
&lt;p>Next, we forecast for all items in the selected sales for the remaining 4 months (30,31,32,33):&lt;/p>
&lt;pre>&lt;code class="language-python">Y_preds_arima = []
for item_id in selected_sales.index.get_level_values(0).unique():
Y = selected_sales['sold'][item_id].values.reshape(-1,1)
Y_train, Y_test = Y[:30], Y[30:]
ypred = gp.predict(X_test)
model = ARIMA(Y[:30], order=(1,0,0)).fit(trend='nc')
ypred = model.predict(30,33)
Y_preds_arima.append(ypred)
Y_preds_arima = np.concatenate(Y_preds_arima, axis=0)
&lt;/code>&lt;/pre>
&lt;h3 id="evaluation">Evaluation&lt;/h3>
&lt;p>Finally computing the RMSE metric between predictions (of GP and ARIMA) and ground-truths for all selected items for the remaining 4 months (30,31,32,33):&lt;/p>
&lt;pre>&lt;code class="language-python">from sklearn.metrics import mean_squared_error as mse
rmse_gp = mse(Y_gts, Y_preds_gp, squared=False)
rmse_arima = mse(Y_gts, Y_preds_arima, squared=False)
print(f&amp;quot;RMSE GP {rmse_gp} \nRMSE ARIMA {rmse_arima}&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>RMSE GP 58.58414105624866
RMSE ARIMA 50.27118697649368
&lt;/code>&lt;/pre>
&lt;p>Knowingly, forecasting 4 months into the future would be difficult. Instead we could consider only the 30th month forecast of all items:&lt;/p>
&lt;pre>&lt;code class="language-python">rmse_gp = mse(Y_gts[::4], Y_preds_gp[::4], squared=False)
rmse_arima = mse(Y_gts[::4], Y_preds_arima[::4], squared=False)
print(f&amp;quot;RMSE GP {rmse_gp} \nRMSE ARIMA {rmse_arima}&amp;quot;)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>RMSE GP 28.212626816686967
RMSE ARIMA 7.92125195871772
&lt;/code>&lt;/pre>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>We observed that the ARIMA model performed better under the RMSE metric for the presented dataset. Still, both RMSE are quite high and should be reduced if the model were to be used in practice.&lt;/p>
&lt;p>To further improve the results and to account to a more realistic setting where some items or shops will have a limited history of sales, we must work on feature engineering to overcome these limitations. Example of features that could be exploited include, but are not limited to:&lt;/p>
&lt;ol>
&lt;li>Shop-specific aggregated stats (mean num of sales)&lt;/li>
&lt;li>City-specific aggregated stats (mean num of sales, pop. size, etc).&lt;/li>
&lt;li>Item categories aggregated stats (mean num of sales for a specific product category)&lt;/li>
&lt;li>Item price and price variations&lt;/li>
&lt;/ol>
&lt;h1 id="further-reading">Further reading&lt;/h1>
&lt;p>If you are interested in time series forecasting I highly recommend the above blog posts:&lt;/p>
&lt;ol>
&lt;li>
&lt;a href="https://www.linkedin.com/pulse/how-use-machine-learning-time-series-forecasting-vegard-flovik-phd" target="_blank" rel="noopener">https://www.linkedin.com/pulse/how-use-machine-learning-time-series-forecasting-vegard-flovik-phd&lt;/a>&lt;/li>
&lt;li>
&lt;a href="https://towardsdatascience.com/an-overview-of-time-series-forecasting-models-a2fa7a358fcb" target="_blank" rel="noopener">https://towardsdatascience.com/an-overview-of-time-series-forecasting-models-a2fa7a358fcb&lt;/a>&lt;/li>
&lt;/ol></description></item><item><title>Bayesian Linear Regression</title><link>https://earnold.me/post/bayesianlr/</link><pubDate>Tue, 12 Mar 2019 20:29:55 +0100</pubDate><guid>https://earnold.me/post/bayesianlr/</guid><description>&lt;p>The bayesian linear regression formulation allows to obtain uncertainty estimates for the predictive distribution that are not available in its point-wise estimate counterpart. This notebook is based on Chapter 3 of Bishop&amp;rsquo;s Pattern Recognition and Machine Learning book.&lt;/p>
&lt;p>This notebook can be downloaded
&lt;a href="https://earnold.me/notebooks/bayesianlinearregression.ipynb">here&lt;/a>.&lt;/p>
&lt;pre>&lt;code class="language-python">import numpy as np
from scipy.stats import multivariate_normal
import matplotlib.pyplot as plt
%matplotlib inline
&lt;/code>&lt;/pre>
&lt;h3 id="generate-sample-dataset">Generate sample dataset&lt;/h3>
&lt;p>Generate N pairs $(x_i,y_i)$ with gaussian noise and $x_i$ sampled from uniform distribution&lt;/p>
&lt;pre>&lt;code class="language-python">N = 12
sigma = 0.1
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">x = np.random.uniform(low=-1, high=1, size=N)
n = np.random.normal(loc=0, scale=sigma, size=N)
y = 0.3*x -0.8 +n
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">plt.plot(x,y, 'r.');
plt.show()
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_7_0.png" alt="png">&lt;/p>
&lt;h2 id="point-estimate">Point estimate&lt;/h2>
&lt;p>We are trying to design a model $\hat{y} = x w_1 + w_0 + \epsilon$ with $\epsilon \sim N(0, \sigma^2)$&lt;/p>
&lt;p>Note that this model and noise assumption result in the following likelihood function: $$p(\hat{y}|x,w) = N(xw_1+w_0, \sigma)$$&lt;/p>
&lt;p>In general we aim for the Lease Squares (LS) solution: $$\min_w \sum_i (y_i-\hat{y}_i)^2$$&lt;/p>
&lt;p>Note that the LS solution is equivalent to the Maximum Likelihood Estimator. The solution can be obtained through minimizing the loss function through Gradient Descent. However, in the case of this simple linear model it is possible to use normal equations (closed form minimization result): $$\hat{w} = (X^TX)^{-1}X^Ty$$&lt;/p>
&lt;pre>&lt;code class="language-python">X = np.zeros((x.shape[0], 2))
X[:,0] = x
X[:,1] = 1
X
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>array([[ 0.07747538, 1. ],
[-0.72983355, 1. ],
[-0.08385175, 1. ],
[ 0.04152017, 1. ],
[-0.27236207, 1. ],
[-0.16471106, 1. ],
[ 0.43409736, 1. ],
[-0.33582112, 1. ],
[-0.48323886, 1. ],
[ 0.54369188, 1. ],
[-0.29194542, 1. ],
[ 0.98406384, 1. ]])
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">w = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)), X.T), y)
w
&lt;/code>&lt;/pre>
&lt;pre>&lt;code>array([ 0.28106915, -0.75913605])
&lt;/code>&lt;/pre>
&lt;p>However, this solution only provides a point estimate and lacks uncertainity information.&lt;/p>
&lt;h2 id="bayesian-inference">Bayesian inference&lt;/h2>
&lt;p>In turn, a bayesian approach treat $w$ as a RV which has a prior. Then, bayesian inference is used to obtain the posterior $p(w|X,Y)$ given observations&lt;/p>
&lt;p>In order to keep the solutions in closed-form, we use a Gaussian prior, allowing for a conjugate prior, for the vector $w$ $$w \sim N(w| m_0, S_0)$$&lt;/p>
&lt;p>Which then results in a Gaussian posterior&lt;/p>
&lt;p>$$p(w|X,Y) = \frac{p(Y|X,w)p(w)}{p(Y|X)} = N(w| m_N, S_N)$$ where $m_N = S_N (S_0^{-1}m_0+\frac{1}{\sigma}X^Ty)$ and $S_N^{-1} = S_0^{-1}+\frac{1}{\sigma}X^TX$&lt;/p>
&lt;p>For simplicity, let&amp;rsquo;s assume $m_0 = 0$ and $S_0 = \alpha^{-1}I = 0.5I$&lt;/p>
&lt;pre>&lt;code class="language-python">#prior parameters
a = 0.2
m0 = np.zeros(2)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def getPosterior(n):
#Get n points from sample dataset
x_ = X[:n]
y_ = y[:n]
#Covariance Matrix
S0I = a*np.identity(2)
SnI = S0I+ 1/sigma*np.dot(x_.T,x_)
Sn = np.linalg.inv(SnI)
#Mean
tt = np.dot(S0I, m0) + 1/sigma*np.dot(x_.T,y_)
Mn = np.dot(Sn, tt)
return multivariate_normal(mean=Mn, cov=Sn)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def plot_dist2D(dist):
x, y = np.mgrid[-1:1:.01, -1:1:.01]
pos = np.empty(x.shape + (2,))
pos[:, :, 0] = y; pos[:, :, 1] = x
plt.contourf(x, y, dist.pdf(pos))
plt.title('Posterior Distribution $p(w|X,Y)$')
plt.xlabel('w0')
plt.ylabel('w1')
&lt;/code>&lt;/pre>
&lt;h4 id="posterior-distribution-plots">Posterior distribution plots&lt;/h4>
&lt;p>We can plot the posterior after aggregating different number of points. Observe how the posterior distributions become narrower when more observation are aggregated&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist2D(getPosterior(1))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_27_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist2D(getPosterior(4))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_28_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist2D(getPosterior(6))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_29_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist2D(getPosterior(10))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_30_0.png" alt="png">&lt;/p>
&lt;p>The full posterior (when all points are incorporated) will have a peak on the mean, $w_{MAP} = m_N$, given the Gaussian distribution. In the case where the prior $p(w)$ is infinitely spread ($a \to 0$), $w_{MAP} = m_N = w_{ML} = (X^TX)^{-1}X^Ty$&lt;/p>
&lt;h4 id="the-predictive-distribution">The predictive distribution&lt;/h4>
&lt;p>Although we have estimated the posterior of parameters $w$, we are primarily interested in predicting the value of $\hat{y}$ for new sample x: $$p(\hat{y}| x, X,Y) = \int p(y|w)p(w|X,Y) dw$$&lt;/p>
&lt;p>Given the likelihood and posterior following Gaussian distributions, this predicitive distribution is also Gaussian: $$p(\hat{y}| x, X,Y) = N(\hat{y}| m_N^Tx, \sigma_N^2(x))$$ where $ \sigma_N^2(x) = \sigma^2 + x^TS_Nx $&lt;/p>
&lt;p>Note that the variance of the predictive distribution depends both on the assumed noise model ($\sigma$) and the uncertainty on the $w$ posterior&lt;/p>
&lt;pre>&lt;code class="language-python">def predictive(x, nTrainingSamples):
xp = np.zeros((2,1))
xp[0,0] = x
xp[1,0] = 1
xp = np.matrix(xp)
#Get posterior given nTrainingSamples
posterior = getPosterior(nTrainingSamples)
Mn = np.matrix(posterior.mean)
Sn = np.matrix(posterior.cov)
#Predictive mean
m = np.matmul(Mn,xp)
#Predictive cov
s = sigma**2 + np.dot(xp.T, np.dot(Sn,xp))
return multivariate_normal(mean=m, cov=s)
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">def plot_dist1D(dist):
x = np.linspace(-4,4, 100)
y = dist.pdf(x)
plt.plot(y,x)
plt.title('Predictive Distribution $p(\hat{y}|x, X,Y)$')
plt.xlabel('pdf')
plt.ylabel('$\hat{y}$')
&lt;/code>&lt;/pre>
&lt;h4 id="we-now-observe-how-the-predictive-distributions-become-more-certain-as-more-training-data-is-obtained">We now observe how the predictive distributions become more certain as more training data is obtained&lt;/h4>
&lt;pre>&lt;code class="language-python">#New values of x where we want to predict y
x = 1.2
&lt;/code>&lt;/pre>
&lt;pre>&lt;code class="language-python">plot_dist1D(predictive(x, 2))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_40_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist1D(predictive(x, 6))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_41_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist1D(predictive(x, 12))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_42_0.png" alt="png">&lt;/p>
&lt;h4 id="we-would-also-observe-how-the-uncertainity-changes-with-the-values-of-x">We would also observe how the uncertainity changes with the values of x&lt;/h4>
&lt;pre>&lt;code class="language-python">plot_dist1D(predictive(1.2, 12))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_44_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist1D(predictive(2, 12))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_45_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist1D(predictive(3, 12))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_46_0.png" alt="png">&lt;/p>
&lt;pre>&lt;code class="language-python">plot_dist1D(predictive(6, 12))
&lt;/code>&lt;/pre>
&lt;p>&lt;img src="output_47_0.png" alt="png">&lt;/p>
&lt;p>The predictive distribution variance grows as x increases, as expected from $\sigma_N(x)$&lt;/p></description></item><item><title>Deploying JupyterHub with Docker and CUDA support</title><link>https://earnold.me/post/jupyterhub/</link><pubDate>Mon, 09 Oct 2017 09:44:42 -0200</pubDate><guid>https://earnold.me/post/jupyterhub/</guid><description>&lt;p>People doing research nowadays can benefit a lot from using
&lt;a href="http://jupyter.org/" target="_blank" rel="noopener">Jupyter notebooks&lt;/a>. It is the case specially with machine learning and data science, where experimentation is frequent and code changes rapidly, as well as new insights are drawn from data and previous experience. In these cases it is useful to have proper documentation of the decision making process and some history of previous attempts to solve a problem.&lt;/p>
&lt;p>At
&lt;a href="http://lcs.ufsc.br/" target="_blank" rel="noopener">LCS&lt;/a> we have a server with fair computing capabilities, including a GTX 1060 GPU. We were looking for a solution that allowed to share this server&amp;rsquo;s resources without a big overhead on the user end, such as having each user installing packages and configuring their environment. The solution we envisioned was to deploy our own
&lt;a href="https://github.com/jupyterhub/jupyterhub" target="_blank" rel="noopener">JupyterHub&lt;/a> instance on the server, with the additional challenge of sharing the GPU.&lt;/p>
&lt;p>Simply put, the JupyterHub service spawns a single-user Jupyter notebook for each user that connects to the server, allowing an easy solution for resource sharing, environment configuration and user isolation. There are many options that must be configured, such as the authentication service and spawner used. Since lately I have been using Docker for service isolation I was glad to know that there was a builtin spawner that created a Docker container for each user.&lt;/p>
&lt;p>The default configuration required the JupyterHub service to be installed on the host machine and provided no isolation from other services/processes. There is the option to create a container with JupyterHub and use the Docker spawner, but it requires a Docker container to launch other containers, quite an inception, huh? To do that we share the Docker socket with the JupyterHub container so that it has access to the host Docker service, enabling it to launch other containers. Although it is not the best solution security-wise, it is the best we could do at the time.&lt;/p>
&lt;p>This solution seemed like a lot of hassle to put a service online. Fortunately, the team behind JupyterHub did a great job and provided a
&lt;a href="https://github.com/jupyterhub/jupyterhub-deploy-docker" target="_blank" rel="noopener">reference deployment&lt;/a> for a single host using Docker. It is important to note that this is not intended to be used in production, since it does not scale well for a very large number of users, in which case it would be recommended to use a kubernetes based deployment. Despite of that, as we are a small research group, this single host solution should be enough.&lt;/p>
&lt;p>The reference deployment uses docker-compose to make the service easy to setup and manage. After configuring a Github application for user authentication and the &lt;strong>Let&amp;rsquo;s Encrypt&lt;/strong> SSL keys (using
&lt;a href="https://github.com/jupyterhub/jupyterhub-deploy-docker/tree/master/examples/letsencrypt" target="_blank" rel="noopener">this script&lt;/a>), I was able to have a working service in a couple of hours. It should have taken less time, but I had to deal with
a problem using the docker-compose file specific to the &lt;strong>Let&amp;rsquo;s Encrypt&lt;/strong> configuration, check
&lt;a href="https://github.com/jupyterhub/jupyterhub-deploy-docker/pull/48" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>So we now have a working version of JupyterHub, but we still need to make the host GPU available inside the notebook container, which turns out to be the greatest challenge. There is a quite useful wrapper to do so called
&lt;a href="https://github.com/NVIDIA/nvidia-docker" target="_blank" rel="noopener">nvidia-docker&lt;/a>. Although we cannot use it directly this time since the JupyterHub&amp;rsquo;s Docker spawner plugin directly calls Docker through the API. Following Andrea Zonca&amp;rsquo;s
&lt;a href="https://zonca.github.io/2016/10/dockerspawner-cuda.html" target="_blank" rel="noopener">blog post&lt;/a> we can configure the spawner to share the GPU resources.&lt;/p>
&lt;p>First of all, we need to get the correct flags for the specific driver and devices:&lt;/p>
&lt;pre>&lt;code>curl -s localhost:3476/docker/cli
&lt;/code>&lt;/pre>
&lt;p>Which in our machine outputs&lt;/p>
&lt;pre>&lt;code>--volume-driver=nvidia-docker --volume=nvidia_driver_384.59:/usr/local/nvidia:ro --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia0
&lt;/code>&lt;/pre>
&lt;p>We then add these options to the spawner configuration in &lt;code>jupyterhub_config.py&lt;/code>, which will cause the same effect as spawning the containers using the &lt;code>nvidia-docker&lt;/code> wrapper:&lt;/p>
&lt;pre>&lt;code>c.DockerSpawner.read_only_volumes = {&amp;quot;nvidia_driver_384.59&amp;quot;:&amp;quot;/usr/local/nvidia&amp;quot;}
c.DockerSpawner.extra_host_config = { &amp;quot;devices&amp;quot;:[&amp;quot;/dev/nvidiactl&amp;quot;,&amp;quot;/dev/nvidia-uvm&amp;quot;,&amp;quot;/dev/nvidia0&amp;quot;] }
&lt;/code>&lt;/pre>
&lt;p>Please note that some of this configuration options are overwritten later in the file, so other than adding these lines you should make sure they remain active. You can also check my own version of the
&lt;a href="https://github.com/eduardohenriquearnold/jupyterhub-deploy-docker/blob/master/jupyterhub_config.py" target="_blank" rel="noopener">jupyterhub_config.py&lt;/a> file.&lt;/p>
&lt;p>Finally, the last step is to create our personalized notebook container image including all the required packages used in our research. Since we want CUDA support, we must use a CUDA enabled image as base, such as the one supported by NVIDIA &lt;code>nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04&lt;/code>. A notebook image must have a start script that will be called upon container creation to start the Jupyter notebook server. I have modified the base image to be based on the NVIDIA CUDA image, as seen
&lt;a href="https://github.com/eduardohenriquearnold/docker-stacks/tree/master/cuda-notebook" target="_blank" rel="noopener">here&lt;/a>. I have also created a specific image for our lab, inheriting from this image and containing software libraries such as OpenCV, PyTorch, TensorFlow and Keras, you can check it
&lt;a href="https://github.com/eduardohenriquearnold/docker-stacks/tree/master/deeptools-notebook" target="_blank" rel="noopener">here&lt;/a>.&lt;/p>
&lt;p>To check that everything worked as expected and the GPU is accessible from within the container you input the following on a Jupyter notebook command cell: &lt;code>!nvidia-smi&lt;/code>. It should return the status of GPU. We now have a nice platform to develop our models and share computing resources.&lt;/p></description></item><item><title>What I have learnt from dogs and cats</title><link>https://earnold.me/post/dogsvscats/</link><pubDate>Mon, 15 May 2017 14:51:59 -0300</pubDate><guid>https://earnold.me/post/dogsvscats/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>To specialize further in image classification I decided to try a classic challenge in this area: the Kaggle competition
&lt;a href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition" target="_blank" rel="noopener">Dogs vs Cats&lt;/a>. By taking part in the challenge we can easily use their data set (25k labeled images) and 12.5k unlabeled test images. Each sample from the data set is a varied-sized RGB image containing either a dog or a cat and our task is to decide to which class the sample belongs. We can also compare our results to other users and have some indicatives of where our model could improve.&lt;/p>
&lt;p>In particular, in this post I would like to address the following topics: transfer learning, the effects of the input image size and regularization on the model performance.&lt;/p>
&lt;h2 id="transfer-learning-approach">Transfer learning approach&lt;/h2>
&lt;p>Transfer learning is the technique of using previously trained models for a specific purpose on a different problem. This can be done by tuning the model parameters on the new problem data set. It is advantageous because the trained model might have valuable feature-extraction capabilities that can be useful if the application domain of the two problems are similar (i.e., object classification). So instead of training the whole model from scratch we just fine-tune the few last layers (responsible for the classification) to the new problem data set.&lt;/p>
&lt;p>Before designing my own architecture I considered fine-tuning good performing models on hard challenges such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). One of these models is the winner of the
&lt;a href="http://www.image-net.org/challenges/LSVRC/2014/" target="_blank" rel="noopener">ILSVRC 2014&lt;/a> Classification+localization challenge, VGG-16 model, organized in 5 convolutional blocks and a dense block. The convolutional blocks group 2-3 convolutional layers in sequence with a max-pooling layer to reduce dimensionality. In total there are 13 convolutional layers and 3 dense layers. The original challenge data set had 1000 classes so the output layer has 1000 output units.&lt;/p>
&lt;figure id="figure-vgg16-macroarchitecture-credits-to-davi-frossard--stanford">
&lt;a data-fancybox="" href="https://earnold.me/post/dogsvscats/vgg16_hud79ff886a16ffd04df83ec83fc5585c1_46325_2000x2000_fit_lanczos_3.png" data-caption="VGG16 Macroarchitecture. Credits to Davi Frossard @ Stanford.">
&lt;img data-src="https://earnold.me/post/dogsvscats/vgg16_hud79ff886a16ffd04df83ec83fc5585c1_46325_2000x2000_fit_lanczos_3.png" class="lazyload" alt="" width="470" height="276">
&lt;/a>
&lt;figcaption>
VGG16 Macroarchitecture. Credits to Davi Frossard @ Stanford.
&lt;/figcaption>
&lt;/figure>
&lt;p>Since we are only dealing with two classes we must adapt the architecture by replacing the three dense layers with a single dense layer containing two &lt;strong>softmax&lt;/strong> activation units. It is reasonable to replace the three layers with a single one because the model has enough complexity, so the features at this point (~ 25k of them) are already high level ones, which can easily determine the image class without further layers.&lt;/p>
&lt;p>To fine tune the model parameters to dogs-vs-cats classification we assume the 13 conv-layers provide good enough features, so we freeze their parameters and allow only the last layer parameters to be updated. This model has a large number of parameters: the 13 conv-layers provide 14,714,688 parameters while the final dense layer has 50,178 parameters. Clearly it is much simpler to fit just these final layer parameters instead of the whole network, which allows us to use our modest data set on this structure.&lt;/p>
&lt;p>The original model image input size was 224 x 224, and although this changes the dimension of the following convolutional layers, their filter sizes are kept the same, so we can still use the weights from the original model, except for the final dense layers, where the weights depend upon the number of units in each layer. Since we replaced the final dense layers we could have chosen any given input size. To keep the original design we kept the standard input dimension. Another consideration is that the pre-processing should be the same assumed by the VGG model, that is, the pixel intensities are not normalized and are kept in the original range $[0,255]$.&lt;/p>
&lt;p>In order to evaluate the model we split the labeled samples into 20k training samples and 5k test samples, since the 12.5k samples provided for evaluation are not labeled. After 10 training epochs we achieved great performance on both the training and test set, as seen on the learning curve below. You can check the notebook with this experiment
&lt;a href="https://earnold.me/notebooks/dogsvscats/vgg16.html">here&lt;/a>.&lt;/p>
&lt;p>Analyzing the results we clearly see that the model performs very well both on training and test sets with accuracy higher than 95% on the test set. This performance can be explained by the complexity of the model and the amount of information it has been trained upon. Even though it performs well and was quite simple to implement, it is a rather heavy model with ~14 million parameters. This leads to the question of whether we can get a similar performance using a simpler, more computational efficient model.&lt;/p>
&lt;h2 id="custom-model-architecture-dognet">Custom model architecture: dognet&lt;/h2>
&lt;p>Since the VGG model was successful in the previous attempt, it is worth considering its architecture as base to a simpler custom model called dognet. Similarly, we organize the convolutional blocks with two conv layers followed by a max-pooling layer, and a final single filter conv layer to merge all feature maps into one. So the new model architecture will have 7 convolutional layers, all using 3x3 filters and RELU activation, with decreasing number of filters: 32, 32, 16, 16, 4, 4, 1. After flattening the last conv layer, a dense layer with 2 outputs and &lt;strong>softmax&lt;/strong> activation is used as the network output, interpreted as class probabilities.&lt;/p>
&lt;p>A important difference in this model is that we insert a batch normalization layer after the input layer with the purpose of normalizing the input across the entire batch, which leads to faster convergence and improved training. Note that the input images pixels values are also scaled into the range of $[0,1]$ by multiplying all the channels by $\frac{1}{255}.$&lt;/p>
&lt;p>The proposed model uses 18,829 parameters, less than 1% of the amount of VGG parameters. We still assume the input image size as 224x224. The first experiment was to check the model had enough capacity to fit the data, meaning it would not underfit the data. So we trained using
&lt;a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">Adadelta&lt;/a> optimizer with no regularization for about 100 epochs. We were able to obtain 100% accuracy on the training set, which proves the model had enough capacity for the data. The performance on the test set, however, was not so satisfactory: 79% accuracy. This suggests a strong overfitting, which is expected as no regularization was used.&lt;/p>
&lt;h3 id="size-matters">Size matters?&lt;/h3>
&lt;p>An issue faced during the first experiment was the time required to train the model: using only an i5 CPU (4 cores) each epoch took about 40 minutes, so training for a 100 epochs took a few almost three days. To reduce training time the images were resized to 64x64 pixels, which had astonishing effects on the temporal performance: an epoch took only 200s to train. This input downscaling did not impact the amount of model parameters so much: there are now 17,869, opposed to the early 18,829 parameters. This is reasonable since most of the network is convolutional, so the number of parameters will not change with the input size, just on the dense layers, which happens to be the case of the final one. In this case very few parameters depend on the input size.&lt;/p>
&lt;p>This downscaled input model, with 100 epochs of training, achieved 98% accuracy on the training set and 80% accuracy on the test set, so still overfitting. This performance is very similar to the one observed on the original input size (224x224), which leads to thinking that in this case downscaling the input is a good choice since it allows faster training times without affecting the model performance.&lt;/p>
&lt;h3 id="regularization">Regularization&lt;/h3>
&lt;p>Previous training attempts resulted in model overfitting, which happens when the model performs well on the training set but poorly on the test set. It is generally caused because the model capacity is greater than the volume of data available for training, thus causing the model to specialize on fine characteristics (noise) specific to the training data and prevent the generalization expected in the test set. To overcome this problem regularization techniques must be employed to balance the model capacity and the amount available data, enforcing that simpler, more general models are obtained.&lt;/p>
&lt;p>In order to observe this effect in more detail we can observe the following figure, showing the progression of the model loss and accuracy as the training process evolve. It is clear that overfitting becomes pronounced around epoch 20, and the test accuracy becomes steady at 80% from this point forward.&lt;/p>
&lt;figure id="figure-no-regularization-training-curves">
&lt;a data-fancybox="" href="https://earnold.me/post/dogsvscats/noreg_curves_huaf098c93fa039b1ecd5816f499cb8c10_29098_2000x2000_fit_lanczos_3.png" data-caption="No regularization training curves.">
&lt;img data-src="https://earnold.me/post/dogsvscats/noreg_curves_huaf098c93fa039b1ecd5816f499cb8c10_29098_2000x2000_fit_lanczos_3.png" class="lazyload" alt="" width="648" height="288">
&lt;/a>
&lt;figcaption>
No regularization training curves.
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="l2-norm">L2 norm&lt;/h4>
&lt;p>Firstly, L2-norm regularization is applied. This method adds a regularization term on the loss function corresponding to the L2-norm metric of a layer weights, penalizing large parameters of the network, as shows the next equation, where $L_t(w)$ is the loss correspondent to training errors (in this case log-loss).&lt;/p>
&lt;p>$$ L(w) = L_t(w) + \lambda \Vert w \Vert^2 $$&lt;/p>
&lt;p>This means the model is looking for finding parameters that both result in low training error, but are also small, which means simpler models and more generalization. This trade-off is tuned through the hyper-parameter $\lambda$, with higher values prioritizing model simplification. The L2 specific effect is introducing a negative parcel to the parameters updates (coming from the gradient of the regularization term), so the weights tend to become smaller at each iteration.&lt;/p>
&lt;p>By applying this technique with moderate regularization $\lambda=10^{-3}$ on all layers, we obtained the following progression of model loss and accuracy. A note aside is that the model should be randomly initialized for training, if the previous weights (obtained without regularization) are used for initialization the model will not be able to improve generalization.&lt;/p>
&lt;figure id="figure-l2-moderate-regularization-training-curves">
&lt;a data-fancybox="" href="https://earnold.me/post/dogsvscats/l2_curves_hu6251224a84b2eeb8f0d5bbf670207ed3_34589_2000x2000_fit_lanczos_3.png" data-caption="L2 moderate regularization training curves.">
&lt;img data-src="https://earnold.me/post/dogsvscats/l2_curves_hu6251224a84b2eeb8f0d5bbf670207ed3_34589_2000x2000_fit_lanczos_3.png" class="lazyload" alt="" width="648" height="288">
&lt;/a>
&lt;figcaption>
L2 moderate regularization training curves.
&lt;/figcaption>
&lt;/figure>
&lt;p>The model still overfits, but observe that the test set accuracy has actually improved, now becoming steady at 85%, showing that regularization has improved the model generalization on new samples.&lt;/p>
&lt;p>We could try a stronger regularization with $\lambda=10^{-2}$, but perhaps we could enhance regularization by aggregating a different technique called dropout.&lt;/p>
&lt;h4 id="dropout">Dropout&lt;/h4>
&lt;p>Dropout works by deactivating a set of random units of a given layer during training phase. Such practice have a good impact on the network because it removes inter-layer co-adaptation, thus avoiding any unit to rely too much on a single previous unit, so enabling generalization. The strength of the regularization is controlled by a hyper-parameter that represents the rate of the layer units to drop. It is commonly applied before dense layers, but could also be used for convolutional ones.&lt;/p>
&lt;p>Combining both L2-norm with $\lambda=10^{-3}$ on all layers and dropout regularization with ratio 0.5 (half units dropped) on convolutional layers, we obtain the following loss and accuracy:&lt;/p>
&lt;figure id="figure-dropout-and-l2-regularization-training-curves">
&lt;a data-fancybox="" href="https://earnold.me/post/dogsvscats/drop_l2_curves_hu6860b813fdd25c9060498295d82a4fd5_39702_2000x2000_fit_lanczos_3.png" data-caption="Dropout and L2 regularization training curves.">
&lt;img data-src="https://earnold.me/post/dogsvscats/drop_l2_curves_hu6860b813fdd25c9060498295d82a4fd5_39702_2000x2000_fit_lanczos_3.png" class="lazyload" alt="" width="648" height="288">
&lt;/a>
&lt;figcaption>
Dropout and L2 regularization training curves.
&lt;/figcaption>
&lt;/figure>
&lt;p>The results show that combining L2 and dropout regularization provides a good solution to prevent model overfitting. Even though the model did not overfit we did not observe any improvement on the test set accuracy if compared to L2 only regularization. Another interesting characteristic is the chaotic behavior of the test loss function, which is due to the probabilistic nature of the unit drops, albeit the mean value of the test loss seems to follow the train loss, which is a good indicative. The notebook containing the dognet architecture along with regularizers is available
&lt;a href="https://earnold.me/notebooks/dogsvscats/dognet.html">here&lt;/a>.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In this specific application the downscaling of input image size did not have a bad impact on model performance and was actually capable of greatly decreasing the training time. Although this may not be the case when dealing with samples containing fine details where two distinct classes look similar to one another.&lt;/p>
&lt;p>Through the series of experiments applying regularization techniques it was possible to prevent model overfiting and improve generalization measured as an increase on test set accuracy, getting up to 85% accuracy. Combining both L2 and dropout proved to be a good idea.&lt;/p>
&lt;p>In order to further increase model accuracy on the test set it would be necessary to gather more training data and possibly apply data augmentation (rotation, scaling, cropping). Another possibility is to use the transfer learning approach, which uses already trained high-level models, offering higher than 96% accuracy on the test set, at the expense of having a much larger model and longer processing times.&lt;/p>
&lt;p>Finally, a note on the Kaggle competition. I think Kaggle has an important role on promoting machine learning and its applications, in engaging researchers and hobbyists alike and even helping people to learn more about recent techniques, letting them compare results, etc. Still, it seems to fail to consider the complexity of the elaborated solutions. To illustrate my point check the
&lt;a href="http://blog.kaggle.com/2017/04/20/dogs-vs-cats-redux-playground-competition-3rd-place-interview-marco-lugo/" target="_blank" rel="noopener">Marco Lugo post&lt;/a> on Kaggle blog showing his 3rd place solution to this challenge. He uses a bunch of complex convolutional models in parallel and predict the output as a weighted average of these classifiers. Of course great results come at the price of increased complexity. I am just saying that complexity should also be taken in account when evaluating model. We may ask ourselves: what is the best score I can get using only the training data they provide, without any pre-loaded models, or an ensemble of 20 complex classifiers? The way Kaggle is organized today does not allow an answer to that question.&lt;/p></description></item><item><title>Neural networks from scratch</title><link>https://earnold.me/post/nn-scratch/</link><pubDate>Wed, 12 Apr 2017 12:00:00 +0000</pubDate><guid>https://earnold.me/post/nn-scratch/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I have used a handful of machine learning models in the past. These include simple linear regression models, support vector machines (SVMs) and neural networks. While working on different projects I was mostly concerned in solving a specific application problem and did not worry about the inner workings of the models I was using. I have delved into more details of SVM during my final undergraduate project but did not have the time to do the same for neural networks.&lt;/p>
&lt;p>After reading and learning about it in more detail it has come the time for me to share it. In this post I will give some brief introduction to neural nets and derive some of the maths behind a simple architecture as well as its implementation using only
&lt;a href="http://www.numpy.org/" target="_blank" rel="noopener">Numpy&lt;/a>. I have based most of this post on
&lt;a href="http://cs231n.github.io/neural-networks-case-study/" target="_blank" rel="noopener">Stanford&amp;rsquo;s CS231&lt;/a> notes which I found very useful for beginners. Some of the math derived here was not available there so it might be useful for a more interested reader.&lt;/p>
&lt;p>Neural networks are organized in layers, each consisting of many units. These units are responsible for crunching the data. Each unit can receive many inputs but has a single output that can be connected to many more units in the following layer. In this study we will address a fully-connected structucture, meaning that every unit in a single layer receives input from all the units from the previous layer. To compute its output, each unit perform a weighted sum of its inputs and a bias term, then apply an activation funcion, in general non-linear, to the calculated sum. The model parameters consists of the connection weights between the units and their bias terms.&lt;/p>
&lt;h2 id="defining-the-model">Defining the model&lt;/h2>
&lt;p>To simplify our study we are going to use a two layer network architecture (the input layer is not counted). The input layer $x$ has $N_x$ units, the hidden layer $h$ has $N_h$ and the output layer $y$ has $N_y$ units.&lt;/p>
&lt;p>We could specify this architecture to perform multi-class classification, so the output layer would give the probability the sample belongs to each of the available classes. In this case we use $N_y$ as the number of classes and to enforce a probabilistic output we use an activation function called &lt;strong>softmax&lt;/strong> on the output layer, which also guarantess that the sum of the classes probabilities sums to 1.&lt;/p>
&lt;p>As for the hidden layer we can use a &lt;strong>RELU&lt;/strong> activation function, because it is avery simple to evaluate non-linear function: $\text{RELU}(z) = \max(0, z)$. There are other reasons why we would use it, mostly to avoid a problem called vanishing gradient, but in a shallow network such as this one this would not be a problem.&lt;/p>
&lt;p>We define the weights of the connections between the input and hidden layer units as the matrix $W1 \in \mathbb{R}^{N_x \times N_h}$, where $W1_{i,j}$ is the weight of the connection between the $i$-th input layer unit to the $j$-th hidden layer unit. The bias vector of the hidden layer is defined as $b1 \in \mathbb{R}^{N_h}$. Similarly we have the parameters of connection between the hidden layer to the output layer: $W2 \in \mathbb{R}^{N_h \times N_y}$ and $b2 \in \mathbb{R}^{N_y}.$&lt;/p>
&lt;p>&lt;img src="https://earnold.me/notebooks/nn-diagram.png" alt="nn-diagram">&lt;/p>
&lt;h2 id="forward-propagation">Forward propagation&lt;/h2>
&lt;p>Assuming we have all the ideal model parameters $W1, W2, b1, b2$, how do we get the output of the network for a given input sample? This is called forward propagation, since the data flows from the input layer, through the hidden layers and finally to the output layer as presented in the following equations.&lt;/p>
&lt;p>$$\begin{align*} \\
h &amp;amp;= \max(0,x W1 + b1) \\
y &amp;amp;= \text{softmax}(\underbrace{h W2 + b2}_\textrm{score})
\end{align*}$$&lt;/p>
&lt;p>The &lt;strong>softmax&lt;/strong> activation function can be expressed as
$$ \text{softmax}(z)_j = \frac{e^{z_j}}{\sum_{i=0}^{N_z}e^{z_i}} $$&lt;/p>
&lt;p>This formulation allows the input $x$ to have many samples, each in a row, so $x \in \mathbb{R}^{n \times N_\text{features}}$, where $n$ is the number of samples in a batch, also called batch size, and $N_\text{features}$ is the number of features, or the dimension of each sample. This would yield an output $y \in \mathbb{R}^{n \times N_y}$, where each sample probability distribution among classes is given in a row.&lt;/p>
&lt;h2 id="training-process">Training process&lt;/h2>
&lt;p>In order to get the ideal model parameters we have to train the network on a training set. This consists of an optimization process where we try to minimize a loss function that tells how close the network output $y$ is to the real labels $\hat{y}$. At each iteration of this process we obtain new values for the parameters that will hopefully decrease the value of the loss function.&lt;/p>
&lt;p>In this example, since we assumed a multi-class classification problem with a probabilistic output, the ideal loss function to use is the categorical cross-entropy function, given by
$$ L_i = -\log y_{\hat{y}_i}$$&lt;/p>
&lt;p>To make sense of this function we can analyse its behaviour. For a given sample $x_i$ belonging to class $\hat{y}_i$ it will compute the negative log of the output probability of the sample belonging to classs $\hat{y}_i$ (given by the $\hat{y}_i$-th component of $y$). Ideally this probability would be 1, which would make $L_i=0$. Whenever this is not the case, there is a loss associated with the sample $x_i$.&lt;/p>
&lt;p>Now we have a measure to evaluate how good our classifier is doing on the training set we can try to optimize the network parameters to get the loss as close to zero as possible. We do this using an iterative algorithm called &lt;strong>Stochastic Gradient Descent&lt;/strong>. To give a brief overview of this method, assume all network parameters are represented in a vector $\theta$. We can compute the variation of the loss function $\Delta L$ given a variation of the parameters vector $\Delta \theta$ as $$\Delta L = \Delta \theta \cdot \nabla L$$ where $\nabla L$ is the gradient of the loss with respect to the parameters $\theta$. We always want to descrease the loss, so we want $\Delta L &amp;lt; 0$. One way to guarantee this condition is to choose the variation of parameters as $$\Delta \theta = - \eta \nabla L$$ for a small-enough learning rate $\eta &amp;gt;0$, which would yield $$\Delta L = -\eta |\nabla L|^2.$$&lt;/p>
&lt;p>This description is for vanilla Gradient Descent, also called batch-GD (where $L = \sum L_i$), so every sample is considered in the gradient. The stochastic part comes when you only consider a single sample at each iteration step, what reduces training time, especially on big datasets. Although it may seem attractive, this method offers slower convergence since there is a lot of zig-zagging between samples optimizations. To overcome this another variation called mini-batch GD can be used. This method lies in between batch-GD (uses one sample) and SGD (uses all samples), since it considers a mini-batch of size $N_b$ to compute the gradient, thus reducing training time and still allowing faster convergence.&lt;/p>
&lt;p>By iteratively running this optimization algorithm we can reduce the loss function and train our model. There is one missing step though: how to compute the loss function gradient $\nabla L.$&lt;/p>
&lt;h2 id="backpropagation-and-gradient-computation">Backpropagation and gradient computation&lt;/h2>
&lt;p>Even considering our small network the loss is a rather complex function of the network parameters given all multiplications and non-linear activations. To compute its gradient $\nabla L$ we must find the derivative of $L$ with respect to all model parameters, which can be difficult to be done analytically. We then use an algorithm called Backpropagation, which is basically the use of calculus' chain-rule. By multiplying the local derivatives from layer to layer we can numerically evaluate the derivative of the loss with respect to any parameter.&lt;/p>
&lt;p>We start from the output layer $y$. Since $$ L_i = -\log y_{\hat{y}_i}$$ we have $$\frac{\partial L_i}{\partial y_k} = \frac{-1}{y_k} 1(\hat{y}_i=k)$$ where 1(z) is the indicator function (1 if argument true, otherwise 0).&lt;/p>
&lt;p>$\DeclareMathOperator{\score}{score}$
We then calculate the derivatives of the output $y_k$ with respect the intermediate variable $\score = h W2 + b2$, with $ y = \text{softmax}(h W2 + b2)$. We must consider two cases, one for the derivative of $y_k$ with respect to $\score_j$ with $k \neq j$ and another with $k=j$.&lt;/p>
&lt;p>For the first case we can write $$ y_k = \frac{e^{\score_k}}{e^{\score_j} + \sum_{i \neq j} e^{\score_i}}$$ then using the quotient rule for derivatives we have:
$$\frac{\partial y_k}{\partial \text{score}_j} = \frac{-e^{\score_k}e^{\score_j}}{(\sum_i e^{\score_i})^2} = -\frac{e^{\score_k}}{\sum_i e^{\score_i}} \frac{e^{\score_j}}{\sum_i e^{\score_i}} = -y_k y_j$$&lt;/p>
&lt;p>For the second case, when $k=j$ we can write $$ y_k = \frac{e^{\score_k}}{e^{\score_k} + \sum_{i \neq k} e^{\score_i}}$$ and then the derivative becomes:&lt;/p>
&lt;div>\begin{eqnarray}
\frac{\partial y_k}{\partial \text{score}_k} &amp;=&amp; \frac{e^{\score_k}(\sum_i e^{\score_i}) - e^{2\score_k}}{(\sum_i e^{\score_i})^2} \\
&amp;=&amp; \frac{e^{\score_k}}{\sum_i e^{\score_i}} - \frac{e^{\score_k}}{\sum_i e^{\score_i}} \frac{e^{\score_k}}{\sum_i e^{\score_i}} \\
&amp;=&amp; y_k-y_k y_k \\
&amp;=&amp; y_k(1-y_k)
\end{eqnarray}&lt;/div>
&lt;p>Now, to calculate the derivative of the loss with respect to the score intermediate variables we use the chain-rule as follows:&lt;/p>
&lt;div>\begin{eqnarray}
\frac{\partial L_i}{\partial \score_j} &amp; = &amp; \frac{\partial L_i}{\partial y_k} \frac{\partial y_k}{\partial \score_j} &amp;\\
&amp; = &amp; \frac{-1}{y_{\hat{y}_i}} \times -y_{\hat{y}_i} y_j = y_j, &amp;\text{ if } j \neq \hat{y}_i \\
&amp; = &amp; \frac{-1}{y_{\hat{y}_i}} \times y_{\hat{y}_i}(1-y_{\hat{y}_i}) = y_{\hat{y}_i} -1, &amp;\text{ if } j=\hat{y}_i
\end{eqnarray}&lt;/div>
&lt;p>For implementation reasons we can call a vector $\text{dscore} \in \mathbb{R}^{N_y}$ where each component is the derivative of the loss regarding a component of the score variable. In a compact form: $$ \text{dscore}_j = \frac{\partial L_i}{\partial \score_j} = y_j - 1(j=\hat{y}_i)$$&lt;/p>
&lt;p>We must now propagate this derivative to the parameters of the output layer. From definition we have $\score = h W2 + b2$. To improve visualization we can expand it in the form (considering a single sample batch, n=1):&lt;/p>
&lt;p>$$\begin{align*}
\score_1 &amp;amp;= h_1 W2_{11} + h_2 W2_{21} + h_3 W2_{31} + \cdots + b2_1 \\
\score_2 &amp;amp;= h_1 W2_{12} + h_2 W2_{22} + h_3 W2_{32} + \cdots + b2_2 \\
\score_3 &amp;amp;= h_1 W2_{13} + h_2 W2_{23} + h_3 W2_{33} + \cdots + b2_3 \\
&amp;amp; \vdots &amp;amp; \\
\score_j &amp;amp;= h_1 W2_{1j} + h_2 W2_{2j} + h_3 W2_{3j} + \cdots + b2_j \\
\end{align*}$$&lt;/p>
&lt;p>It is easy to see that $$\frac{\partial \score_j}{\partial b2_j} = 1 \implies \frac{\partial L_i}{\partial b2_j} = \frac{\partial L_i}{\partial \score_j} \frac{\partial \score_j}{\partial b2_j} = \frac{\partial L_i}{\partial \score_j}.$$ This imples that the vector of weights update for $b2$ is $db2 = \text{dscore}$&lt;/p>
&lt;p>Similarly, we have $$ \frac{\partial \score_j}{\partial W2_{kj}} = h_k \implies \frac{\partial L_i}{\partial W2_{kj}} = \frac{\partial L_i}{\partial \score_j} \frac{\partial \score_j}{\partial W2_{kj}} = h_k \frac{\partial L_i}{\partial \score_j}$$&lt;/p>
&lt;p>In this case, the matrix of weights updates is given by&lt;/p>
&lt;p>$$\begin{align*}
dW2 &amp;amp;= \begin{pmatrix} \
\frac{\partial L}{\partial W_{11}} &amp;amp; \frac{\partial L}{\partial W_{12}} &amp;amp; \cdots \\
\frac{\partial L}{\partial W_{21}} &amp;amp; \frac{\partial L}{\partial W_{22}} &amp;amp; \cdots \\
\vdots &amp;amp; \vdots &amp;amp; \ddots \\
\end{pmatrix} \\
&amp;amp;= \begin{pmatrix} \
h_1 \text{dscore}_1 &amp;amp; h_1 \text{dscore}_2 &amp;amp; \cdots \\
h_2 \text{dscore}_1 &amp;amp; h_2 \text{dscore}_2 &amp;amp; \cdots \\
\vdots &amp;amp; \vdots &amp;amp; \ddots \\
\end{pmatrix} \\
&amp;amp; = \begin{pmatrix} \
h_1 \\
h_2 \\
\vdots \\
\end{pmatrix} \
\begin{pmatrix} \
\text{dscore}_1 &amp;amp; \text{dscore}_2 &amp;amp; \cdots \\
\end{pmatrix} \\
&amp;amp; = h^T \text{dscore} \\
\end{align*}$$&lt;/p>
&lt;p>To propagate the gradient to the hidden layer we must first calculate the gradient with respect to $h$: $$\frac{\partial \score_j}{\partial h_k} = W2_{kj}$$&lt;/p>
&lt;p>The differential parameter vector $dh$ is given by:&lt;/p>
&lt;p>$$\begin{align*}
dh^T &amp;amp;= \begin{pmatrix} \
\frac{\partial L}{\partial h_1} \\
\frac{\partial L}{\partial h_2} \\
\vdots \\
\end{pmatrix} \\
&amp;amp;= \begin{pmatrix} \
\frac{\partial L}{\partial\score_1}\frac{\partial \score_1}{\partial h_1}+ \frac{\partial L}{\partial\score_2}\frac{\partial \score_2}{\partial h_1} + \cdots\\
\frac{\partial L}{\partial\score_1}\frac{\partial \score_1}{\partial h_2}+ \frac{\partial L}{\partial\score_2}\frac{\partial \score_2}{\partial h_2} + \cdots\\
\vdots \\
\end{pmatrix}\\
&amp;amp;= \begin{pmatrix} \
\frac{\partial \score_1}{\partial h_1} &amp;amp; \frac{\partial \score_2}{\partial h_1} &amp;amp; \cdots\\
\frac{\partial \score_1}{\partial h_2} &amp;amp; \frac{\partial \score_2}{\partial h_2} &amp;amp; \cdots\\
\vdots &amp;amp; \vdots &amp;amp; \ddots \\
\end{pmatrix} \
\begin{pmatrix} \
\frac{\partial L}{\partial\score_1}\\
\frac{\partial L}{\partial\score_2}\\
\vdots \\
\end{pmatrix} \\
&amp;amp;= W2 \times \text{dscore}^T\\
dh &amp;amp;= \text{dscore} \times W2^T \\
\end{align*}$$&lt;/p>
&lt;p>Next we consider the RELU activation: $h=max(0, \underbrace{xW1+b1}_\textrm{r})$. $\frac{dh}{dr} = 1(r&amp;gt;0)$. Thus, $dr = dh \times 1(h&amp;gt;0)$.&lt;/p>
&lt;p>Finally, for the hidden layer, we can observe $dh$ as the output and $x$ as input, so by extending the equations for $dW2$ and $db2$ we have&lt;/p>
&lt;div>$$\begin{align*}
dW1 &amp;= x^T \text{dr} \\
db1 &amp;= \text{dr}
\end{align*}$$&lt;/div>
&lt;h2 id="numpy-implementation">Numpy implementation&lt;/h2>
&lt;p>The forward propagation is straightforward as can be seen below.&lt;/p>
&lt;pre>&lt;code class="language-python">h = np.maximum(0, np.dot(x,W1)+b1)
score = np.dot(h,W2)+b2
y = np.exp(score)
y /= np.sum(y, axis=1, keepdims=True)
&lt;/code>&lt;/pre>
&lt;p>Although understanding backpropagation can be difficult, the resulting equations are somewhat simple to implement, because they only need to consider the local derivatives at each step.&lt;/p>
&lt;p>Primarily we computer the $\text{dscore}$ intermediate variable, where y_t represents $\hat{y}$, the true class label.&lt;/p>
&lt;pre>&lt;code class="language-python">dscore = np.copy(y)
dscore[range(n), y_t] -= 1
dscore /= n
&lt;/code>&lt;/pre>
&lt;p>Next the output layer parameters updates are calculated. Since we now have $n$ training samples, we have $L = \frac{1}{n} \sum_{i=0}^n L_i$, so dscore gets summed (the division by $n$ has already taken place in the previous step).&lt;/p>
&lt;pre>&lt;code class="language-python">dW2 = np.dot(h.T, dscore)
db2 = np.sum(dscore, axis=0, keepdims=True)
&lt;/code>&lt;/pre>
&lt;p>The hidden layer activation derivative is calculated, followed by the parameter updates.&lt;/p>
&lt;pre>&lt;code class="language-python">dh = np.dot(dscore, W2.T)
dr = dh
dr[h &amp;lt;= 0] = 0
dW1 = np.dot(x.T, dr)
db1 = np.sum(dr, axis=0, keepdims=True)
&lt;/code>&lt;/pre>
&lt;p>Finally we apply the weight updates using the a specified learning rate $\eta$ (lr).&lt;/p>
&lt;pre>&lt;code class="language-python">W1 -= lr*dW1
b1 -= lr*db1
W2 -= lr*dW2
b2 -= lr*db2
&lt;/code>&lt;/pre>
&lt;h2 id="source-code">Source code&lt;/h2>
&lt;p>To check the code and results, please visit
&lt;a href="https://earnold.me/notebooks/nn-scratch.html">this notebook&lt;/a>.&lt;/p></description></item></channel></rss>