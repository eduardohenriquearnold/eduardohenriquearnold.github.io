<!doctype html><html lang=en-us>
<head>
<meta charset=utf-8>
<meta name=viewport content="width=device-width,initial-scale=1">
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=generator content="Source Themes Academic 4.8.0">
<meta name=author content="Eduardo Arnold">
<meta name=description content="Despite the popularity of standard linear regression, its bayesian counterpart is not as well-known. In this tutorial we explore its benefits and learn how to build it from scratch in Python with NumPy.">
<link rel=alternate hreflang=en-us href=https://earnold.me/post/bayesianlr/>
<meta name=theme-color content="#2962ff">
<script src=/js/mathjax-config.js></script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin=anonymous>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin=anonymous>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css crossorigin=anonymous title=hl-light>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled>
<script src=https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin=anonymous async></script>
<script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script>
<link rel=stylesheet href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
<link rel=stylesheet href=/css/academic.css>
<link rel=manifest href=/index.webmanifest>
<link rel=icon type=image/png href=/images/icon_hu86725dfa3481f0beabea4a23db3db448_10764_32x32_fill_lanczos_center_3.png>
<link rel=apple-touch-icon type=image/png href=/images/icon_hu86725dfa3481f0beabea4a23db3db448_10764_192x192_fill_lanczos_center_3.png>
<link rel=canonical href=https://earnold.me/post/bayesianlr/>
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:site" content="@eduardoarnoldh">
<meta property="twitter:creator" content="@eduardoarnoldh">
<meta property="og:site_name" content="Eduardo Arnold">
<meta property="og:url" content="https://earnold.me/post/bayesianlr/">
<meta property="og:title" content="Bayesian Linear Regression | Eduardo Arnold">
<meta property="og:description" content="Despite the popularity of standard linear regression, its bayesian counterpart is not as well-known. In this tutorial we explore its benefits and learn how to build it from scratch in Python with NumPy."><meta property="og:image" content="https://earnold.me/post/bayesianlr/featured.png">
<meta property="twitter:image" content="https://earnold.me/post/bayesianlr/featured.png"><meta property="og:locale" content="en-us">
<meta property="article:published_time" content="2019-03-12T20:29:55+01:00">
<meta property="article:modified_time" content="2019-03-12T20:29:55+01:00">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://earnold.me/post/bayesianlr/"},"headline":"Bayesian Linear Regression","image":["https://earnold.me/post/bayesianlr/featured.png"],"datePublished":"2019-03-12T20:29:55+01:00","dateModified":"2019-03-12T20:29:55+01:00","author":{"@type":"Person","name":"Eduardo Arnold"},"publisher":{"@type":"Organization","name":"Eduardo Arnold","logo":{"@type":"ImageObject","url":"https://earnold.me/images/icon_hu86725dfa3481f0beabea4a23db3db448_10764_192x192_fill_lanczos_center_3.png"}},"description":"Despite the popularity of standard linear regression, its bayesian counterpart is not as well-known. In this tutorial we explore its benefits and learn how to build it from scratch in Python with NumPy."}</script>
<title>Bayesian Linear Regression | Eduardo Arnold</title>
</head>
<body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents>
<aside class=search-results id=search>
<div class=container>
<section class=search-header>
<div class="row no-gutters justify-content-between mb-3">
<div class=col-6>
<h1>Search</h1>
</div>
<div class="col-6 col-search-close">
<a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a>
</div>
</div>
<div id=search-box>
</div>
</section>
<section class=section-search-results>
<div id=search-hits>
</div>
</section>
</div>
</aside>
<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main>
<div class=container>
<div class="d-none d-lg-inline-flex">
<a class=navbar-brand href=/>Eduardo Arnold</a>
</div>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span>
</button>
<div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
<a class=navbar-brand href=/>Eduardo Arnold</a>
</div>
<div class="navbar-collapse main-menu-item collapse justify-content-end" id=navbar-content>
<ul class="navbar-nav d-md-inline-flex">
<li class=nav-item>
<a class=nav-link href=/#about><span>Home</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#publications><span>Publications</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#posts><span>Posts</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#projects><span>Projects</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#talks><span>Talks</span></a>
</li>
<li class=nav-item>
<a class=nav-link href=/#contact><span>Contact</span></a>
</li>
</ul>
</div>
<ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
</ul>
</div>
</nav>
<article class=article>
<div class="article-container pt-3">
<h1>Bayesian Linear Regression</h1>
<div class=article-metadata>
<span class=article-date>
Mar 12, 2019
</span>
<span class=middot-divider></span>
<span class=article-reading-time>
4 min read
</span>
</div>
</div>
<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style=max-width:404px;max-height:280px>
<div style=position:relative>
<img src=/post/bayesianlr/featured.png alt class=featured-image>
</div>
</div>
<div class=article-container>
<div class=article-style>
<p>The bayesian linear regression formulation allows to obtain uncertainty estimates for the predictive distribution that are not available in its point-wise estimate counterpart. This notebook is based on Chapter 3 of Bishop&rsquo;s Pattern Recognition and Machine Learning book.</p>
<p>This notebook can be downloaded
<a href=/notebooks/bayesianlinearregression.ipynb>here</a>.</p>
<pre><code class=language-python>import numpy as np
from scipy.stats import multivariate_normal
import matplotlib.pyplot as plt
%matplotlib inline
</code></pre>
<h3 id=generate-sample-dataset>Generate sample dataset</h3>
<p>Generate N pairs $(x_i,y_i)$ with gaussian noise and $x_i$ sampled from uniform distribution</p>
<pre><code class=language-python>N = 12
sigma = 0.1
</code></pre>
<pre><code class=language-python>x = np.random.uniform(low=-1, high=1, size=N)
n = np.random.normal(loc=0, scale=sigma, size=N)
y = 0.3*x -0.8 +n
</code></pre>
<pre><code class=language-python>plt.plot(x,y, 'r.');
plt.show()
</code></pre>
<p><img src=output_7_0.png alt=png></p>
<h2 id=point-estimate>Point estimate</h2>
<p>We are trying to design a model $\hat{y} = x w_1 + w_0 + \epsilon$ with $\epsilon \sim N(0, \sigma^2)$</p>
<p>Note that this model and noise assumption result in the following likelihood function: $$p(\hat{y}|x,w) = N(xw_1+w_0, \sigma)$$</p>
<p>In general we aim for the Lease Squares (LS) solution: $$\min_w \sum_i (y_i-\hat{y}_i)^2$$</p>
<p>Note that the LS solution is equivalent to the Maximum Likelihood Estimator. The solution can be obtained through minimizing the loss function through Gradient Descent. However, in the case of this simple linear model it is possible to use normal equations (closed form minimization result): $$\hat{w} = (X^TX)^{-1}X^Ty$$</p>
<pre><code class=language-python>X = np.zeros((x.shape[0], 2))
X[:,0] = x
X[:,1] = 1
X
</code></pre>
<pre><code>array([[ 0.07747538,  1.        ],
       [-0.72983355,  1.        ],
       [-0.08385175,  1.        ],
       [ 0.04152017,  1.        ],
       [-0.27236207,  1.        ],
       [-0.16471106,  1.        ],
       [ 0.43409736,  1.        ],
       [-0.33582112,  1.        ],
       [-0.48323886,  1.        ],
       [ 0.54369188,  1.        ],
       [-0.29194542,  1.        ],
       [ 0.98406384,  1.        ]])
</code></pre>
<pre><code class=language-python>w = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)), X.T), y)
w
</code></pre>
<pre><code>array([ 0.28106915, -0.75913605])
</code></pre>
<p>However, this solution only provides a point estimate and lacks uncertainity information.</p>
<h2 id=bayesian-inference>Bayesian inference</h2>
<p>In turn, a bayesian approach treat $w$ as a RV which has a prior. Then, bayesian inference is used to obtain the posterior $p(w|X,Y)$ given observations</p>
<p>In order to keep the solutions in closed-form, we use a Gaussian prior, allowing for a conjugate prior, for the vector $w$ $$w \sim N(w| m_0, S_0)$$</p>
<p>Which then results in a Gaussian posterior</p>
<p>$$p(w|X,Y) = \frac{p(Y|X,w)p(w)}{p(Y|X)} = N(w| m_N, S_N)$$ where $m_N = S_N (S_0^{-1}m_0+\frac{1}{\sigma}X^Ty)$ and $S_N^{-1} = S_0^{-1}+\frac{1}{\sigma}X^TX$</p>
<p>For simplicity, let&rsquo;s assume $m_0 = 0$ and $S_0 = \alpha^{-1}I = 0.5I$</p>
<pre><code class=language-python>#prior parameters
a = 0.2
m0 = np.zeros(2)
</code></pre>
<pre><code class=language-python>def getPosterior(n):
    #Get n points from sample dataset
    x_ = X[:n]
    y_ = y[:n]
    
    #Covariance Matrix
    S0I = a*np.identity(2)
    SnI = S0I+ 1/sigma*np.dot(x_.T,x_)
    Sn = np.linalg.inv(SnI)
    
    #Mean
    tt = np.dot(S0I, m0) + 1/sigma*np.dot(x_.T,y_)
    Mn = np.dot(Sn, tt)
    return multivariate_normal(mean=Mn, cov=Sn)
</code></pre>
<pre><code class=language-python>def plot_dist2D(dist):
    x, y = np.mgrid[-1:1:.01, -1:1:.01]
    pos = np.empty(x.shape + (2,))
    pos[:, :, 0] = y; pos[:, :, 1] = x
    plt.contourf(x, y, dist.pdf(pos))
    plt.title('Posterior Distribution $p(w|X,Y)$')
    plt.xlabel('w0')
    plt.ylabel('w1')
</code></pre>
<h4 id=posterior-distribution-plots>Posterior distribution plots</h4>
<p>We can plot the posterior after aggregating different number of points. Observe how the posterior distributions become narrower when more observation are aggregated</p>
<pre><code class=language-python>plot_dist2D(getPosterior(1))
</code></pre>
<p><img src=output_27_0.png alt=png></p>
<pre><code class=language-python>plot_dist2D(getPosterior(4))
</code></pre>
<p><img src=output_28_0.png alt=png></p>
<pre><code class=language-python>plot_dist2D(getPosterior(6))
</code></pre>
<p><img src=output_29_0.png alt=png></p>
<pre><code class=language-python>plot_dist2D(getPosterior(10))
</code></pre>
<p><img src=output_30_0.png alt=png></p>
<p>The full posterior (when all points are incorporated) will have a peak on the mean, $w_{MAP} = m_N$, given the Gaussian distribution. In the case where the prior $p(w)$ is infinitely spread ($a \to 0$), $w_{MAP} = m_N = w_{ML} = (X^TX)^{-1}X^Ty$</p>
<h4 id=the-predictive-distribution>The predictive distribution</h4>
<p>Although we have estimated the posterior of parameters $w$, we are primarily interested in predicting the value of $\hat{y}$ for new sample x: $$p(\hat{y}| x, X,Y) = \int p(y|w)p(w|X,Y) dw$$</p>
<p>Given the likelihood and posterior following Gaussian distributions, this predicitive distribution is also Gaussian: $$p(\hat{y}| x, X,Y) = N(\hat{y}| m_N^Tx, \sigma_N^2(x))$$ where $ \sigma_N^2(x) = \sigma^2 + x^TS_Nx $</p>
<p>Note that the variance of the predictive distribution depends both on the assumed noise model ($\sigma$) and the uncertainty on the $w$ posterior</p>
<pre><code class=language-python>def predictive(x, nTrainingSamples):
    xp = np.zeros((2,1))
    xp[0,0] = x
    xp[1,0] = 1
    xp = np.matrix(xp)
    
    #Get posterior given nTrainingSamples
    posterior = getPosterior(nTrainingSamples)
    Mn = np.matrix(posterior.mean)
    Sn = np.matrix(posterior.cov)
    
    #Predictive mean   
    m = np.matmul(Mn,xp)
    
    #Predictive cov
    s = sigma**2 + np.dot(xp.T, np.dot(Sn,xp))
    return multivariate_normal(mean=m, cov=s)    
</code></pre>
<pre><code class=language-python>def plot_dist1D(dist):
    x = np.linspace(-4,4, 100)
    y = dist.pdf(x)
    plt.plot(y,x)
    plt.title('Predictive Distribution $p(\hat{y}|x, X,Y)$')
    plt.xlabel('pdf')
    plt.ylabel('$\hat{y}$')
</code></pre>
<h4 id=we-now-observe-how-the-predictive-distributions-become-more-certain-as-more-training-data-is-obtained>We now observe how the predictive distributions become more certain as more training data is obtained</h4>
<pre><code class=language-python>#New values of x where we want to predict y
x = 1.2
</code></pre>
<pre><code class=language-python>plot_dist1D(predictive(x, 2))
</code></pre>
<p><img src=output_40_0.png alt=png></p>
<pre><code class=language-python>plot_dist1D(predictive(x, 6))
</code></pre>
<p><img src=output_41_0.png alt=png></p>
<pre><code class=language-python>plot_dist1D(predictive(x, 12))
</code></pre>
<p><img src=output_42_0.png alt=png></p>
<h4 id=we-would-also-observe-how-the-uncertainity-changes-with-the-values-of-x>We would also observe how the uncertainity changes with the values of x</h4>
<pre><code class=language-python>plot_dist1D(predictive(1.2, 12))
</code></pre>
<p><img src=output_44_0.png alt=png></p>
<pre><code class=language-python>plot_dist1D(predictive(2, 12))
</code></pre>
<p><img src=output_45_0.png alt=png></p>
<pre><code class=language-python>plot_dist1D(predictive(3, 12))
</code></pre>
<p><img src=output_46_0.png alt=png></p>
<pre><code class=language-python>plot_dist1D(predictive(6, 12))
</code></pre>
<p><img src=output_47_0.png alt=png></p>
<p>The predictive distribution variance grows as x increases, as expected from $\sigma_N(x)$</p>
</div>
<div class="media author-card content-widget-hr">
<img class="avatar mr-3 avatar-circle" src=/author/eduardo-arnold/avatar_hu0560068c008c418db3fb9e5dbdd74d76_96460_270x270_fill_q90_lanczos_center.jpg alt="Eduardo Arnold">
<div class=media-body>
<h5 class=card-title><a href=https://earnold.me/>Eduardo Arnold</a></h5>
<h6 class=card-subtitle>PhD Canditate</h6>
<p class=card-text>I&rsquo;m a PhD candidate with the Intelligent Vehicles group at the University of Warwick. My research is focused on perception methods for autonomous driving, particularly cooperative 3D object detection.</p>
<ul class=network-icon aria-hidden=true>
<li>
<a href=/#contact>
<i class="fas fa-envelope"></i>
</a>
</li>
<li>
<a href=https://twitter.com/eduardoarnoldh target=_blank rel=noopener>
<i class="fab fa-twitter"></i>
</a>
</li>
<li>
<a href="https://scholar.google.com/citations?user=IRSg76kAAAAJ" target=_blank rel=noopener>
<i class="ai ai-google-scholar"></i>
</a>
</li>
<li>
<a href=https://github.com/eduardohenriquearnold target=_blank rel=noopener>
<i class="fab fa-github"></i>
</a>
</li>
<li>
<a href=https://linkedin.com/in/eduardohenriquearnold target=_blank rel=noopener>
<i class="fab fa-linkedin"></i>
</a>
</li>
</ul>
</div>
</div>
</div>
</article>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js></script>
<script>const code_highlighting=!0</script>
<script>const isSiteThemeDark=!1</script>
<script src=/js/academic.min.f30a8b5a880e824aab9653942c89109b.js></script>
<div class=container>
<footer class=site-footer>
<p class=powered-by>
</p>
<p class=powered-by>
Powered by the
<a href=https://sourcethemes.com/academic/ target=_blank rel=noopener>Academic theme</a> for
<a href=https://gohugo.io target=_blank rel=noopener>Hugo</a>.
<span class=float-right aria-hidden=true>
<a href=# class=back-to-top>
<span class=button_icon>
<i class="fas fa-chevron-up fa-2x"></i>
</span>
</a>
</span>
</p>
</footer>
</div>
<div id=modal class="modal fade" role=dialog>
<div class=modal-dialog>
<div class=modal-content>
<div class=modal-header>
<h5 class=modal-title>Cite</h5>
<button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span>
</button>
</div>
<div class=modal-body>
<pre><code class="tex hljs"></code></pre>
</div>
<div class=modal-footer>
<a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank>
<i class="fas fa-copy"></i> Copy
</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank>
<i class="fas fa-download"></i> Download
</a>
<div id=modal-error></div>
</div>
</div>
</div>
</div>
</body>
</html>