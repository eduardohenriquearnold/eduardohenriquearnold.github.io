<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>machine learning | Eduardo Arnold</title><link>https://earnold.me/tag/machine-learning/</link><atom:link href="https://earnold.me/tag/machine-learning/index.xml" rel="self" type="application/rss+xml"/><description>machine learning</description><generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 15 May 2017 14:51:59 -0300</lastBuildDate><image><url>https://earnold.me/images/icon_hu86725dfa3481f0beabea4a23db3db448_10764_512x512_fill_lanczos_center_3.png</url><title>machine learning</title><link>https://earnold.me/tag/machine-learning/</link></image><item><title>What I have learnt from dogs and cats</title><link>https://earnold.me/post/dogsvscats/</link><pubDate>Mon, 15 May 2017 14:51:59 -0300</pubDate><guid>https://earnold.me/post/dogsvscats/</guid><description>&lt;h2 id="motivation">Motivation&lt;/h2>
&lt;p>To specialize further in image classification I decided to try a classic challenge in this area: the Kaggle competition
&lt;a href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition" target="_blank" rel="noopener">Dogs vs Cats&lt;/a>. By taking part in the challenge we can easily use their data set (25k labeled images) and 12.5k unlabeled test images. Each sample from the data set is a varied-sized RGB image containing either a dog or a cat and our task is to decide to which class the sample belongs. We can also compare our results to other users and have some indicatives of where our model could improve.&lt;/p>
&lt;p>In particular, in this post I would like to address the following topics: transfer learning, the effects of the input image size and regularization on the model performance.&lt;/p>
&lt;h2 id="transfer-learning-approach">Transfer learning approach&lt;/h2>
&lt;p>Transfer learning is the technique of using previously trained models for a specific purpose on a different problem. This can be done by tuning the model parameters on the new problem data set. It is advantageous because the trained model might have valuable feature-extraction capabilities that can be useful if the application domain of the two problems are similar (i.e., object classification). So instead of training the whole model from scratch we just fine-tune the few last layers (responsible for the classification) to the new problem data set.&lt;/p>
&lt;p>Before designing my own architecture I considered fine-tuning good performing models on hard challenges such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). One of these models is the winner of the
&lt;a href="http://www.image-net.org/challenges/LSVRC/2014/" target="_blank" rel="noopener">ILSVRC 2014&lt;/a> Classification+localization challenge, VGG-16 model, organized in 5 convolutional blocks and a dense block. The convolutional blocks group 2-3 convolutional layers in sequence with a max-pooling layer to reduce dimensionality. In total there are 13 convolutional layers and 3 dense layers. The original challenge data set had 1000 classes so the output layer has 1000 output units.&lt;/p>
&lt;figure id="figure-vgg16-macroarchitecture-credits-to-davi-frossard--stanford">
&lt;a data-fancybox="" href="https://earnold.me/post/dogsvscats/vgg16_hud79ff886a16ffd04df83ec83fc5585c1_46325_2000x2000_fit_lanczos_3.png" data-caption="VGG16 Macroarchitecture. Credits to Davi Frossard @ Stanford.">
&lt;img data-src="https://earnold.me/post/dogsvscats/vgg16_hud79ff886a16ffd04df83ec83fc5585c1_46325_2000x2000_fit_lanczos_3.png" class="lazyload" alt="" width="470" height="276">
&lt;/a>
&lt;figcaption>
VGG16 Macroarchitecture. Credits to Davi Frossard @ Stanford.
&lt;/figcaption>
&lt;/figure>
&lt;p>Since we are only dealing with two classes we must adapt the architecture by replacing the three dense layers with a single dense layer containing two &lt;strong>softmax&lt;/strong> activation units. It is reasonable to replace the three layers with a single one because the model has enough complexity, so the features at this point (~ 25k of them) are already high level ones, which can easily determine the image class without further layers.&lt;/p>
&lt;p>To fine tune the model parameters to dogs-vs-cats classification we assume the 13 conv-layers provide good enough features, so we freeze their parameters and allow only the last layer parameters to be updated. This model has a large number of parameters: the 13 conv-layers provide 14,714,688 parameters while the final dense layer has 50,178 parameters. Clearly it is much simpler to fit just these final layer parameters instead of the whole network, which allows us to use our modest data set on this structure.&lt;/p>
&lt;p>The original model image input size was 224 x 224, and although this changes the dimension of the following convolutional layers, their filter sizes are kept the same, so we can still use the weights from the original model, except for the final dense layers, where the weights depend upon the number of units in each layer. Since we replaced the final dense layers we could have chosen any given input size. To keep the original design we kept the standard input dimension. Another consideration is that the pre-processing should be the same assumed by the VGG model, that is, the pixel intensities are not normalized and are kept in the original range $[0,255]$.&lt;/p>
&lt;p>In order to evaluate the model we split the labeled samples into 20k training samples and 5k test samples, since the 12.5k samples provided for evaluation are not labeled. After 10 training epochs we achieved great performance on both the training and test set, as seen on the learning curve below. You can check the notebook with this experiment
&lt;a href="https://earnold.me/notebooks/dogsvscats/vgg16.html">here&lt;/a>.&lt;/p>
&lt;p>Analyzing the results we clearly see that the model performs very well both on training and test sets with accuracy higher than 95% on the test set. This performance can be explained by the complexity of the model and the amount of information it has been trained upon. Even though it performs well and was quite simple to implement, it is a rather heavy model with ~14 million parameters. This leads to the question of whether we can get a similar performance using a simpler, more computational efficient model.&lt;/p>
&lt;h2 id="custom-model-architecture-dognet">Custom model architecture: dognet&lt;/h2>
&lt;p>Since the VGG model was successful in the previous attempt, it is worth considering its architecture as base to a simpler custom model called dognet. Similarly, we organize the convolutional blocks with two conv layers followed by a max-pooling layer, and a final single filter conv layer to merge all feature maps into one. So the new model architecture will have 7 convolutional layers, all using 3x3 filters and RELU activation, with decreasing number of filters: 32, 32, 16, 16, 4, 4, 1. After flattening the last conv layer, a dense layer with 2 outputs and &lt;strong>softmax&lt;/strong> activation is used as the network output, interpreted as class probabilities.&lt;/p>
&lt;p>A important difference in this model is that we insert a batch normalization layer after the input layer with the purpose of normalizing the input across the entire batch, which leads to faster convergence and improved training. Note that the input images pixels values are also scaled into the range of $[0,1]$ by multiplying all the channels by $\frac{1}{255}.$&lt;/p>
&lt;p>The proposed model uses 18,829 parameters, less than 1% of the amount of VGG parameters. We still assume the input image size as 224x224. The first experiment was to check the model had enough capacity to fit the data, meaning it would not underfit the data. So we trained using
&lt;a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">Adadelta&lt;/a> optimizer with no regularization for about 100 epochs. We were able to obtain 100% accuracy on the training set, which proves the model had enough capacity for the data. The performance on the test set, however, was not so satisfactory: 79% accuracy. This suggests a strong overfitting, which is expected as no regularization was used.&lt;/p>
&lt;h3 id="size-matters">Size matters?&lt;/h3>
&lt;p>An issue faced during the first experiment was the time required to train the model: using only an i5 CPU (4 cores) each epoch took about 40 minutes, so training for a 100 epochs took a few almost three days. To reduce training time the images were resized to 64x64 pixels, which had astonishing effects on the temporal performance: an epoch took only 200s to train. This input downscaling did not impact the amount of model parameters so much: there are now 17,869, opposed to the early 18,829 parameters. This is reasonable since most of the network is convolutional, so the number of parameters will not change with the input size, just on the dense layers, which happens to be the case of the final one. In this case very few parameters depend on the input size.&lt;/p>
&lt;p>This downscaled input model, with 100 epochs of training, achieved 98% accuracy on the training set and 80% accuracy on the test set, so still overfitting. This performance is very similar to the one observed on the original input size (224x224), which leads to thinking that in this case downscaling the input is a good choice since it allows faster training times without affecting the model performance.&lt;/p>
&lt;h3 id="regularization">Regularization&lt;/h3>
&lt;p>Previous training attempts resulted in model overfitting, which happens when the model performs well on the training set but poorly on the test set. It is generally caused because the model capacity is greater than the volume of data available for training, thus causing the model to specialize on fine characteristics (noise) specific to the training data and prevent the generalization expected in the test set. To overcome this problem regularization techniques must be employed to balance the model capacity and the amount available data, enforcing that simpler, more general models are obtained.&lt;/p>
&lt;p>In order to observe this effect in more detail we can observe the following figure, showing the progression of the model loss and accuracy as the training process evolve. It is clear that overfitting becomes pronounced around epoch 20, and the test accuracy becomes steady at 80% from this point forward.&lt;/p>
&lt;figure id="figure-no-regularization-training-curves">
&lt;a data-fancybox="" href="https://earnold.me/post/dogsvscats/noreg_curves_huaf098c93fa039b1ecd5816f499cb8c10_29098_2000x2000_fit_lanczos_3.png" data-caption="No regularization training curves.">
&lt;img data-src="https://earnold.me/post/dogsvscats/noreg_curves_huaf098c93fa039b1ecd5816f499cb8c10_29098_2000x2000_fit_lanczos_3.png" class="lazyload" alt="" width="648" height="288">
&lt;/a>
&lt;figcaption>
No regularization training curves.
&lt;/figcaption>
&lt;/figure>
&lt;h4 id="l2-norm">L2 norm&lt;/h4>
&lt;p>Firstly, L2-norm regularization is applied. This method adds a regularization term on the loss function corresponding to the L2-norm metric of a layer weights, penalizing large parameters of the network, as shows the next equation, where $L_t(w)$ is the loss correspondent to training errors (in this case log-loss).&lt;/p>
&lt;p>$$ L(w) = L_t(w) + \lambda \Vert w \Vert^2 $$&lt;/p>
&lt;p>This means the model is looking for finding parameters that both result in low training error, but are also small, which means simpler models and more generalization. This trade-off is tuned through the hyper-parameter $\lambda$, with higher values prioritizing model simplification. The L2 specific effect is introducing a negative parcel to the parameters updates (coming from the gradient of the regularization term), so the weights tend to become smaller at each iteration.&lt;/p>
&lt;p>By applying this technique with moderate regularization $\lambda=10^{-3}$ on all layers, we obtained the following progression of model loss and accuracy. A note aside is that the model should be randomly initialized for training, if the previous weights (obtained without regularization) are used for initialization the model will not be able to improve generalization.&lt;/p>
&lt;figure id="figure-l2-moderate-regularization-training-curves">
&lt;a data-fancybox="" href="https://earnold.me/post/dogsvscats/l2_curves_hu6251224a84b2eeb8f0d5bbf670207ed3_34589_2000x2000_fit_lanczos_3.png" data-caption="L2 moderate regularization training curves.">
&lt;img data-src="https://earnold.me/post/dogsvscats/l2_curves_hu6251224a84b2eeb8f0d5bbf670207ed3_34589_2000x2000_fit_lanczos_3.png" class="lazyload" alt="" width="648" height="288">
&lt;/a>
&lt;figcaption>
L2 moderate regularization training curves.
&lt;/figcaption>
&lt;/figure>
&lt;p>The model still overfits, but observe that the test set accuracy has actually improved, now becoming steady at 85%, showing that regularization has improved the model generalization on new samples.&lt;/p>
&lt;p>We could try a stronger regularization with $\lambda=10^{-2}$, but perhaps we could enhance regularization by aggregating a different technique called dropout.&lt;/p>
&lt;h4 id="dropout">Dropout&lt;/h4>
&lt;p>Dropout works by deactivating a set of random units of a given layer during training phase. Such practice have a good impact on the network because it removes inter-layer co-adaptation, thus avoiding any unit to rely too much on a single previous unit, so enabling generalization. The strength of the regularization is controlled by a hyper-parameter that represents the rate of the layer units to drop. It is commonly applied before dense layers, but could also be used for convolutional ones.&lt;/p>
&lt;p>Combining both L2-norm with $\lambda=10^{-3}$ on all layers and dropout regularization with ratio 0.5 (half units dropped) on convolutional layers, we obtain the following loss and accuracy:&lt;/p>
&lt;figure id="figure-dropout-and-l2-regularization-training-curves">
&lt;a data-fancybox="" href="https://earnold.me/post/dogsvscats/drop_l2_curves_hu6860b813fdd25c9060498295d82a4fd5_39702_2000x2000_fit_lanczos_3.png" data-caption="Dropout and L2 regularization training curves.">
&lt;img data-src="https://earnold.me/post/dogsvscats/drop_l2_curves_hu6860b813fdd25c9060498295d82a4fd5_39702_2000x2000_fit_lanczos_3.png" class="lazyload" alt="" width="648" height="288">
&lt;/a>
&lt;figcaption>
Dropout and L2 regularization training curves.
&lt;/figcaption>
&lt;/figure>
&lt;p>The results show that combining L2 and dropout regularization provides a good solution to prevent model overfitting. Even though the model did not overfit we did not observe any improvement on the test set accuracy if compared to L2 only regularization. Another interesting characteristic is the chaotic behavior of the test loss function, which is due to the probabilistic nature of the unit drops, albeit the mean value of the test loss seems to follow the train loss, which is a good indicative. The notebook containing the dognet architecture along with regularizers is available
&lt;a href="https://earnold.me/notebooks/dogsvscats/dognet.html">here&lt;/a>.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>In this specific application the downscaling of input image size did not have a bad impact on model performance and was actually capable of greatly decreasing the training time. Although this may not be the case when dealing with samples containing fine details where two distinct classes look similar to one another.&lt;/p>
&lt;p>Through the series of experiments applying regularization techniques it was possible to prevent model overfiting and improve generalization measured as an increase on test set accuracy, getting up to 85% accuracy. Combining both L2 and dropout proved to be a good idea.&lt;/p>
&lt;p>In order to further increase model accuracy on the test set it would be necessary to gather more training data and possibly apply data augmentation (rotation, scaling, cropping). Another possibility is to use the transfer learning approach, which uses already trained high-level models, offering higher than 96% accuracy on the test set, at the expense of having a much larger model and longer processing times.&lt;/p>
&lt;p>Finally, a note on the Kaggle competition. I think Kaggle has an important role on promoting machine learning and its applications, in engaging researchers and hobbyists alike and even helping people to learn more about recent techniques, letting them compare results, etc. Still, it seems to fail to consider the complexity of the elaborated solutions. To illustrate my point check the
&lt;a href="http://blog.kaggle.com/2017/04/20/dogs-vs-cats-redux-playground-competition-3rd-place-interview-marco-lugo/" target="_blank" rel="noopener">Marco Lugo post&lt;/a> on Kaggle blog showing his 3rd place solution to this challenge. He uses a bunch of complex convolutional models in parallel and predict the output as a weighted average of these classifiers. Of course great results come at the price of increased complexity. I am just saying that complexity should also be taken in account when evaluating model. We may ask ourselves: what is the best score I can get using only the training data they provide, without any pre-loaded models, or an ensemble of 20 complex classifiers? The way Kaggle is organized today does not allow an answer to that question.&lt;/p></description></item><item><title>Neural networks from scratch</title><link>https://earnold.me/post/nn-scratch/</link><pubDate>Wed, 12 Apr 2017 12:00:00 +0000</pubDate><guid>https://earnold.me/post/nn-scratch/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I have used a handful of machine learning models in the past. These include simple linear regression models, support vector machines (SVMs) and neural networks. While working on different projects I was mostly concerned in solving a specific application problem and did not worry about the inner workings of the models I was using. I have delved into more details of SVM during my final undergraduate project but did not have the time to do the same for neural networks.&lt;/p>
&lt;p>After reading and learning about it in more detail it has come the time for me to share it. In this post I will give some brief introduction to neural nets and derive some of the maths behind a simple architecture as well as its implementation using only
&lt;a href="http://www.numpy.org/" target="_blank" rel="noopener">Numpy&lt;/a>. I have based most of this post on
&lt;a href="http://cs231n.github.io/neural-networks-case-study/" target="_blank" rel="noopener">Stanford&amp;rsquo;s CS231&lt;/a> notes which I found very useful for beginners. Some of the math derived here was not available there so it might be useful for a more interested reader.&lt;/p>
&lt;p>Neural networks are organized in layers, each consisting of many units. These units are responsible for crunching the data. Each unit can receive many inputs but has a single output that can be connected to many more units in the following layer. In this study we will address a fully-connected structucture, meaning that every unit in a single layer receives input from all the units from the previous layer. To compute its output, each unit perform a weighted sum of its inputs and a bias term, then apply an activation funcion, in general non-linear, to the calculated sum. The model parameters consists of the connection weights between the units and their bias terms.&lt;/p>
&lt;h2 id="defining-the-model">Defining the model&lt;/h2>
&lt;p>To simplify our study we are going to use a two layer network architecture (the input layer is not counted). The input layer $x$ has $N_x$ units, the hidden layer $h$ has $N_h$ and the output layer $y$ has $N_y$ units.&lt;/p>
&lt;p>We could specify this architecture to perform multi-class classification, so the output layer would give the probability the sample belongs to each of the available classes. In this case we use $N_y$ as the number of classes and to enforce a probabilistic output we use an activation function called &lt;strong>softmax&lt;/strong> on the output layer, which also guarantess that the sum of the classes probabilities sums to 1.&lt;/p>
&lt;p>As for the hidden layer we can use a &lt;strong>RELU&lt;/strong> activation function, because it is avery simple to evaluate non-linear function: $\text{RELU}(z) = \max(0, z)$. There are other reasons why we would use it, mostly to avoid a problem called vanishing gradient, but in a shallow network such as this one this would not be a problem.&lt;/p>
&lt;p>We define the weights of the connections between the input and hidden layer units as the matrix $W1 \in \mathbb{R}^{N_x \times N_h}$, where $W1_{i,j}$ is the weight of the connection between the $i$-th input layer unit to the $j$-th hidden layer unit. The bias vector of the hidden layer is defined as $b1 \in \mathbb{R}^{N_h}$. Similarly we have the parameters of connection between the hidden layer to the output layer: $W2 \in \mathbb{R}^{N_h \times N_y}$ and $b2 \in \mathbb{R}^{N_y}.$&lt;/p>
&lt;p>&lt;img src="https://earnold.me/notebooks/nn-diagram.png" alt="nn-diagram">&lt;/p>
&lt;h2 id="forward-propagation">Forward propagation&lt;/h2>
&lt;p>Assuming we have all the ideal model parameters $W1, W2, b1, b2$, how do we get the output of the network for a given input sample? This is called forward propagation, since the data flows from the input layer, through the hidden layers and finally to the output layer as presented in the following equations.&lt;/p>
&lt;p>$$\begin{align*} \\
h &amp;amp;= \max(0,x W1 + b1) \\
y &amp;amp;= \text{softmax}(\underbrace{h W2 + b2}_\textrm{score})
\end{align*}$$&lt;/p>
&lt;p>The &lt;strong>softmax&lt;/strong> activation function can be expressed as
$$ \text{softmax}(z)_j = \frac{e^{z_j}}{\sum_{i=0}^{N_z}e^{z_i}} $$&lt;/p>
&lt;p>This formulation allows the input $x$ to have many samples, each in a row, so $x \in \mathbb{R}^{n \times N_\text{features}}$, where $n$ is the number of samples in a batch, also called batch size, and $N_\text{features}$ is the number of features, or the dimension of each sample. This would yield an output $y \in \mathbb{R}^{n \times N_y}$, where each sample probability distribution among classes is given in a row.&lt;/p>
&lt;h2 id="training-process">Training process&lt;/h2>
&lt;p>In order to get the ideal model parameters we have to train the network on a training set. This consists of an optimization process where we try to minimize a loss function that tells how close the network output $y$ is to the real labels $\hat{y}$. At each iteration of this process we obtain new values for the parameters that will hopefully decrease the value of the loss function.&lt;/p>
&lt;p>In this example, since we assumed a multi-class classification problem with a probabilistic output, the ideal loss function to use is the categorical cross-entropy function, given by
$$ L_i = -\log y_{\hat{y}_i}$$&lt;/p>
&lt;p>To make sense of this function we can analyse its behaviour. For a given sample $x_i$ belonging to class $\hat{y}_i$ it will compute the negative log of the output probability of the sample belonging to classs $\hat{y}_i$ (given by the $\hat{y}_i$-th component of $y$). Ideally this probability would be 1, which would make $L_i=0$. Whenever this is not the case, there is a loss associated with the sample $x_i$.&lt;/p>
&lt;p>Now we have a measure to evaluate how good our classifier is doing on the training set we can try to optimize the network parameters to get the loss as close to zero as possible. We do this using an iterative algorithm called &lt;strong>Stochastic Gradient Descent&lt;/strong>. To give a brief overview of this method, assume all network parameters are represented in a vector $\theta$. We can compute the variation of the loss function $\Delta L$ given a variation of the parameters vector $\Delta \theta$ as $$\Delta L = \Delta \theta \cdot \nabla L$$ where $\nabla L$ is the gradient of the loss with respect to the parameters $\theta$. We always want to descrease the loss, so we want $\Delta L &amp;lt; 0$. One way to guarantee this condition is to choose the variation of parameters as $$\Delta \theta = - \eta \nabla L$$ for a small-enough learning rate $\eta &amp;gt;0$, which would yield $$\Delta L = -\eta |\nabla L|^2.$$&lt;/p>
&lt;p>This description is for vanilla Gradient Descent, also called batch-GD (where $L = \sum L_i$), so every sample is considered in the gradient. The stochastic part comes when you only consider a single sample at each iteration step, what reduces training time, especially on big datasets. Although it may seem attractive, this method offers slower convergence since there is a lot of zig-zagging between samples optimizations. To overcome this another variation called mini-batch GD can be used. This method lies in between batch-GD (uses one sample) and SGD (uses all samples), since it considers a mini-batch of size $N_b$ to compute the gradient, thus reducing training time and still allowing faster convergence.&lt;/p>
&lt;p>By iteratively running this optimization algorithm we can reduce the loss function and train our model. There is one missing step though: how to compute the loss function gradient $\nabla L.$&lt;/p>
&lt;h2 id="backpropagation-and-gradient-computation">Backpropagation and gradient computation&lt;/h2>
&lt;p>Even considering our small network the loss is a rather complex function of the network parameters given all multiplications and non-linear activations. To compute its gradient $\nabla L$ we must find the derivative of $L$ with respect to all model parameters, which can be difficult to be done analytically. We then use an algorithm called Backpropagation, which is basically the use of calculus&amp;rsquo; chain-rule. By multiplying the local derivatives from layer to layer we can numerically evaluate the derivative of the loss with respect to any parameter.&lt;/p>
&lt;p>We start from the output layer $y$. Since $$ L_i = -\log y_{\hat{y}_i}$$ we have $$\frac{\partial L_i}{\partial y_k} = \frac{-1}{y_k} 1(\hat{y}_i=k)$$ where 1(z) is the indicator function (1 if argument true, otherwise 0).&lt;/p>
&lt;p>$\DeclareMathOperator{\score}{score}$
We then calculate the derivatives of the output $y_k$ with respect the intermediate variable $\score = h W2 + b2$, with $ y = \text{softmax}(h W2 + b2)$. We must consider two cases, one for the derivative of $y_k$ with respect to $\score_j$ with $k \neq j$ and another with $k=j$.&lt;/p>
&lt;p>For the first case we can write $$ y_k = \frac{e^{\score_k}}{e^{\score_j} + \sum_{i \neq j} e^{\score_i}}$$ then using the quotient rule for derivatives we have:
$$\frac{\partial y_k}{\partial \text{score}_j} = \frac{-e^{\score_k}e^{\score_j}}{(\sum_i e^{\score_i})^2} = -\frac{e^{\score_k}}{\sum_i e^{\score_i}} \frac{e^{\score_j}}{\sum_i e^{\score_i}} = -y_k y_j$$&lt;/p>
&lt;p>For the second case, when $k=j$ we can write $$ y_k = \frac{e^{\score_k}}{e^{\score_k} + \sum_{i \neq k} e^{\score_i}}$$ and then the derivative becomes:&lt;/p>
&lt;div>\begin{eqnarray}
\frac{\partial y_k}{\partial \text{score}_k} &amp;=&amp; \frac{e^{\score_k}(\sum_i e^{\score_i}) - e^{2\score_k}}{(\sum_i e^{\score_i})^2} \\
&amp;=&amp; \frac{e^{\score_k}}{\sum_i e^{\score_i}} - \frac{e^{\score_k}}{\sum_i e^{\score_i}} \frac{e^{\score_k}}{\sum_i e^{\score_i}} \\
&amp;=&amp; y_k-y_k y_k \\
&amp;=&amp; y_k(1-y_k)
\end{eqnarray}&lt;/div>
&lt;p>Now, to calculate the derivative of the loss with respect to the score intermediate variables we use the chain-rule as follows:&lt;/p>
&lt;div>\begin{eqnarray}
\frac{\partial L_i}{\partial \score_j} &amp; = &amp; \frac{\partial L_i}{\partial y_k} \frac{\partial y_k}{\partial \score_j} &amp;\\
&amp; = &amp; \frac{-1}{y_{\hat{y}_i}} \times -y_{\hat{y}_i} y_j = y_j, &amp;\text{ if } j \neq \hat{y}_i \\
&amp; = &amp; \frac{-1}{y_{\hat{y}_i}} \times y_{\hat{y}_i}(1-y_{\hat{y}_i}) = y_{\hat{y}_i} -1, &amp;\text{ if } j=\hat{y}_i
\end{eqnarray}&lt;/div>
&lt;p>For implementation reasons we can call a vector $\text{dscore} \in \mathbb{R}^{N_y}$ where each component is the derivative of the loss regarding a component of the score variable. In a compact form: $$ \text{dscore}_j = \frac{\partial L_i}{\partial \score_j} = y_j - 1(j=\hat{y}_i)$$&lt;/p>
&lt;p>We must now propagate this derivative to the parameters of the output layer. From definition we have $\score = h W2 + b2$. To improve visualization we can expand it in the form (considering a single sample batch, n=1):&lt;/p>
&lt;p>$$\begin{align*}
\score_1 &amp;amp;= h_1 W2_{11} + h_2 W2_{21} + h_3 W2_{31} + \cdots + b2_1 \\
\score_2 &amp;amp;= h_1 W2_{12} + h_2 W2_{22} + h_3 W2_{32} + \cdots + b2_2 \\
\score_3 &amp;amp;= h_1 W2_{13} + h_2 W2_{23} + h_3 W2_{33} + \cdots + b2_3 \\
&amp;amp; \vdots &amp;amp; \\
\score_j &amp;amp;= h_1 W2_{1j} + h_2 W2_{2j} + h_3 W2_{3j} + \cdots + b2_j \\
\end{align*}$$&lt;/p>
&lt;p>It is easy to see that $$\frac{\partial \score_j}{\partial b2_j} = 1 \implies \frac{\partial L_i}{\partial b2_j} = \frac{\partial L_i}{\partial \score_j} \frac{\partial \score_j}{\partial b2_j} = \frac{\partial L_i}{\partial \score_j}.$$ This imples that the vector of weights update for $b2$ is $db2 = \text{dscore}$&lt;/p>
&lt;p>Similarly, we have $$ \frac{\partial \score_j}{\partial W2_{kj}} = h_k \implies \frac{\partial L_i}{\partial W2_{kj}} = \frac{\partial L_i}{\partial \score_j} \frac{\partial \score_j}{\partial W2_{kj}} = h_k \frac{\partial L_i}{\partial \score_j}$$&lt;/p>
&lt;p>In this case, the matrix of weights updates is given by&lt;/p>
&lt;p>$$\begin{align*}
dW2 &amp;amp;= \begin{pmatrix} \
\frac{\partial L}{\partial W_{11}} &amp;amp; \frac{\partial L}{\partial W_{12}} &amp;amp; \cdots \\
\frac{\partial L}{\partial W_{21}} &amp;amp; \frac{\partial L}{\partial W_{22}} &amp;amp; \cdots \\
\vdots &amp;amp; \vdots &amp;amp; \ddots \\
\end{pmatrix} \\
&amp;amp;= \begin{pmatrix} \
h_1 \text{dscore}_1 &amp;amp; h_1 \text{dscore}_2 &amp;amp; \cdots \\
h_2 \text{dscore}_1 &amp;amp; h_2 \text{dscore}_2 &amp;amp; \cdots \\
\vdots &amp;amp; \vdots &amp;amp; \ddots \\
\end{pmatrix} \\
&amp;amp; = \begin{pmatrix} \
h_1 \\
h_2 \\
\vdots \\
\end{pmatrix} \
\begin{pmatrix} \
\text{dscore}_1 &amp;amp; \text{dscore}_2 &amp;amp; \cdots \\
\end{pmatrix} \\
&amp;amp; = h^T \text{dscore} \\
\end{align*}$$&lt;/p>
&lt;p>To propagate the gradient to the hidden layer we must first calculate the gradient with respect to $h$: $$\frac{\partial \score_j}{\partial h_k} = W2_{kj}$$&lt;/p>
&lt;p>The differential parameter vector $dh$ is given by:&lt;/p>
&lt;p>$$\begin{align*}
dh^T &amp;amp;= \begin{pmatrix} \
\frac{\partial L}{\partial h_1} \\
\frac{\partial L}{\partial h_2} \\
\vdots \\
\end{pmatrix} \\
&amp;amp;= \begin{pmatrix} \
\frac{\partial L}{\partial\score_1}\frac{\partial \score_1}{\partial h_1}+ \frac{\partial L}{\partial\score_2}\frac{\partial \score_2}{\partial h_1} + \cdots\\
\frac{\partial L}{\partial\score_1}\frac{\partial \score_1}{\partial h_2}+ \frac{\partial L}{\partial\score_2}\frac{\partial \score_2}{\partial h_2} + \cdots\\
\vdots \\
\end{pmatrix}\\
&amp;amp;= \begin{pmatrix} \
\frac{\partial \score_1}{\partial h_1} &amp;amp; \frac{\partial \score_2}{\partial h_1} &amp;amp; \cdots\\
\frac{\partial \score_1}{\partial h_2} &amp;amp; \frac{\partial \score_2}{\partial h_2} &amp;amp; \cdots\\
\vdots &amp;amp; \vdots &amp;amp; \ddots \\
\end{pmatrix} \
\begin{pmatrix} \
\frac{\partial L}{\partial\score_1}\\
\frac{\partial L}{\partial\score_2}\\
\vdots \\
\end{pmatrix} \\
&amp;amp;= W2 \times \text{dscore}^T\\
dh &amp;amp;= \text{dscore} \times W2^T \\
\end{align*}$$&lt;/p>
&lt;p>Next we consider the RELU activation: $h=max(0, \underbrace{xW1+b1}_\textrm{r})$. $\frac{dh}{dr} = 1(r&amp;gt;0)$. Thus, $dr = dh \times 1(h&amp;gt;0)$.&lt;/p>
&lt;p>Finally, for the hidden layer, we can observe $dh$ as the output and $x$ as input, so by extending the equations for $dW2$ and $db2$ we have&lt;/p>
&lt;div>$$\begin{align*}
dW1 &amp;= x^T \text{dr} \\
db1 &amp;= \text{dr}
\end{align*}$$&lt;/div>
&lt;h2 id="numpy-implementation">Numpy implementation&lt;/h2>
&lt;p>The forward propagation is straightforward as can be seen below.&lt;/p>
&lt;pre>&lt;code class="language-python">h = np.maximum(0, np.dot(x,W1)+b1)
score = np.dot(h,W2)+b2
y = np.exp(score)
y /= np.sum(y, axis=1, keepdims=True)
&lt;/code>&lt;/pre>
&lt;p>Although understanding backpropagation can be difficult, the resulting equations are somewhat simple to implement, because they only need to consider the local derivatives at each step.&lt;/p>
&lt;p>Primarily we computer the $\text{dscore}$ intermediate variable, where y_t represents $\hat{y}$, the true class label.&lt;/p>
&lt;pre>&lt;code class="language-python">dscore = np.copy(y)
dscore[range(n), y_t] -= 1
dscore /= n
&lt;/code>&lt;/pre>
&lt;p>Next the output layer parameters updates are calculated. Since we now have $n$ training samples, we have $L = \frac{1}{n} \sum_{i=0}^n L_i$, so dscore gets summed (the division by $n$ has already taken place in the previous step).&lt;/p>
&lt;pre>&lt;code class="language-python">dW2 = np.dot(h.T, dscore)
db2 = np.sum(dscore, axis=0, keepdims=True)
&lt;/code>&lt;/pre>
&lt;p>The hidden layer activation derivative is calculated, followed by the parameter updates.&lt;/p>
&lt;pre>&lt;code class="language-python">dh = np.dot(dscore, W2.T)
dr = dh
dr[h &amp;lt;= 0] = 0
dW1 = np.dot(x.T, dr)
db1 = np.sum(dr, axis=0, keepdims=True)
&lt;/code>&lt;/pre>
&lt;p>Finally we apply the weight updates using the a specified learning rate $\eta$ (lr).&lt;/p>
&lt;pre>&lt;code class="language-python">W1 -= lr*dW1
b1 -= lr*db1
W2 -= lr*dW2
b2 -= lr*db2
&lt;/code>&lt;/pre>
&lt;h2 id="source-code">Source code&lt;/h2>
&lt;p>To check the code and results, please visit
&lt;a href="https://earnold.me/notebooks/nn-scratch.html">this notebook&lt;/a>.&lt;/p></description></item></channel></rss>