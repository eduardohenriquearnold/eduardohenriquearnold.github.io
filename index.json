[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a PhD candidate with the Intelligent Vehicles group at the University of Warwick. My research is focused on perception methods for autonomous driving, particularly Cooperative 3D Object Detection.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://earnold.me/author/eduardo-arnold/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/eduardo-arnold/","section":"authors","summary":"I\u0026rsquo;m a PhD candidate with the Intelligent Vehicles group at the University of Warwick. My research is focused on perception methods for autonomous driving, particularly Cooperative 3D Object Detection.","tags":null,"title":"Eduardo Arnold","type":"authors"},{"authors":null,"categories":null,"content":"This post shows how to visualise lidar point-clouds obtained from CARLA in real-time using the animation functionality from Mayavi. Although this post uses real-time data from CARLA, one can easily change the source of information to real sensors or simply replay recorded sensor data.\nMotivation CARLA provides a lidar visualisation script using Open3D available here. However, I personally found Open3D to have quite a long dependency list since it is a library for manipulating 3D data including an extensive list of algorithms. So I would rather use a tool created specifically for 3D data visualisation - enters Mayavi. Mayavi has a much smaller dependency list and is widely used to plot point clouds in the autonomous driving domain - usually adding bounding boxes to represent objects. However it comes with some perks, namely when creating visualisations for a continuous and asynchronous data stream. Since I could not find a better reference for this problem I decided to share my solution in this post.\nCARLA Setup Firstly, let\u0026rsquo;s set up the CARLA end to receive the data. This section assumes prior knowledge of the CARLA simulation environment and is partly adapted from PythonAPI/open3d_lidar.py\nThe first step is connecting with the CARLA server and setting up the synchronous mode, which will ensure that we get consistent point clouds.\nimport carla client = carla.Client('localhost', 2000) client.set_timeout(2.0) world = client.get_world() try: settings = world.get_settings() traffic_manager = client.get_trafficmanager(8000) traffic_manager.set_synchronous_mode(True) delta = 0.05 settings.fixed_delta_seconds = delta settings.synchronous_mode = True settings.no_rendering_mode = arg.no_rendering world.apply_settings(settings)  Next we spawn an ego-vehicle into our simulated world using a random starting position and a random blueprint (vehicle model):\nblueprint_library = world.get_blueprint_library() vehicle_bp = blueprint_library.filter(arg.filter)[0] vehicle_transform = random.choice(world.get_map().get_spawn_points()) vehicle = world.spawn_actor(vehicle_bp, vehicle_transform) vehicle.set_autopilot(arg.no_autopilot)  We must now create our ray-cast lidar sensor and set-up some parameters (a complete list of sensor parameters is available here):\nlidar_bp = blueprint_library.find('sensor.lidar.ray_cast') lidar_bp.set_attribute('noise_stddev', '0.2') lidar_bp.set_attribute('channels', str(64)) #number of lasers, normally 64 or 128 (on newer lidar models). lidar_bp.set_attribute('range', str(100)) #range in meters lidar_bp.set_attribute('rotation_frequency', str(1.0 / delta)) #ensures we will get a full sweep within a simulation frame lidar_transform = carla.Transform(carla.Location(x=-0.5, z=1.8)) lidar = world.spawn_actor(lidar_bp, lidar_transform, attach_to=vehicle) lidar.listen(lidar_callback)  The last line sets up the callback function that gets called everytime new data (point clouds) arrives, but we still do not know what this function should look like.\nThe simulation runs in synchronous mode in such a way that we must send a world.tick() event at every iteration step so that the simulation can run another loop iteration, updating the physical models and generating new sensor data. This prevents us getting flooded with data if our processing pipeline runs much slower than the simulation itself. Although the simulation runs in synchronous mode, the data is still transfered in an ansynchronous manner, which we receive through the callback function lidar_callback(data). To make sure the simulation runs continuously we create an infinite loop as\nwhile True: time.sleep(0.005) world.tick()  We still need to figure out what the lidar_callback function looks like. This will depend on how we visualise our data, so now we dive into the Mayavi part!\nMayavi Given a set of points pts with shape [N,3] where $N$ is the number of points and an optional set of intensities for each given point, one can visualise the point cloud using\nfrom mayavi import mlab #given a set of points pts [N,3] and a set of intensities [N,] mlab.points3d(pts[:,0], pts[:,1], pts[:,2], intensity, mode='point') mlab.show()  Given this, one could create the lidar callback function as\ndef lidar_callback(data): data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4'))) data = np.reshape(data, (int(data.shape[0] / 4), 4)) #Isolate the intensity intensity = data[:, -1] #Isolate the 3D data points = data[:, :-1] #We're negating the y to correclty visualize a world that matches #what we see in Unreal since Mayavi uses a right-handed coordinate system points[:, :1] = -points[:, :1] mlab.points3d(points[:,0], points[:,1], points[:,2], intensity, mode='point') mlab.show()  This creates a static visualisation each time a new packet of data arrives, which is quite inefficient and does not allow the user to interect with the data (i.e. change viewing angles). Mayavi provides a animation guide that shows how to create visualisations that change with time. However it assumes that the data is updated in synchronous intervals which is not the case when we obtain the data with a asynchronous callback function from an external source such as a simulation tool or a real sensor.\nOne way to solve this is to create a visualisation within the main thread scope (required by Mayavi) and update this visualisation once we get any callback with new data. This solution however requires calling mlab.show() on the main thread, which blocks the execution of code until the Mayavi visualisation screen is closed and means that we can no longer keep sending the world.tick() signals back to the simulator. To overcome this limitation we create a secondary thread that is responsible for sending the world.tick() updates, while at the same time creating an empty Mayavi visualisation window:\nimport threading def carlaEventLoop(world): while True: time.sleep(0.005) world.tick() loopThread = threading.Thread(target=carlaEventLoop, args=[world], daemon=True) loopThread.start() vis = mlab.points3d(0, 0, 0, 0, mode='point', figure=fig) mlab.show()  Now one could implement the lidar callback function as\ndef lidar_callback(data, vis): data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4'))) data = np.reshape(data, (int(data.shape[0] / 4), 4)) #Isolate the intensity intensity = data[:, -1] #Isolate the 3D data points = data[:, :-1] #We're negating the y to correclty visualize a world that matches #what we see in Unreal since Mayavi uses a right-handed coordinate system points[:, :1] = -points[:, :1] #Update visualisation using Mayavi animation guide vis.mlab_source.reset(x=points[:,0], y=points[:,1], z=points[:,2], scalars=intensity) #To register this callback we use a lambda function to mask the vis variable with the empty visualisation created previously lidar.listen(lambda data: lidar_callback(data, vis))  This formulation tends to work most of the time, but occasionally fails with VTK errors such as Source array too small, requested tuple at index 11719, but there are only 11625 tuples in the array.. The error seems related to the frequency of updates created by the callback function. Although I did not have time to investigate why exactly this issue arises, I was able to come up with an alternative error-free solution.\nThe alternative solution consists of creating a buffer that stores the most recent point cloud received through the callback function, but only updating the Mayavi visualisation in synchronous intervals. The main part of the code (after creating the sensors and vehicle) looks like:\ndef lidar_callback(data, buf): data = np.copy(np.frombuffer(point_cloud.raw_data, dtype=np.dtype('f4'))) data = np.reshape(data, (int(data.shape[0] / 4), 4)) #Isolate the intensity intensity = data[:, -1] #Isolate the 3D data points = data[:, :-1] #We're negating the y to correclty visualize a world that matches #what we see in Unreal since Mayavi uses a right-handed coordinate system points[:, :1] = -points[:, :1] #copy points/intensities into buffer buf['pts'] = points buf['intensity'] = intensity def carlaEventLoop(world): while True: time.sleep(0.005) world.tick() def main(): #creates client,world ... #spawns vehicle ... #creates sensor ... #creates empty visualisation vis = mlab.points3d(0, 0, 0, 0, mode='point', figure=fig) #defines empty buffer buf = {'pts': np.zeros((1,3)), 'intensity':np.zeros(1)} #set callback lidar.listen(lambda data: lidar_callback(data, buf)) #creates thread for event loop loopThread = threading.Thread(target=carlaEventLoop, args=[world], daemon=True) loopThread.start() #define mayavi animation loop @mlab.animate(delay=100) def anim(): while True: vis.mlab_source.reset(x=buf['pts'][:,0], y=buf['pts'][:,1], z=buf['pts'][:,2], scalars=buf['intensity']) yield #start visualisation loop in the main-thread, blocking other executions anim() mlab.show()  Visualisation results Using the default lidar noise parameters for point dropout and Gaussian noise parameters we can now visualise the point clouds coming from the CARLA simulator in real time directly in the Mayavi interface:\nSource code You may find the complete code for this post in mayavi_lidar.py.\n","date":1605639240,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605639240,"objectID":"5f842a82dc3819f01c769b89a2a41a69","permalink":"https://earnold.me/post/mayavilidar/","publishdate":"2020-11-17T15:54:00-03:00","relpermalink":"/post/mayavilidar/","section":"post","summary":"Using the CARLA autonomous driving simulator to generate lidar point clouds which are then visualised in real-time using Mayavi.\n","tags":["simulation","computer vision"],"title":"Real-time visualisation of simulated lidar point-clouds with Mayavi and CARLA","type":"post"},{"authors":["Eduardo Arnold","Mehrdad Dianati","Robert de Temple","Saber Fallah"],"categories":null,"content":"","date":1602806400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1602806400,"objectID":"468f5c3ecffaa3ee942cccb17a0db366","permalink":"https://earnold.me/publication/arnold-2020-coop3dod/","publishdate":"2020-10-16T09:14:31.801192Z","relpermalink":"/publication/arnold-2020-coop3dod/","section":"publication","summary":"3D object detection is a common function within the perception system of an autonomous vehicle and outputs a list of 3D bounding boxes around objects of interest. Various 3D object detection methods have relied on fusion of different sensor modalities to overcome limitations of individual sensors. However, occlusion, limited field-of-view and low-point density of the sensor data cannot be reliably and cost-effectively addressed by multi-modal sensing from a single point of view. Alternatively, cooperative perception incorporates information from spatially diverse sensors distributed around the environment as a way to mitigate these limitations. This article proposes two schemes for cooperative 3D object detection using single modality sensors. The early fusion scheme combines point clouds from multiple spatially diverse sensing points of view before detection. In contrast, the late fusion scheme fuses the independently detected bounding boxes from multiple spatially diverse sensors. We evaluate the performance of both schemes, and their hybrid combination, using a synthetic cooperative dataset created in two complex driving scenarios, a T-junction and a roundabout. The evaluation shows that the early fusion approach outperforms late fusion by a significant margin at the cost of higher communication bandwidth. The results demonstrate that cooperative perception can recall more than 95% of the objects as opposed to 30% for single-point sensing in the most challenging scenario. To provide practical insights into the deployment of such system, we report how the number of sensors and their configuration impact the detection performance of the system.","tags":null,"title":"Cooperative Perception for 3D Object Detection in Driving Scenarios using Infrastructure Sensors","type":"publication"},{"authors":null,"categories":null,"content":"","date":1601683200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601683200,"objectID":"822331e73ed0ba29cdd472f4390acdb9","permalink":"https://earnold.me/project/raytracer/","publishdate":"2020-10-03T00:00:00Z","relpermalink":"/project/raytracer/","section":"project","summary":"Created a simple ray tracer renderer from scratch using C++.","tags":["computer-graphics"],"title":"Ray Tracer from Scratch","type":"project"},{"authors":[],"categories":[],"content":"Problem definition Sales forecasting is a very common problem faced by many companies. Given a history of sales of a certain product we would like to predict the demand of that product for a time window in the future. This is useful as it allows companies and industries to plan their workload and reduce waste of resources. This problem is a common example of time series forecasting and there are many approaches to tackle it.\nToday we will learn a bit more about this with a practical example: the Kaggle Predict Future Sales dataset. This challenge aims to predict future sales of different products for the next month in different shops of a retail chain. In this notebook we do not aim at solving this challenge, rather, we will explore some concepts of time series forecasting that could be used to solve such problem.\nYou can download the Jupyter notebook version of this tutorial here.\nExploring the dataset First we need to download the whole dataset and extract it to a data folder in the root of this notebook path.\nimport numpy as np import pandas as pd import matplotlib.pyplot as plt from ipywidgets import interact_manual plt.style.use('ggplot') %matplotlib inline  #load sales and rename columns to ease understanding dtype = {'date_block_number':np.uint16, 'shop_id':np.uint32, 'item_id':np.uint32, 'item_cnt_day':np.float32} sales = pd.read_csv('data/sales_train.csv', dtype=dtype) \\ .rename(columns={'date_block_num': 'month', 'item_cnt_day':'sold'})  sales.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n  date month shop_id item_id item_price sold     0 02.01.2013 0 59 22154 999.00 1.0   1 03.01.2013 0 25 2552 899.00 1.0   2 05.01.2013 0 25 2552 899.00 -1.0   3 06.01.2013 0 25 2554 1709.05 1.0   4 15.01.2013 0 25 2555 1099.00 1.0     As we can see, we have daily records for each shop and item. Note that some of the sold values are negative because they include returns.\nFirstly, since we are interested in estimating the monthly sales, we will aggregate the dataset by month using the handy month feature, which ranges from 0 (representing January 2013) to 33 (October 2015). We will group the sales by the shop and item ids. The aggregation will sum all sold fields during the month for each product and shop, and average the item_price (it is likely to change from month to month).\ngsales = sales.groupby(['shop_id','item_id','month']).agg({'item_price':np.mean, 'sold':np.sum})  gsales[:20]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n    item_price sold   shop_id item_id month       0 30 1 265.0 31.0   31 1 434.0 11.0   32 0 221.0 6.0   1 221.0 10.0   33 0 347.0 3.0   1 347.0 3.0   35 0 247.0 1.0   1 247.0 14.0   36 1 357.0 1.0   40 1 127.0 1.0   42 1 127.0 1.0   43 0 221.0 1.0   49 1 127.0 2.0   51 0 128.5 2.0   1 127.0 3.0   57 1 167.0 1.0   59 1 110.0 1.0   61 0 195.0 1.0   75 0 76.0 1.0   85 1 190.0 1.0     We can now observe that for the first shop many items only have selling records for the first two months. We would like to investigate how this varies for different products across all 60 shops.\n@interact_manual(shop_id = (0,59,1)) def plot_product_record_frequency(shop_id): count_months = gsales.reset_index().groupby(['shop_id','item_id']).size()[shop_id] plt.bar(count_months.keys(), count_months) plt.xlabel('product_id') plt.ylabel('Num months available')  interactive(children=(IntSlider(value=29, description='shop_id', max=59), Button(description='Run Interact', s…  However, this interactive visualisation will not work unless you have a notebook running, so we will plot the sales frequency for some of the stores below:\ndef plot_product_record_frequency(shop_id): count_months = gsales.reset_index().groupby(['shop_id','item_id']).size()[shop_id] plt.bar(count_months.keys(), count_months) plt.xlabel('product_id') plt.ylabel('Num months available') plt.title(f'shop_id {shop_id}') fig=plt.figure(figsize=(12, 8), dpi= 80, facecolor='w', edgecolor='k') for i, shop_id in enumerate([0,1,29,31]): plt.subplot(2,2,i+1) plot_product_record_frequency(shop_id) plt.tight_layout()  We can observe that some shops have a very limited record of sales, e.g. shop 0 and 1. On the other hand, shop 29 and 31 have a considerable number of items with a record above 10 weeks.\nPerhaps we should look into the shops with the larger number of sales, which we can inspect by observing the distribution of shop_id:\nsales['shop_id'].plot.hist(bins=60) plt.xlabel('product_id');  If we observe the previous histogram for the number of weeks of records for each product of a given shop, we will observe indeed that the shop 31 has a comprehensive number of products with a significant history of sales.\nConsidering the behaviour of multiple shops introduce complexity steaming from this biased distribution of sales. Some shops will have very little preior information about their sales, so it would be difficult to make a good prediction of sales in these shops. For this reason we will neglect different shops and instead try to estimate a new volume of sales for all products in the whole supermarket chain. So, we modify our aggregated sales DataFrame gsales accordingly:\ngsales = sales.groupby(['item_id','month']).agg({'item_price':np.mean, 'sold':np.sum})  We can also observe what is the number of months available for each product sale history in this chain-wide collection:\ncount_months = gsales.reset_index().groupby(['item_id']).size() plt.bar(count_months.keys(),count_months); plt.xlabel('product_id') plt.ylabel('Num months available');  Although in a real forecast scenario we would like to have a good prediction of sales even when the history for a given product is minimal, in this example we will focus on products with full history for all 34 months.\nproductsIdx = count_months[count_months == 34].index.values selected_sales = gsales.loc[productsIdx] productsIdx.shape  (523,)  We obtained 523 unique products that have a selling history for all of the 33 months. An example of the first 60 items are below:\nselected_sales[:60]   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }  \n   item_price sold   item_id month       32 0 338.110349 299.0   1 337.771930 208.0   2 343.794702 178.0   3 341.888889 97.0   4 347.000000 66.0   5 342.070886 79.0   6 345.951190 87.0   7 340.172727 72.0   8 340.017544 59.0   9 184.592593 58.0   10 144.316456 81.0   11 147.994444 89.0   12 144.710526 84.0   13 144.066667 48.0   14 149.000000 44.0   15 143.714286 30.0   16 149.000000 26.0   17 149.000000 26.0   18 130.500000 12.0   19 148.991176 34.0   20 144.771429 37.0   21 148.991667 37.0   22 146.060714 29.0   23 149.000000 40.0   24 145.572727 42.0   25 146.387333 32.0   26 146.869167 40.0   27 149.000000 20.0   28 149.000000 20.0   29 149.000000 26.0   30 149.000000 21.0   31 148.714286 30.0   32 149.000000 19.0   33 149.000000 22.0   33 0 488.517241 61.0   1 484.170732 39.0   2 490.870968 32.0   3 489.500000 16.0   4 499.000000 12.0   5 205.046512 44.0   6 195.439130 46.0   7 197.277778 35.0   8 198.052381 43.0   9 195.915152 33.0   10 194.866667 15.0   11 195.900000 42.0   12 197.487805 42.0   13 196.862069 29.0   14 197.673333 30.0   15 199.000000 18.0   16 196.658824 17.0   17 199.000000 21.0   18 199.000000 14.0   19 197.756250 17.0   20 199.000000 13.0   21 199.000000 14.0   22 195.460000 20.0   23 199.000000 21.0   24 199.000000 19.0   25 199.000000 26.0     We have explored the dataset and observed some items of interest. The next step is using this information to predict the number of sales over another month\nTime Series Analysis Firstly, let\u0026rsquo;s observe what the sales time series look like for some of the products:\nfig=plt.figure(figsize=(12, 8), dpi= 80, facecolor='w', edgecolor='k') for i, idx in enumerate(selected_sales.index.get_level_values(0).unique()[10:20]): plt.subplot(5,2,i+1) selected_sales.loc[idx]['sold'].plot(style='r.') plt.title(f'Sales for item_id {idx}') plt.tight_layout()  We can see some seasonality patterns in the plotted data, so we will decompose one of the observed time series into a trend and seasonal effects with a multiplicative model $Y(t) = T(t) S(t) r(t)$ where $T(t)$ is the trend, $S(t)$ the the seasonal component and $r(t)$ is the residual.\nfrom statsmodels.tsa.seasonal import seasonal_decompose item_id = 491 sales_product = selected_sales.loc[item_id]['sold'].values dec = seasonal_decompose(sales_product, model='multiplicative', period=12) dec.plot();  We observed that the residual values tend to be close to 1, which means the observed series has a good fit to the seasonal and trend decomposition.\nThis analysis shows that the observed time series have a trend that decreases throughout the months, and a seasonal component, which seems to peak around August and December. This decomposition could be exploited to improve prediction results, however we would require a model that incorporate this seasonality pattern.\nTime series forecasting In this section we will evaluate two different models for time series forecasting: Auto Regressive Integrated Moving Average (ARIMA) and Gaussian Process (GP).\nFirstly, we train the models on each product time series up to month 30. Then, we evaluate for the remaining of months available across all selected products. Our metric will be the Root Mean Squared Error (RMSE) computed with the predicted and ground-truth time series. Please note that this metric is the same as used in the original challenge.\nGaussian Processes GPs are non-parametric models that can represent a posterior over functions. They work well when we do not wish to impose strong assumptions on the generative data process. For a more detailed explanation of GPs, please visit this distill post.\nWe chose experimented with a multitude of kernels, the best performing were the simpler ones: RBF and Matern. Note that the parameters of these kernels are optimised (within the given bounds) during the fit process. Please note that we normalise the target sales by the maximum value such that the target number of sales ranges from [0,1].\nFirst, let\u0026rsquo;s observe what happens using a single item_id and extrapolating between the months:\nfrom sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import WhiteKernel, RBF, Matern, ExpSineSquared, ConstantKernel kernel = RBF() gp = GaussianProcessRegressor(kernel = kernel, n_restarts_optimizer=2, alpha = 1e-5, normalize_y=True)  item_id = 53 X = np.arange(0,34,0.05).reshape(-1,1) Y = selected_sales['sold'][item_id].values.reshape(-1,1) ymax = Y.max() gp.fit(np.arange(30).reshape(-1,1), Y[:30]/ymax) Y_pred, Y_pred_std = gp.predict(X, return_std=True) Y_pred = Y_pred.reshape(-1)*ymax fig=plt.figure(figsize=(6, 4), dpi= 80, facecolor='w', edgecolor='k') plt.plot(X, Y_pred, 'r.', label='Pred') plt.fill_between(X.reshape(-1), Y_pred - Y_pred_std, Y_pred + Y_pred_std, alpha=0.2, color='k'); plt.plot(np.arange(34), Y, 'bx', label='GT') plt.axvline(29.5, 0, 1, color='black', ls='--', label='Train/Test split') plt.legend();  Now we will create a GP for each item_id time series within our selected sales:\nY_preds_gp = [] Y_gts = [] for item_id in selected_sales.index.get_level_values(0).unique(): X = np.arange(34).reshape(-1,1) X_train, X_test = X[:30], X[30:] Y = selected_sales['sold'][item_id].values.reshape(-1,1) Y_train, Y_test = Y[:30], Y[30:] ymax = Y_train.max() kernel = RBF() gp = GaussianProcessRegressor(kernel = kernel, n_restarts_optimizer=0, alpha = 1e-2, normalize_y=True) gp.fit(X_train, Y_train/ymax) ypred = gp.predict(X_test)*ymax Y_preds_gp.append(ypred) Y_gts.append(Y_test) Y_preds_gp = np.concatenate(Y_preds_gp, axis=0) Y_gts = np.concatenate(Y_gts, axis=0)  ARIMA Auto Regressive Integrated Moving Average models the time series using $$Y(t) = \\alpha + \\beta_1 Y(t-1) + \\beta_2 Y(t-2) + \\dots + \\beta_p Y(t-p) + \\gamma_1 \\epsilon(t-1) + \\gamma_2 \\epsilon(t-2) + \\dots + \\gamma_q \\epsilon(t-q) + \\epsilon(t)$$ where $\\epsilon(t)$ is the residual from the ground-truth and estimated value. The parameters $\\alpha, \\beta, \\gamma$ are optimised during fitting. The hyper-parameters $p,d,q$ correspond to the order of the process, i.e. how many terms of previous timestamps, how many previous error terms and how many times to differentiate the time series until it becomes stationary.\nFor simplicity, we assume our time-series are stationary and use $p=1$, $q=0$, $d=0$\nAgain, let\u0026rsquo;s start by visualising a single time-series and the resulting ARIMA prediction.\nfrom statsmodels.tsa.arima_model import ARIMA item_id = 53 Y = selected_sales['sold'][item_id].values.reshape(-1,1) model = ARIMA(Y[:30], order=(1,0,0)).fit(trend='nc') Y_pred = model.predict(0,33) fig=plt.figure(figsize=(6, 4), dpi= 80, facecolor='w', edgecolor='k') plt.plot(X, Y_pred, 'r.', label='Pred') plt.plot(np.arange(34), Y, 'bx', label='GT') plt.axvline(29.5, 0, 1, color='black', ls='--', label='Train/Test split') plt.legend();  We may also observe the approximate distribution of the residuals:\nresiduals = pd.DataFrame(model.resid) residuals.plot.kde();  Next, we forecast for all items in the selected sales for the remaining 4 months (30,31,32,33):\nY_preds_arima = [] for item_id in selected_sales.index.get_level_values(0).unique(): Y = selected_sales['sold'][item_id].values.reshape(-1,1) Y_train, Y_test = Y[:30], Y[30:] ypred = gp.predict(X_test) model = ARIMA(Y[:30], order=(1,0,0)).fit(trend='nc') ypred = model.predict(30,33) Y_preds_arima.append(ypred) Y_preds_arima = np.concatenate(Y_preds_arima, axis=0)  Evaluation Finally computing the RMSE metric between predictions (of GP and ARIMA) and ground-truths for all selected items for the remaining 4 months (30,31,32,33):\nfrom sklearn.metrics import mean_squared_error as mse rmse_gp = mse(Y_gts, Y_preds_gp, squared=False) rmse_arima = mse(Y_gts, Y_preds_arima, squared=False) print(f\u0026quot;RMSE GP {rmse_gp} \\nRMSE ARIMA {rmse_arima}\u0026quot;)  RMSE GP 58.58414105624866 RMSE ARIMA 50.27118697649368  Knowingly, forecasting 4 months into the future would be difficult. Instead we could consider only the 30th month forecast of all items:\nrmse_gp = mse(Y_gts[::4], Y_preds_gp[::4], squared=False) rmse_arima = mse(Y_gts[::4], Y_preds_arima[::4], squared=False) print(f\u0026quot;RMSE GP {rmse_gp} \\nRMSE ARIMA {rmse_arima}\u0026quot;)  RMSE GP 28.212626816686967 RMSE ARIMA 7.92125195871772  Conclusion We observed that the ARIMA model performed better under the RMSE metric for the presented dataset. Still, both RMSE are quite high and should be reduced if the model were to be used in practice.\nTo further improve the results and to account to a more realistic setting where some items or shops will have a limited history of sales, we must work on feature engineering to overcome these limitations. Example of features that could be exploited include, but are not limited to:\n Shop-specific aggregated stats (mean num of sales) City-specific aggregated stats (mean num of sales, pop. size, etc). Item categories aggregated stats (mean num of sales for a specific product category) Item price and price variations  Further reading If you are interested in time series forecasting I highly recommend the above blog posts:\n  https://www.linkedin.com/pulse/how-use-machine-learning-time-series-forecasting-vegard-flovik-phd  https://towardsdatascience.com/an-overview-of-time-series-forecasting-models-a2fa7a358fcb  ","date":1595516277,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595516277,"objectID":"881be2be6e9426f6c868fc0b33860762","permalink":"https://earnold.me/post/timeseriesforecasting/","publishdate":"2020-07-23T15:57:57+01:00","relpermalink":"/post/timeseriesforecasting/","section":"post","summary":"Problem definition Sales forecasting is a very common problem faced by many companies. Given a history of sales of a certain product we would like to predict the demand of that product for a time window in the future.","tags":[],"title":"Forecasting sales with Gaussian Processes and Autoregressive models","type":"post"},{"authors":["Johannes Hiller","Sami Koskinen","Riccardo Berta","Nisrine Osman","Ben Nagy","Francesco Bellotti","Ashfaqur Rahman","Erik Svanberg","Hendrik Weber","Eduardo H Arnold"," others"],"categories":null,"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588291200,"objectID":"c963ea8e81221d31e4571652d9f24776","permalink":"https://earnold.me/publication/hiller-2020-l-3-pilot/","publishdate":"2020-07-14T09:14:31.801929Z","relpermalink":"/publication/hiller-2020-l-3-pilot/","section":"publication","summary":"","tags":null,"title":"The L3Pilot Data Management Toolchain for a Level 3 Vehicle Automation Pilot","type":"publication"},{"authors":[],"categories":[],"content":"This talk was presented with my colleague Dr Graham Lee who is a lead engineer with the Intelligent Vehicles group at WMG. Autonomous vehicles are set to transform the worldwide economy. This presentation will highlight the challenges and opportunities in the development and deployment of autonomous vehicles. Exploring the role of connectivity between vehicles and infrastructure is key to achieve full autonomy. Novel research in platooning, cooperative perception and trajectory negotiation will be discussed along with methods for testing and validation of autonomous driving functions.\n","date":1582824600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595240714,"objectID":"66e3d8bf1cf175d8406dc94fbc3c40f3","permalink":"https://earnold.me/talk/iet2020/","publishdate":"2020-02-27T18:30:00+01:00","relpermalink":"/talk/iet2020/","section":"talk","summary":"Autonomous vehicles are set to transform the worldwide economy. This presentation will highlight the challenges and opportunities in the development and deployment of autonomous vehicles. Exploring the role of connectivity between vehicles and infrastructure is key to achieve full autonomy. Novel research in platooning, cooperative perception and trajectory negotiation will be discussed along with methods for testing and validation of autonomous driving functions.","tags":[],"title":"IET Talk - Challenges and Opportunities in Connected and Autonomous Vehicles","type":"talk"},{"authors":[],"categories":[],"content":"","date":1573234200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595240752,"objectID":"14a4dbef9635aa975aee55e8a8bcae3c","permalink":"https://earnold.me/talk/brumai2019/","publishdate":"2019-11-08T18:30:00+01:00","relpermalink":"/talk/brumai2019/","section":"talk","summary":"This presentation provides an introduction to the challenges in the perception domain of autonomous vehicles and provides solutions based on cooperation among vehicles and infrastructure, particularly focusing on 3D object detection.","tags":[],"title":"Cooperative perception for 3D object detection in challenging driving scenarios using infrastructure sensors","type":"talk"},{"authors":["Eduardo Arnold","Omar Y Al-Jarrah","Mehrdad Dianati","Saber Fallah","David Oxtoby","Alex Mouzakitis"],"categories":null,"content":"","date":1560038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560038400,"objectID":"aff5a9378c59515f7b7d219b63a5adbe","permalink":"https://earnold.me/publication/arnold-2019-cooperative/","publishdate":"2020-07-14T09:14:31.801492Z","relpermalink":"/publication/arnold-2019-cooperative/","section":"publication","summary":"3D object classification can be realised by rendering views of the same object from different angles and aggregating all the views to build a classifier. Although this approach has been previously proposed for general objects classification, most existing works did not consider visual impairments. In contrast, this paper considers the problem of 3D object classification for driving applications under impairments (e.g. occlusion and sensor noise) by generating an application-specific dataset. We present a cooperative object classification method where multiple images of the same object seen from different perspectives (agents) are exploited to generate more accurate classification. We consider model generalisation capability and its resilience to impairments. We introduce an occlusion model with higher resemblance to real-world occlusion and use a simplified sensor noise model. The experimental results show that the cooperative model, relying on multiple views, significantly outperforms single-view methods and is effective in mitigating the effects of occlusion and sensor noise.","tags":null,"title":"Cooperative Object Classification for Driving Applications","type":"publication"},{"authors":["Francesco Bellotti","Riccardo Berta","Ahmad Kobeissi","Nisrine Osman","Eduardo Arnold","Mehrdad Dianati","Ben Nagy","Alessandro De Gloria"],"categories":null,"content":"","date":1560038400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560038400,"objectID":"6099b8fe417b6f90978bc44aa1b6dbb0","permalink":"https://earnold.me/publication/bellotti-2019-designing/","publishdate":"2020-07-14T09:14:31.801712Z","relpermalink":"/publication/bellotti-2019-designing/","section":"publication","summary":"","tags":null,"title":"Designing an IoT Framework for Automated Driving Impact Analysis","type":"publication"},{"authors":[],"categories":[],"content":"The bayesian linear regression formulation allows to obtain uncertainty estimates for the predictive distribution that are not available in its point-wise estimate counterpart. This notebook is based on Chapter 3 of Bishop\u0026rsquo;s Pattern Recognition and Machine Learning book.\nThis notebook can be downloaded here.\nimport numpy as np from scipy.stats import multivariate_normal import matplotlib.pyplot as plt %matplotlib inline  Generate sample dataset Generate N pairs $(x_i,y_i)$ with gaussian noise and $x_i$ sampled from uniform distribution\nN = 12 sigma = 0.1  x = np.random.uniform(low=-1, high=1, size=N) n = np.random.normal(loc=0, scale=sigma, size=N) y = 0.3*x -0.8 +n  plt.plot(x,y, 'r.'); plt.show()  Point estimate We are trying to design a model $\\hat{y} = x w_1 + w_0 + \\epsilon$ with $\\epsilon \\sim N(0, \\sigma^2)$\nNote that this model and noise assumption result in the following likelihood function: $$p(\\hat{y}|x,w) = N(xw_1+w_0, \\sigma)$$\nIn general we aim for the Lease Squares (LS) solution: $$\\min_w \\sum_i (y_i-\\hat{y}_i)^2$$\nNote that the LS solution is equivalent to the Maximum Likelihood Estimator. The solution can be obtained through minimizing the loss function through Gradient Descent. However, in the case of this simple linear model it is possible to use normal equations (closed form minimization result): $$\\hat{w} = (X^TX)^{-1}X^Ty$$\nX = np.zeros((x.shape[0], 2)) X[:,0] = x X[:,1] = 1 X  array([[ 0.07747538, 1. ], [-0.72983355, 1. ], [-0.08385175, 1. ], [ 0.04152017, 1. ], [-0.27236207, 1. ], [-0.16471106, 1. ], [ 0.43409736, 1. ], [-0.33582112, 1. ], [-0.48323886, 1. ], [ 0.54369188, 1. ], [-0.29194542, 1. ], [ 0.98406384, 1. ]])  w = np.dot(np.dot(np.linalg.inv(np.dot(X.T,X)), X.T), y) w  array([ 0.28106915, -0.75913605])  However, this solution only provides a point estimate and lacks uncertainity information.\nBayesian inference In turn, a bayesian approach treat $w$ as a RV which has a prior. Then, bayesian inference is used to obtain the posterior $p(w|X,Y)$ given observations\nIn order to keep the solutions in closed-form, we use a Gaussian prior, allowing for a conjugate prior, for the vector $w$ $$w \\sim N(w| m_0, S_0)$$\nWhich then results in a Gaussian posterior\n$$p(w|X,Y) = \\frac{p(Y|X,w)p(w)}{p(Y|X)} = N(w| m_N, S_N)$$ where $m_N = S_N (S_0^{-1}m_0+\\frac{1}{\\sigma}X^Ty)$ and $S_N^{-1} = S_0^{-1}+\\frac{1}{\\sigma}X^TX$\nFor simplicity, let\u0026rsquo;s assume $m_0 = 0$ and $S_0 = \\alpha^{-1}I = 0.5I$\n#prior parameters a = 0.2 m0 = np.zeros(2)  def getPosterior(n): #Get n points from sample dataset x_ = X[:n] y_ = y[:n] #Covariance Matrix S0I = a*np.identity(2) SnI = S0I+ 1/sigma*np.dot(x_.T,x_) Sn = np.linalg.inv(SnI) #Mean tt = np.dot(S0I, m0) + 1/sigma*np.dot(x_.T,y_) Mn = np.dot(Sn, tt) return multivariate_normal(mean=Mn, cov=Sn)  def plot_dist2D(dist): x, y = np.mgrid[-1:1:.01, -1:1:.01] pos = np.empty(x.shape + (2,)) pos[:, :, 0] = y; pos[:, :, 1] = x plt.contourf(x, y, dist.pdf(pos)) plt.title('Posterior Distribution $p(w|X,Y)$') plt.xlabel('w0') plt.ylabel('w1')  Posterior distribution plots We can plot the posterior after aggregating different number of points. Observe how the posterior distributions become narrower when more observation are aggregated\nplot_dist2D(getPosterior(1))  plot_dist2D(getPosterior(4))  plot_dist2D(getPosterior(6))  plot_dist2D(getPosterior(10))  The full posterior (when all points are incorporated) will have a peak on the mean, $w_{MAP} = m_N$, given the Gaussian distribution. In the case where the prior $p(w)$ is infinitely spread ($a \\to 0$), $w_{MAP} = m_N = w_{ML} = (X^TX)^{-1}X^Ty$\nThe predictive distribution Although we have estimated the posterior of parameters $w$, we are primarily interested in predicting the value of $\\hat{y}$ for new sample x: $$p(\\hat{y}| x, X,Y) = \\int p(y|w)p(w|X,Y) dw$$\nGiven the likelihood and posterior following Gaussian distributions, this predicitive distribution is also Gaussian: $$p(\\hat{y}| x, X,Y) = N(\\hat{y}| m_N^Tx, \\sigma_N^2(x))$$ where $ \\sigma_N^2(x) = \\sigma^2 + x^TS_Nx $\nNote that the variance of the predictive distribution depends both on the assumed noise model ($\\sigma$) and the uncertainty on the $w$ posterior\ndef predictive(x, nTrainingSamples): xp = np.zeros((2,1)) xp[0,0] = x xp[1,0] = 1 xp = np.matrix(xp) #Get posterior given nTrainingSamples posterior = getPosterior(nTrainingSamples) Mn = np.matrix(posterior.mean) Sn = np.matrix(posterior.cov) #Predictive mean m = np.matmul(Mn,xp) #Predictive cov s = sigma**2 + np.dot(xp.T, np.dot(Sn,xp)) return multivariate_normal(mean=m, cov=s)  def plot_dist1D(dist): x = np.linspace(-4,4, 100) y = dist.pdf(x) plt.plot(y,x) plt.title('Predictive Distribution $p(\\hat{y}|x, X,Y)$') plt.xlabel('pdf') plt.ylabel('$\\hat{y}$')  We now observe how the predictive distributions become more certain as more training data is obtained #New values of x where we want to predict y x = 1.2  plot_dist1D(predictive(x, 2))  plot_dist1D(predictive(x, 6))  plot_dist1D(predictive(x, 12))  We would also observe how the uncertainity changes with the values of x plot_dist1D(predictive(1.2, 12))  plot_dist1D(predictive(2, 12))  plot_dist1D(predictive(3, 12))  plot_dist1D(predictive(6, 12))  The predictive distribution variance grows as x increases, as expected from $\\sigma_N(x)$\n","date":1552418995,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552418995,"objectID":"e71ef56dcee9097146dc8f749cf7aec1","permalink":"https://earnold.me/post/bayesianlr/","publishdate":"2019-03-12T20:29:55+01:00","relpermalink":"/post/bayesianlr/","section":"post","summary":"Despite the popularity of standard linear regression, its bayesian counterpart is not as well-known. In this tutorial we explore its benefits and learn how to build it from scratch in Python with NumPy.","tags":[],"title":"Bayesian Linear Regression","type":"post"},{"authors":["Eduardo Arnold","Omar Y Al-Jarrah","Mehrdad Dianati","Saber Fallah","David Oxtoby","Alex Mouzakitis"],"categories":null,"content":"","date":1547856e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547856e3,"objectID":"18b893621b8ae1d5d9d0c5d7bb098906","permalink":"https://earnold.me/publication/arnold-2019-survey/","publishdate":"2020-07-14T09:14:31.801192Z","relpermalink":"/publication/arnold-2019-survey/","section":"publication","summary":"An Autonomous Vehicle (AV) requires an accurate perception of its surrounding environment to operate reliably. The perception system of an AV, which normally employs machine learning (e.g., deep learning), transforms sensory data into semantic information that enables autonomous driving. Object detection is a fundamental function of this perception system that has been tackled by several works, most of which use 2D detection methods. However, 2D methods do not provide depth information, which is required for driving tasks, such as path planning, collision avoidance, etc. Alternatively, 3D object detection methods introduce a third dimension that reveals more detailed object’s size and location information. Nonetheless, the detection accuracy of such methods needs to be improved. To the best of our knowledge this is the first survey on 3D object detection methods used for autonomous driving applications. This paper presents an overview of 3D object detection methods and prevalently used sensors and datasets in AVs. It then discusses and categorizes recent works based on sensors modalities into monocular, point cloud-based and fusion methods. We then summarize the results of the surveyed works and identify research gaps and future research directions.","tags":null,"title":"A Survey on 3D Object Detection Methods for Autonomous Driving Applications","type":"publication"},{"authors":null,"categories":null,"content":"People doing research nowadays can benefit a lot from using Jupyter notebooks. It is the case specially with machine learning and data science, where experimentation is frequent and code changes rapidly, as well as new insights are drawn from data and previous experience. In these cases it is useful to have proper documentation of the decision making process and some history of previous attempts to solve a problem.\nAt LCS we have a server with fair computing capabilities, including a GTX 1060 GPU. We were looking for a solution that allowed to share this server\u0026rsquo;s resources without a big overhead on the user end, such as having each user installing packages and configuring their environment. The solution we envisioned was to deploy our own JupyterHub instance on the server, with the additional challenge of sharing the GPU.\nSimply put, the JupyterHub service spawns a single-user Jupyter notebook for each user that connects to the server, allowing an easy solution for resource sharing, environment configuration and user isolation. There are many options that must be configured, such as the authentication service and spawner used. Since lately I have been using Docker for service isolation I was glad to know that there was a builtin spawner that created a Docker container for each user.\nThe default configuration required the JupyterHub service to be installed on the host machine and provided no isolation from other services/processes. There is the option to create a container with JupyterHub and use the Docker spawner, but it requires a Docker container to launch other containers, quite an inception, huh? To do that we share the Docker socket with the JupyterHub container so that it has access to the host Docker service, enabling it to launch other containers. Although it is not the best solution security-wise, it is the best we could do at the time.\nThis solution seemed like a lot of hassle to put a service online. Fortunately, the team behind JupyterHub did a great job and provided a reference deployment for a single host using Docker. It is important to note that this is not intended to be used in production, since it does not scale well for a very large number of users, in which case it would be recommended to use a kubernetes based deployment. Despite of that, as we are a small research group, this single host solution should be enough.\nThe reference deployment uses docker-compose to make the service easy to setup and manage. After configuring a Github application for user authentication and the Let\u0026rsquo;s Encrypt SSL keys (using this script), I was able to have a working service in a couple of hours. It should have taken less time, but I had to deal with a problem using the docker-compose file specific to the Let\u0026rsquo;s Encrypt configuration, check here.\nSo we now have a working version of JupyterHub, but we still need to make the host GPU available inside the notebook container, which turns out to be the greatest challenge. There is a quite useful wrapper to do so called nvidia-docker. Although we cannot use it directly this time since the JupyterHub\u0026rsquo;s Docker spawner plugin directly calls Docker through the API. Following Andrea Zonca\u0026rsquo;s blog post we can configure the spawner to share the GPU resources.\nFirst of all, we need to get the correct flags for the specific driver and devices:\ncurl -s localhost:3476/docker/cli  Which in our machine outputs\n--volume-driver=nvidia-docker --volume=nvidia_driver_384.59:/usr/local/nvidia:ro --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia0  We then add these options to the spawner configuration in jupyterhub_config.py, which will cause the same effect as spawning the containers using the nvidia-docker wrapper:\nc.DockerSpawner.read_only_volumes = {\u0026quot;nvidia_driver_384.59\u0026quot;:\u0026quot;/usr/local/nvidia\u0026quot;} c.DockerSpawner.extra_host_config = { \u0026quot;devices\u0026quot;:[\u0026quot;/dev/nvidiactl\u0026quot;,\u0026quot;/dev/nvidia-uvm\u0026quot;,\u0026quot;/dev/nvidia0\u0026quot;] }  Please note that some of this configuration options are overwritten later in the file, so other than adding these lines you should make sure they remain active. You can also check my own version of the jupyterhub_config.py file.\nFinally, the last step is to create our personalized notebook container image including all the required packages used in our research. Since we want CUDA support, we must use a CUDA enabled image as base, such as the one supported by NVIDIA nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04. A notebook image must have a start script that will be called upon container creation to start the Jupyter notebook server. I have modified the base image to be based on the NVIDIA CUDA image, as seen here. I have also created a specific image for our lab, inheriting from this image and containing software libraries such as OpenCV, PyTorch, TensorFlow and Keras, you can check it here.\nTo check that everything worked as expected and the GPU is accessible from within the container you input the following on a Jupyter notebook command cell: !nvidia-smi. It should return the status of GPU. We now have a nice platform to develop our models and share computing resources.\n","date":1507549482,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1507549482,"objectID":"433de0aec2869560b9652b4d03faa4f5","permalink":"https://earnold.me/post/jupyterhub/","publishdate":"2017-10-09T09:44:42-02:00","relpermalink":"/post/jupyterhub/","section":"post","summary":"A brief summary of my experience deploying JupyterHub at our lab.\n","tags":["devops","docker"],"title":"Deploying JupyterHub with Docker and CUDA support","type":"post"},{"authors":null,"categories":null,"content":"Motivation To specialize further in image classification I decided to try a classic challenge in this area: the Kaggle competition Dogs vs Cats. By taking part in the challenge we can easily use their data set (25k labeled images) and 12.5k unlabeled test images. Each sample from the data set is a varied-sized RGB image containing either a dog or a cat and our task is to decide to which class the sample belongs. We can also compare our results to other users and have some indicatives of where our model could improve.\nIn particular, in this post I would like to address the following topics: transfer learning, the effects of the input image size and regularization on the model performance.\nTransfer learning approach Transfer learning is the technique of using previously trained models for a specific purpose on a different problem. This can be done by tuning the model parameters on the new problem data set. It is advantageous because the trained model might have valuable feature-extraction capabilities that can be useful if the application domain of the two problems are similar (i.e., object classification). So instead of training the whole model from scratch we just fine-tune the few last layers (responsible for the classification) to the new problem data set.\nBefore designing my own architecture I considered fine-tuning good performing models on hard challenges such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC). One of these models is the winner of the ILSVRC 2014 Classification+localization challenge, VGG-16 model, organized in 5 convolutional blocks and a dense block. The convolutional blocks group 2-3 convolutional layers in sequence with a max-pooling layer to reduce dimensionality. In total there are 13 convolutional layers and 3 dense layers. The original challenge data set had 1000 classes so the output layer has 1000 output units.\n  VGG16 Macroarchitecture. Credits to Davi Frossard @ Stanford.   Since we are only dealing with two classes we must adapt the architecture by replacing the three dense layers with a single dense layer containing two softmax activation units. It is reasonable to replace the three layers with a single one because the model has enough complexity, so the features at this point (~ 25k of them) are already high level ones, which can easily determine the image class without further layers.\nTo fine tune the model parameters to dogs-vs-cats classification we assume the 13 conv-layers provide good enough features, so we freeze their parameters and allow only the last layer parameters to be updated. This model has a large number of parameters: the 13 conv-layers provide 14,714,688 parameters while the final dense layer has 50,178 parameters. Clearly it is much simpler to fit just these final layer parameters instead of the whole network, which allows us to use our modest data set on this structure.\nThe original model image input size was 224 x 224, and although this changes the dimension of the following convolutional layers, their filter sizes are kept the same, so we can still use the weights from the original model, except for the final dense layers, where the weights depend upon the number of units in each layer. Since we replaced the final dense layers we could have chosen any given input size. To keep the original design we kept the standard input dimension. Another consideration is that the pre-processing should be the same assumed by the VGG model, that is, the pixel intensities are not normalized and are kept in the original range $[0,255]$.\nIn order to evaluate the model we split the labeled samples into 20k training samples and 5k test samples, since the 12.5k samples provided for evaluation are not labeled. After 10 training epochs we achieved great performance on both the training and test set, as seen on the learning curve below. You can check the notebook with this experiment here.\nAnalyzing the results we clearly see that the model performs very well both on training and test sets with accuracy higher than 95% on the test set. This performance can be explained by the complexity of the model and the amount of information it has been trained upon. Even though it performs well and was quite simple to implement, it is a rather heavy model with ~14 million parameters. This leads to the question of whether we can get a similar performance using a simpler, more computational efficient model.\nCustom model architecture: dognet Since the VGG model was successful in the previous attempt, it is worth considering its architecture as base to a simpler custom model called dognet. Similarly, we organize the convolutional blocks with two conv layers followed by a max-pooling layer, and a final single filter conv layer to merge all feature maps into one. So the new model architecture will have 7 convolutional layers, all using 3x3 filters and RELU activation, with decreasing number of filters: 32, 32, 16, 16, 4, 4, 1. After flattening the last conv layer, a dense layer with 2 outputs and softmax activation is used as the network output, interpreted as class probabilities.\nA important difference in this model is that we insert a batch normalization layer after the input layer with the purpose of normalizing the input across the entire batch, which leads to faster convergence and improved training. Note that the input images pixels values are also scaled into the range of $[0,1]$ by multiplying all the channels by $\\frac{1}{255}.$\nThe proposed model uses 18,829 parameters, less than 1% of the amount of VGG parameters. We still assume the input image size as 224x224. The first experiment was to check the model had enough capacity to fit the data, meaning it would not underfit the data. So we trained using Adadelta optimizer with no regularization for about 100 epochs. We were able to obtain 100% accuracy on the training set, which proves the model had enough capacity for the data. The performance on the test set, however, was not so satisfactory: 79% accuracy. This suggests a strong overfitting, which is expected as no regularization was used.\nSize matters? An issue faced during the first experiment was the time required to train the model: using only an i5 CPU (4 cores) each epoch took about 40 minutes, so training for a 100 epochs took a few almost three days. To reduce training time the images were resized to 64x64 pixels, which had astonishing effects on the temporal performance: an epoch took only 200s to train. This input downscaling did not impact the amount of model parameters so much: there are now 17,869, opposed to the early 18,829 parameters. This is reasonable since most of the network is convolutional, so the number of parameters will not change with the input size, just on the dense layers, which happens to be the case of the final one. In this case very few parameters depend on the input size.\nThis downscaled input model, with 100 epochs of training, achieved 98% accuracy on the training set and 80% accuracy on the test set, so still overfitting. This performance is very similar to the one observed on the original input size (224x224), which leads to thinking that in this case downscaling the input is a good choice since it allows faster training times without affecting the model performance.\nRegularization Previous training attempts resulted in model overfitting, which happens when the model performs well on the training set but poorly on the test set. It is generally caused because the model capacity is greater than the volume of data available for training, thus causing the model to specialize on fine characteristics (noise) specific to the training data and prevent the generalization expected in the test set. To overcome this problem regularization techniques must be employed to balance the model capacity and the amount available data, enforcing that simpler, more general models are obtained.\nIn order to observe this effect in more detail we can observe the following figure, showing the progression of the model loss and accuracy as the training process evolve. It is clear that overfitting becomes pronounced around epoch 20, and the test accuracy becomes steady at 80% from this point forward.\n  No regularization training curves.   L2 norm Firstly, L2-norm regularization is applied. This method adds a regularization term on the loss function corresponding to the L2-norm metric of a layer weights, penalizing large parameters of the network, as shows the next equation, where $L_t(w)$ is the loss correspondent to training errors (in this case log-loss).\n$$ L(w) = L_t(w) + \\lambda \\Vert w \\Vert^2 $$\nThis means the model is looking for finding parameters that both result in low training error, but are also small, which means simpler models and more generalization. This trade-off is tuned through the hyper-parameter $\\lambda$, with higher values prioritizing model simplification. The L2 specific effect is introducing a negative parcel to the parameters updates (coming from the gradient of the regularization term), so the weights tend to become smaller at each iteration.\nBy applying this technique with moderate regularization $\\lambda=10^{-3}$ on all layers, we obtained the following progression of model loss and accuracy. A note aside is that the model should be randomly initialized for training, if the previous weights (obtained without regularization) are used for initialization the model will not be able to improve generalization.\n  L2 moderate regularization training curves.   The model still overfits, but observe that the test set accuracy has actually improved, now becoming steady at 85%, showing that regularization has improved the model generalization on new samples.\nWe could try a stronger regularization with $\\lambda=10^{-2}$, but perhaps we could enhance regularization by aggregating a different technique called dropout.\nDropout Dropout works by deactivating a set of random units of a given layer during training phase. Such practice have a good impact on the network because it removes inter-layer co-adaptation, thus avoiding any unit to rely too much on a single previous unit, so enabling generalization. The strength of the regularization is controlled by a hyper-parameter that represents the rate of the layer units to drop. It is commonly applied before dense layers, but could also be used for convolutional ones.\nCombining both L2-norm with $\\lambda=10^{-3}$ on all layers and dropout regularization with ratio 0.5 (half units dropped) on convolutional layers, we obtain the following loss and accuracy:\n  Dropout and L2 regularization training curves.   The results show that combining L2 and dropout regularization provides a good solution to prevent model overfitting. Even though the model did not overfit we did not observe any improvement on the test set accuracy if compared to L2 only regularization. Another interesting characteristic is the chaotic behavior of the test loss function, which is due to the probabilistic nature of the unit drops, albeit the mean value of the test loss seems to follow the train loss, which is a good indicative. The notebook containing the dognet architecture along with regularizers is available here.\nConclusion In this specific application the downscaling of input image size did not have a bad impact on model performance and was actually capable of greatly decreasing the training time. Although this may not be the case when dealing with samples containing fine details where two distinct classes look similar to one another.\nThrough the series of experiments applying regularization techniques it was possible to prevent model overfiting and improve generalization measured as an increase on test set accuracy, getting up to 85% accuracy. Combining both L2 and dropout proved to be a good idea.\nIn order to further increase model accuracy on the test set it would be necessary to gather more training data and possibly apply data augmentation (rotation, scaling, cropping). Another possibility is to use the transfer learning approach, which uses already trained high-level models, offering higher than 96% accuracy on the test set, at the expense of having a much larger model and longer processing times.\nFinally, a note on the Kaggle competition. I think Kaggle has an important role on promoting machine learning and its applications, in engaging researchers and hobbyists alike and even helping people to learn more about recent techniques, letting them compare results, etc. Still, it seems to fail to consider the complexity of the elaborated solutions. To illustrate my point check the Marco Lugo post on Kaggle blog showing his 3rd place solution to this challenge. He uses a bunch of complex convolutional models in parallel and predict the output as a weighted average of these classifiers. Of course great results come at the price of increased complexity. I am just saying that complexity should also be taken in account when evaluating model. We may ask ourselves: what is the best score I can get using only the training data they provide, without any pre-loaded models, or an ensemble of 20 complex classifiers? The way Kaggle is organized today does not allow an answer to that question.\n","date":1494870719,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1494870719,"objectID":"3dbdd8d15da907e6a73f5e61d1ab1b36","permalink":"https://earnold.me/post/dogsvscats/","publishdate":"2017-05-15T14:51:59-03:00","relpermalink":"/post/dogsvscats/","section":"post","summary":"A couple of lessons discovered when exploring the Dogs vs cats image classification challenge.\n","tags":["machine learning","deep learning"],"title":"What I have learnt from dogs and cats","type":"post"},{"authors":null,"categories":null,"content":"Introduction I have used a handful of machine learning models in the past. These include simple linear regression models, support vector machines (SVMs) and neural networks. While working on different projects I was mostly concerned in solving a specific application problem and did not worry about the inner workings of the models I was using. I have delved into more details of SVM during my final undergraduate project but did not have the time to do the same for neural networks.\nAfter reading and learning about it in more detail it has come the time for me to share it. In this post I will give some brief introduction to neural nets and derive some of the maths behind a simple architecture as well as its implementation using only Numpy. I have based most of this post on Stanford\u0026rsquo;s CS231 notes which I found very useful for beginners. Some of the math derived here was not available there so it might be useful for a more interested reader.\nNeural networks are organized in layers, each consisting of many units. These units are responsible for crunching the data. Each unit can receive many inputs but has a single output that can be connected to many more units in the following layer. In this study we will address a fully-connected structucture, meaning that every unit in a single layer receives input from all the units from the previous layer. To compute its output, each unit perform a weighted sum of its inputs and a bias term, then apply an activation funcion, in general non-linear, to the calculated sum. The model parameters consists of the connection weights between the units and their bias terms.\nDefining the model To simplify our study we are going to use a two layer network architecture (the input layer is not counted). The input layer $x$ has $N_x$ units, the hidden layer $h$ has $N_h$ and the output layer $y$ has $N_y$ units.\nWe could specify this architecture to perform multi-class classification, so the output layer would give the probability the sample belongs to each of the available classes. In this case we use $N_y$ as the number of classes and to enforce a probabilistic output we use an activation function called softmax on the output layer, which also guarantess that the sum of the classes probabilities sums to 1.\nAs for the hidden layer we can use a RELU activation function, because it is avery simple to evaluate non-linear function: $\\text{RELU}(z) = \\max(0, z)$. There are other reasons why we would use it, mostly to avoid a problem called vanishing gradient, but in a shallow network such as this one this would not be a problem.\nWe define the weights of the connections between the input and hidden layer units as the matrix $W1 \\in \\mathbb{R}^{N_x \\times N_h}$, where $W1_{i,j}$ is the weight of the connection between the $i$-th input layer unit to the $j$-th hidden layer unit. The bias vector of the hidden layer is defined as $b1 \\in \\mathbb{R}^{N_h}$. Similarly we have the parameters of connection between the hidden layer to the output layer: $W2 \\in \\mathbb{R}^{N_h \\times N_y}$ and $b2 \\in \\mathbb{R}^{N_y}.$\nForward propagation Assuming we have all the ideal model parameters $W1, W2, b1, b2$, how do we get the output of the network for a given input sample? This is called forward propagation, since the data flows from the input layer, through the hidden layers and finally to the output layer as presented in the following equations.\n$$\\begin{align*} \\\\ h \u0026amp;= \\max(0,x W1 + b1) \\\\ y \u0026amp;= \\text{softmax}(\\underbrace{h W2 + b2}_\\textrm{score}) \\end{align*}$$\nThe softmax activation function can be expressed as $$ \\text{softmax}(z)_j = \\frac{e^{z_j}}{\\sum_{i=0}^{N_z}e^{z_i}} $$\nThis formulation allows the input $x$ to have many samples, each in a row, so $x \\in \\mathbb{R}^{n \\times N_\\text{features}}$, where $n$ is the number of samples in a batch, also called batch size, and $N_\\text{features}$ is the number of features, or the dimension of each sample. This would yield an output $y \\in \\mathbb{R}^{n \\times N_y}$, where each sample probability distribution among classes is given in a row.\nTraining process In order to get the ideal model parameters we have to train the network on a training set. This consists of an optimization process where we try to minimize a loss function that tells how close the network output $y$ is to the real labels $\\hat{y}$. At each iteration of this process we obtain new values for the parameters that will hopefully decrease the value of the loss function.\nIn this example, since we assumed a multi-class classification problem with a probabilistic output, the ideal loss function to use is the categorical cross-entropy function, given by $$ L_i = -\\log y_{\\hat{y}_i}$$\nTo make sense of this function we can analyse its behaviour. For a given sample $x_i$ belonging to class $\\hat{y}_i$ it will compute the negative log of the output probability of the sample belonging to classs $\\hat{y}_i$ (given by the $\\hat{y}_i$-th component of $y$). Ideally this probability would be 1, which would make $L_i=0$. Whenever this is not the case, there is a loss associated with the sample $x_i$.\nNow we have a measure to evaluate how good our classifier is doing on the training set we can try to optimize the network parameters to get the loss as close to zero as possible. We do this using an iterative algorithm called Stochastic Gradient Descent. To give a brief overview of this method, assume all network parameters are represented in a vector $\\theta$. We can compute the variation of the loss function $\\Delta L$ given a variation of the parameters vector $\\Delta \\theta$ as $$\\Delta L = \\Delta \\theta \\cdot \\nabla L$$ where $\\nabla L$ is the gradient of the loss with respect to the parameters $\\theta$. We always want to descrease the loss, so we want $\\Delta L \u0026lt; 0$. One way to guarantee this condition is to choose the variation of parameters as $$\\Delta \\theta = - \\eta \\nabla L$$ for a small-enough learning rate $\\eta \u0026gt;0$, which would yield $$\\Delta L = -\\eta |\\nabla L|^2.$$\nThis description is for vanilla Gradient Descent, also called batch-GD (where $L = \\sum L_i$), so every sample is considered in the gradient. The stochastic part comes when you only consider a single sample at each iteration step, what reduces training time, especially on big datasets. Although it may seem attractive, this method offers slower convergence since there is a lot of zig-zagging between samples optimizations. To overcome this another variation called mini-batch GD can be used. This method lies in between batch-GD (uses one sample) and SGD (uses all samples), since it considers a mini-batch of size $N_b$ to compute the gradient, thus reducing training time and still allowing faster convergence.\nBy iteratively running this optimization algorithm we can reduce the loss function and train our model. There is one missing step though: how to compute the loss function gradient $\\nabla L.$\nBackpropagation and gradient computation Even considering our small network the loss is a rather complex function of the network parameters given all multiplications and non-linear activations. To compute its gradient $\\nabla L$ we must find the derivative of $L$ with respect to all model parameters, which can be difficult to be done analytically. We then use an algorithm called Backpropagation, which is basically the use of calculus' chain-rule. By multiplying the local derivatives from layer to layer we can numerically evaluate the derivative of the loss with respect to any parameter.\nWe start from the output layer $y$. Since $$ L_i = -\\log y_{\\hat{y}_i}$$ we have $$\\frac{\\partial L_i}{\\partial y_k} = \\frac{-1}{y_k} 1(\\hat{y}_i=k)$$ where 1(z) is the indicator function (1 if argument true, otherwise 0).\n$\\DeclareMathOperator{\\score}{score}$ We then calculate the derivatives of the output $y_k$ with respect the intermediate variable $\\score = h W2 + b2$, with $ y = \\text{softmax}(h W2 + b2)$. We must consider two cases, one for the derivative of $y_k$ with respect to $\\score_j$ with $k \\neq j$ and another with $k=j$.\nFor the first case we can write $$ y_k = \\frac{e^{\\score_k}}{e^{\\score_j} + \\sum_{i \\neq j} e^{\\score_i}}$$ then using the quotient rule for derivatives we have: $$\\frac{\\partial y_k}{\\partial \\text{score}_j} = \\frac{-e^{\\score_k}e^{\\score_j}}{(\\sum_i e^{\\score_i})^2} = -\\frac{e^{\\score_k}}{\\sum_i e^{\\score_i}} \\frac{e^{\\score_j}}{\\sum_i e^{\\score_i}} = -y_k y_j$$\nFor the second case, when $k=j$ we can write $$ y_k = \\frac{e^{\\score_k}}{e^{\\score_k} + \\sum_{i \\neq k} e^{\\score_i}}$$ and then the derivative becomes:\n\\begin{eqnarray} \\frac{\\partial y_k}{\\partial \\text{score}_k} \u0026=\u0026 \\frac{e^{\\score_k}(\\sum_i e^{\\score_i}) - e^{2\\score_k}}{(\\sum_i e^{\\score_i})^2} \\\\ \u0026=\u0026 \\frac{e^{\\score_k}}{\\sum_i e^{\\score_i}} - \\frac{e^{\\score_k}}{\\sum_i e^{\\score_i}} \\frac{e^{\\score_k}}{\\sum_i e^{\\score_i}} \\\\ \u0026=\u0026 y_k-y_k y_k \\\\ \u0026=\u0026 y_k(1-y_k) \\end{eqnarray} Now, to calculate the derivative of the loss with respect to the score intermediate variables we use the chain-rule as follows:\n\\begin{eqnarray} \\frac{\\partial L_i}{\\partial \\score_j} \u0026 = \u0026 \\frac{\\partial L_i}{\\partial y_k} \\frac{\\partial y_k}{\\partial \\score_j} \u0026\\\\ \u0026 = \u0026 \\frac{-1}{y_{\\hat{y}_i}} \\times -y_{\\hat{y}_i} y_j = y_j, \u0026\\text{ if } j \\neq \\hat{y}_i \\\\ \u0026 = \u0026 \\frac{-1}{y_{\\hat{y}_i}} \\times y_{\\hat{y}_i}(1-y_{\\hat{y}_i}) = y_{\\hat{y}_i} -1, \u0026\\text{ if } j=\\hat{y}_i \\end{eqnarray} For implementation reasons we can call a vector $\\text{dscore} \\in \\mathbb{R}^{N_y}$ where each component is the derivative of the loss regarding a component of the score variable. In a compact form: $$ \\text{dscore}_j = \\frac{\\partial L_i}{\\partial \\score_j} = y_j - 1(j=\\hat{y}_i)$$\nWe must now propagate this derivative to the parameters of the output layer. From definition we have $\\score = h W2 + b2$. To improve visualization we can expand it in the form (considering a single sample batch, n=1):\n$$\\begin{align*} \\score_1 \u0026amp;= h_1 W2_{11} + h_2 W2_{21} + h_3 W2_{31} + \\cdots + b2_1 \\\\ \\score_2 \u0026amp;= h_1 W2_{12} + h_2 W2_{22} + h_3 W2_{32} + \\cdots + b2_2 \\\\ \\score_3 \u0026amp;= h_1 W2_{13} + h_2 W2_{23} + h_3 W2_{33} + \\cdots + b2_3 \\\\ \u0026amp; \\vdots \u0026amp; \\\\ \\score_j \u0026amp;= h_1 W2_{1j} + h_2 W2_{2j} + h_3 W2_{3j} + \\cdots + b2_j \\\\ \\end{align*}$$\nIt is easy to see that $$\\frac{\\partial \\score_j}{\\partial b2_j} = 1 \\implies \\frac{\\partial L_i}{\\partial b2_j} = \\frac{\\partial L_i}{\\partial \\score_j} \\frac{\\partial \\score_j}{\\partial b2_j} = \\frac{\\partial L_i}{\\partial \\score_j}.$$ This imples that the vector of weights update for $b2$ is $db2 = \\text{dscore}$\nSimilarly, we have $$ \\frac{\\partial \\score_j}{\\partial W2_{kj}} = h_k \\implies \\frac{\\partial L_i}{\\partial W2_{kj}} = \\frac{\\partial L_i}{\\partial \\score_j} \\frac{\\partial \\score_j}{\\partial W2_{kj}} = h_k \\frac{\\partial L_i}{\\partial \\score_j}$$\nIn this case, the matrix of weights updates is given by\n$$\\begin{align*} dW2 \u0026amp;= \\begin{pmatrix} \\ \\frac{\\partial L}{\\partial W_{11}} \u0026amp; \\frac{\\partial L}{\\partial W_{12}} \u0026amp; \\cdots \\\\ \\frac{\\partial L}{\\partial W_{21}} \u0026amp; \\frac{\\partial L}{\\partial W_{22}} \u0026amp; \\cdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\\\ \\end{pmatrix} \\\\ \u0026amp;= \\begin{pmatrix} \\ h_1 \\text{dscore}_1 \u0026amp; h_1 \\text{dscore}_2 \u0026amp; \\cdots \\\\ h_2 \\text{dscore}_1 \u0026amp; h_2 \\text{dscore}_2 \u0026amp; \\cdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\\\ \\end{pmatrix} \\\\ \u0026amp; = \\begin{pmatrix} \\ h_1 \\\\ h_2 \\\\ \\vdots \\\\ \\end{pmatrix} \\ \\begin{pmatrix} \\ \\text{dscore}_1 \u0026amp; \\text{dscore}_2 \u0026amp; \\cdots \\\\ \\end{pmatrix} \\\\ \u0026amp; = h^T \\text{dscore} \\\\ \\end{align*}$$\nTo propagate the gradient to the hidden layer we must first calculate the gradient with respect to $h$: $$\\frac{\\partial \\score_j}{\\partial h_k} = W2_{kj}$$\nThe differential parameter vector $dh$ is given by:\n$$\\begin{align*} dh^T \u0026amp;= \\begin{pmatrix} \\ \\frac{\\partial L}{\\partial h_1} \\\\ \\frac{\\partial L}{\\partial h_2} \\\\ \\vdots \\\\ \\end{pmatrix} \\\\ \u0026amp;= \\begin{pmatrix} \\ \\frac{\\partial L}{\\partial\\score_1}\\frac{\\partial \\score_1}{\\partial h_1}+ \\frac{\\partial L}{\\partial\\score_2}\\frac{\\partial \\score_2}{\\partial h_1} + \\cdots\\\\ \\frac{\\partial L}{\\partial\\score_1}\\frac{\\partial \\score_1}{\\partial h_2}+ \\frac{\\partial L}{\\partial\\score_2}\\frac{\\partial \\score_2}{\\partial h_2} + \\cdots\\\\ \\vdots \\\\ \\end{pmatrix}\\\\ \u0026amp;= \\begin{pmatrix} \\ \\frac{\\partial \\score_1}{\\partial h_1} \u0026amp; \\frac{\\partial \\score_2}{\\partial h_1} \u0026amp; \\cdots\\\\ \\frac{\\partial \\score_1}{\\partial h_2} \u0026amp; \\frac{\\partial \\score_2}{\\partial h_2} \u0026amp; \\cdots\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \\\\ \\end{pmatrix} \\ \\begin{pmatrix} \\ \\frac{\\partial L}{\\partial\\score_1}\\\\ \\frac{\\partial L}{\\partial\\score_2}\\\\ \\vdots \\\\ \\end{pmatrix} \\\\ \u0026amp;= W2 \\times \\text{dscore}^T\\\\ dh \u0026amp;= \\text{dscore} \\times W2^T \\\\ \\end{align*}$$\nNext we consider the RELU activation: $h=max(0, \\underbrace{xW1+b1}_\\textrm{r})$. $\\frac{dh}{dr} = 1(r\u0026gt;0)$. Thus, $dr = dh \\times 1(h\u0026gt;0)$.\nFinally, for the hidden layer, we can observe $dh$ as the output and $x$ as input, so by extending the equations for $dW2$ and $db2$ we have\n$$\\begin{align*} dW1 \u0026= x^T \\text{dr} \\\\ db1 \u0026= \\text{dr} \\end{align*}$$ Numpy implementation The forward propagation is straightforward as can be seen below.\nh = np.maximum(0, np.dot(x,W1)+b1) score = np.dot(h,W2)+b2 y = np.exp(score) y /= np.sum(y, axis=1, keepdims=True)  Although understanding backpropagation can be difficult, the resulting equations are somewhat simple to implement, because they only need to consider the local derivatives at each step.\nPrimarily we computer the $\\text{dscore}$ intermediate variable, where y_t represents $\\hat{y}$, the true class label.\ndscore = np.copy(y) dscore[range(n), y_t] -= 1 dscore /= n  Next the output layer parameters updates are calculated. Since we now have $n$ training samples, we have $L = \\frac{1}{n} \\sum_{i=0}^n L_i$, so dscore gets summed (the division by $n$ has already taken place in the previous step).\ndW2 = np.dot(h.T, dscore) db2 = np.sum(dscore, axis=0, keepdims=True)  The hidden layer activation derivative is calculated, followed by the parameter updates.\ndh = np.dot(dscore, W2.T) dr = dh dr[h \u0026lt;= 0] = 0 dW1 = np.dot(x.T, dr) db1 = np.sum(dr, axis=0, keepdims=True)  Finally we apply the weight updates using the a specified learning rate $\\eta$ (lr).\nW1 -= lr*dW1 b1 -= lr*db1 W2 -= lr*dW2 b2 -= lr*db2  Source code To check the code and results, please visit this notebook.\n","date":1491998400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491998400,"objectID":"86938d9facd0de017580760ff16b15db","permalink":"https://earnold.me/post/nn-scratch/","publishdate":"2017-04-12T12:00:00Z","relpermalink":"/post/nn-scratch/","section":"post","summary":"Understanding a bit of the math from neural nets and its implementation on Numpy.\n","tags":["machine learning","deep learning"],"title":"Neural networks from scratch","type":"post"},{"authors":null,"categories":null,"content":"A short talk about a project I took part during my internship at Instituto SENAI de Tecnologia and which was also my final year project for my undergraduate program. My contribution was to create an automatic human detection system for complex industrial environments. We used depth images from a overhead camera position to perform detection.\nTwo approaches to the detection problem were implemented and evaluated: traditional learning algorithms with manual feature extraction and deep learning classifiers. During this talk I revealed details and tradeoffs of both methods as well as a quantitative evaluation of their performance.\n","date":1491501600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491501600,"objectID":"ac2e8f1a420967b5b26f31bade134669","permalink":"https://earnold.me/talk/dsfloripa2019/","publishdate":"2017-04-06T18:00:00Z","relpermalink":"/talk/dsfloripa2019/","section":"talk","summary":"A short talk about a project I took part during my internship at Instituto SENAI de Tecnologia and which was also my final year project for my undergraduate program. My contribution was to create an automatic human detection system for complex industrial environments.","tags":null,"title":"Detecting people using depth frames and deep classifiers","type":"talk"},{"authors":null,"categories":null,"content":"A presentation introducing computer vision concepts and show how one can easily develop interesting applications such as augmented reality with Python. It was focused on implementation details for developers and was presented together with William Jamir Silva.\nThe code can be divided into two sections, one focusing on computer vision and another on computer graphics. The first one uses image processing algorithms to extract the position and identification of markers put in front of a camera. The latter one uses such information to render 3D models over the marker in the image in real time.\n","date":1463011200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1463011200,"objectID":"ce6cb5cd6329816d5c82a4ac9b9a09af","permalink":"https://earnold.me/talk/tdc2016/","publishdate":"2016-05-12T00:00:00Z","relpermalink":"/talk/tdc2016/","section":"talk","summary":"A short presentation showing how to develop a simple augmented reality application with Python and OpenCV.","tags":null,"title":"Computer vision with Python and OpenCV","type":"talk"},{"authors":null,"categories":null,"content":"","date":1435104e3,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435104e3,"objectID":"eb2d54da1455e0dad313cbf401bfcbc4","permalink":"https://earnold.me/project/tflow/","publishdate":"2015-06-24T00:00:00Z","relpermalink":"/project/tflow/","section":"project","summary":"Automatic traffic flow and density estimation based on computer vision.","tags":["computer-vision"],"title":"TFlow","type":"project"},{"authors":null,"categories":null,"content":"","date":1421452800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1421452800,"objectID":"4d8bb5a7f38c0845087bd0b7a5b6dcf8","permalink":"https://earnold.me/project/sudosolver/","publishdate":"2015-01-17T00:00:00Z","relpermalink":"/project/sudosolver/","section":"project","summary":"A C++ Sudoku Solver and Generator.","tags":["computer-vision"],"title":"SudoSolver","type":"project"}]